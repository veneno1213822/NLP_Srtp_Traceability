{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312082b8-41e9-4214-ba82-7d0feff8fc6a",
   "metadata": {},
   "source": [
    "# æŒ‡å®šè®­ç»ƒä½¿ç”¨æ˜¾å¡å·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eab8fa1-cf2d-4f56-a67e-4b029db5abd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:30:03.599087Z",
     "iopub.status.busy": "2024-09-10T04:30:03.598705Z",
     "iopub.status.idle": "2024-09-10T04:30:03.609487Z",
     "shell.execute_reply": "2024-09-10T04:30:03.607935Z",
     "shell.execute_reply.started": "2024-09-10T04:30:03.599049Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8602ede4-446b-44b3-a0e7-c75d9af6b302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:30:05.505107Z",
     "iopub.status.busy": "2024-09-10T04:30:05.504379Z",
     "iopub.status.idle": "2024-09-10T04:30:13.786245Z",
     "shell.execute_reply": "2024-09-10T04:30:13.785096Z",
     "shell.execute_reply.started": "2024-09-10T04:30:05.505066Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-10 04:30:08,395] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import oneke_trainer\n",
    "importlib.reload(oneke_trainer)\n",
    "from oneke_trainer import OneKETrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5174f-ffe7-48a3-9ff3-03c1b315ad6f",
   "metadata": {},
   "source": [
    "# è®­ç»ƒå‚æ•°\n",
    "åŒ…æ‹¬åŸºåº§æ¨¡å‹ï¼Œloraæ¨¡å‹å’Œè®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b6235f-12fe-4a07-96dc-9c69a76654e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:30:16.506956Z",
     "iopub.status.busy": "2024-09-10T04:30:16.506211Z",
     "iopub.status.idle": "2024-09-10T04:30:16.514421Z",
     "shell.execute_reply": "2024-09-10T04:30:16.512999Z",
     "shell.execute_reply.started": "2024-09-10T04:30:16.506894Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5\"\n",
    "lora_path = \"lora/baichuan7B-data2text-continue\"\n",
    "data_path = \"data2text/data2text-test-all-v1-Copy1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5780db77-0e85-490d-b6b5-28494494ac02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:30:19.508348Z",
     "iopub.status.busy": "2024-09-10T04:30:19.507499Z",
     "iopub.status.idle": "2024-09-10T04:30:19.515838Z",
     "shell.execute_reply": "2024-09-10T04:30:19.514450Z",
     "shell.execute_reply.started": "2024-09-10T04:30:19.508289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Training...\n"
     ]
    }
   ],
   "source": [
    "oneke_train = OneKETrainer(model_path, lora_path, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42445692-0028-426a-a79e-69c8255500da",
   "metadata": {},
   "source": [
    "# è®­ç»ƒè¿”å›æ•°æ®\n",
    "åŒ…æ‹¬è®­ç»ƒå¼€å§‹æ—¶é—´ï¼Œè®­ç»ƒç»“æŸæ—¶é—´ï¼Œæœ€å¤§æ˜¾å­˜å ç”¨ï¼Œä½¿ç”¨æ˜¾å¡å·ï¼Œç”Ÿæˆloraæ¨¡å‹çš„è·¯å¾„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcae5ffc-89b0-4b88-8ac6-eebb6b3a6064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T04:31:02.455949Z",
     "iopub.status.busy": "2024-09-10T04:31:02.455100Z",
     "iopub.status.idle": "2024-09-10T04:36:47.077988Z",
     "shell.execute_reply": "2024-09-10T04:36:47.077114Z",
     "shell.execute_reply.started": "2024-09-10T04:31:02.455887Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "File renamed from /root/data2text-oneke/data2text/data2text_20240910/data2text-ner-train-val.jsonl to /root/data2text-oneke/data2text/data2text_20240910/data2text-ner-train-val-json.json\n",
      "File renamed from /root/data2text-oneke/data2text/data2text_20240910/data2text-ner-test.jsonl to /root/data2text-oneke/data2text/data2text_20240910/data2text-ner-test-json.json\n",
      "09/10/2024 04:31:02 - WARNING - args.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1332] 2024-09-10 04:31:02,484 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1764] 2024-09-10 04:31:02,484 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/10/2024 04:31:02 - INFO - args.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "09/10/2024 04:31:02 - INFO - args.parser - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/data2text-oneke/lora/NER_lora_model_20240910/runs/Sep10_04-31-02_06d8af42ca5a,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/root/data2text-oneke/lora/NER_lora_model_20240910,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/data2text-oneke/lora/NER_lora_model_20240910,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "09/10/2024 04:31:02 - INFO - src.finetune - Start Time: 2024:09:10 04:31:02\n",
      "09/10/2024 04:31:02 - INFO - src.finetune - model_args:ModelArguments(model_name_or_path='cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5', model_name='baichuan', cache_dir=None, use_fast_tokenizer=True, trust_remote_code=True, use_auth_token=False, model_revision='main', split_special_tokens=False, bits=4, adam8bit=False, double_quant=True, quant_type='nf4', checkpoint_dir=['lora/baichuan7B-data2text-continue'])\n",
      "data_args:DataArguments(train_file='/root/data2text-oneke/data2text/data2text_20240910/data2text-ner-train-transformed.json', valid_file='/root/data2text-oneke/data2text/data2text_20240910/data2text-ner-val-transformed.json', predict_file=None, preprocessing_num_workers=16, overwrite_cache=False, cache_path=None, template='baichuan2', system_prompt=None, max_source_length=400, max_target_length=300, cutoff_len=700, val_set_size=1000, pad_to_max_length=False, ignore_pad_token_for_loss=True, train_on_prompt=False, language='zh', id_text='input')\n",
      "training_args:TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/root/data2text-oneke/lora/NER_lora_model_20240910/runs/Sep10_04-31-02_06d8af42ca5a,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/root/data2text-oneke/lora/NER_lora_model_20240910,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/root/data2text-oneke/lora/NER_lora_model_20240910,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "finetuning_args:FinetuningArguments(dpo_beta=0.1, ppo_logger=None, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_checkpoint=None, ref_model_quantization_bit=None, reward_model=None, reward_model_checkpoint=None, reward_model_quantization_bit=None, reward_model_type='lora', lora_r=64, lora_alpha=64, lora_dropout=0.05, lora_target_modules=['W_pack', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'], additional_target=None, resume_lora_training=True, num_layer_trainable=3, name_module_trainable=['mlp'], stage='sft', finetuning_type='lora', upcast_layernorm=False, neft_alpha=0, export_dir=None, plot_loss=False)\n",
      "generating_args:GenerationArguments(max_length=512, max_new_tokens=256, min_new_tokens=None, do_sample=False, num_beams=1, num_beam_groups=1, penalty_alpha=None, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n",
      "09/10/2024 04:31:02 - INFO - src.finetune - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "trainer_class:<class 'transformers.trainer.Trainer'>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:1850] 2024-09-10 04:31:02,497 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1850] 2024-09-10 04:31:02,497 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1850] 2024-09-10 04:31:02,498 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1850] 2024-09-10 04:31:02,498 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:713] 2024-09-10 04:31:02,554 >> loading configuration file cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5/config.json\n",
      "[INFO|configuration_utils.py:713] 2024-09-10 04:31:02,556 >> loading configuration file cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-09-10 04:31:02,557 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"_name_or_path\": \"cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5\",\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/10/2024 04:31:02 - INFO - model.loader - Quantizing model to 4 bit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "[INFO|modeling_utils.py:2854] 2024-09-10 04:31:02,606 >> loading weights file cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1200] 2024-09-10 04:31:12,901 >> Instantiating BaichuanForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2024-09-10 04:31:13,720 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2971] 2024-09-10 04:31:13,963 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
      "[INFO|modeling_utils.py:3643] 2024-09-10 04:31:28,894 >> All model checkpoint weights were used when initializing BaichuanForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3651] 2024-09-10 04:31:28,896 >> All the weights of BaichuanForCausalLM were initialized from the model checkpoint at cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BaichuanForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:728] 2024-09-10 04:31:28,900 >> loading configuration file cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2024-09-10 04:31:28,901 >> Generate config GenerationConfig {\n",
      "  \"assistant_token_id\": 196,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.85,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"user_token_id\": 195\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/10/2024 04:31:29 - INFO - model.adapter - Gradient checkpointing enabled.\n",
      "09/10/2024 04:31:29 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "09/10/2024 04:31:29 - INFO - model.adapter - Resume model checkpoint(s): lora/baichuan7B-data2text-continue .\n",
      "09/10/2024 04:32:14 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): lora/baichuan7B-data2text-continue\n",
      "09/10/2024 04:32:14 - INFO - model.loader - trainable params: 143130624 || all params: 7649103872 || trainable%: 1.8712\n",
      "09/10/2024 04:32:14 - INFO - src.finetune - BOS:1,<s>\tEOS:2,</s>\tPAD:0,<unk>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d5e107e102fab8e0\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 100 examples [00:00, 1278.12 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-35eb21852551bf7e\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 25 examples [00:00, 10683.40 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "[INFO|tokenization_utils_base.py:926] 2024-09-10 04:32:15,872 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00015_of_00016.arrow\n",
      "Spawning 16 processes\n",
      "Map (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00004_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00005_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00006_of_00016.arrow\n",
      "Map (num_proc=16):   6%|â–Œ         | 6/100 [00:00<00:02, 32.20 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00011_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00012_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00009_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00000_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00013_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00015_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00014_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00002_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00001_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00003_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00010_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00007_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-d5e107e102fab8e0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0d14562c31db9a11_00008_of_00016.arrow\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 269.93 examples/s]\n",
      "Concatenating 16 shards\n",
      "[INFO|tokenization_utils_base.py:926] 2024-09-10 04:32:16,787 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00015_of_00016.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 7837, 2925, 1664, 7837, 6919, 2925, 1664, 7837, 4956, 2925, 1664, 7837, 6315, 65226, 1664, 12269, 5338, 1664, 5317, 2365, 4104, 7323, 3591, 2756, 8401, 9808, 93331, 23550, 12725, 6699, 92649, 41113, 9808, 15117, 66, 16032, 92538, 2841, 2072, 2019, 2274, 92595, 1850, 92505, 1925, 11219, 30385, 11705, 1805, 66, 52639, 196, 30338, 7837, 5338, 86381, 1664, 7837, 6919, 5338, 86381, 1664, 7837, 4956, 5338, 86381, 1664, 7837, 6315, 5338, 27381, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®ä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»inputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„å®ä½“ç±»å‹è¿”å›ç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ã€‚\", \"schema\": [\"è£…å¤‡\", \"è£…å¤‡åç§°\", \"è£…å¤‡æ•°é‡\", \"è£…å¤‡ç±»å‹\"], \"input\": \"ä¸€ä»½å…³äºæˆç«‹èˆªç©ºè®¾å¤‡é›†å›¢å…¬å¸çš„æ€»ç»Ÿä»¤è‰æ¡ˆå·²è¢«æäº¤ç»™æ™®äº¬æ€»ç»Ÿç­¾ç½²ã€‚è¯¥å…¬å¸å°†åŒ…æ‹¬æ‰€æœ‰æŠ€æœ¯ç³»ç»Ÿç§‘å­¦ç”Ÿäº§ä¸­å¿ƒçš„ä¼ä¸šä»¥åŠå…¶ä»–ä¸€ç³»åˆ—ä¼ä¸šã€‚\"}<reserved_107>{\"è£…å¤‡\": [], \"è£…å¤‡åç§°\": [], \"è£…å¤‡æ•°é‡\": [], \"è£…å¤‡ç±»å‹\": []}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 7837, 5338, 86381, 1664, 7837, 6919, 5338, 86381, 1664, 7837, 4956, 5338, 86381, 1664, 7837, 6315, 5338, 27381, 92795, 2]\n",
      "labels:\n",
      " {\"è£…å¤‡\": [], \"è£…å¤‡åç§°\": [], \"è£…å¤‡æ•°é‡\": [], \"è£…å¤‡ç±»å‹\": []}</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 16 processes\n",
      "Map (num_proc=16):   0%|          | 0/25 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00000_of_00016.arrow\n",
      "Map (num_proc=16):   8%|â–Š         | 2/25 [00:00<00:01, 11.56 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00015_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00009_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00010_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00014_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00012_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00011_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00006_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00003_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00002_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00004_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00005_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00001_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00013_of_00016.arrow\n",
      "Map (num_proc=16):  20%|â–ˆâ–ˆ        | 5/25 [00:00<00:01, 14.53 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00008_of_00016.arrow\n",
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-35eb21852551bf7e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ad2b8cc2e8f76ab5_00007_of_00016.arrow\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 48.34 examples/s]\n",
      "Concatenating 16 shards\n",
      "[INFO|trainer.py:403] 2024-09-10 04:32:17,875 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 3461, 3531, 2925, 1664, 3461, 6315, 2925, 1664, 92892, 92534, 2925, 1664, 2020, 65226, 1664, 12269, 5338, 1664, 61358, 37594, 9821, 62579, 57664, 18988, 92364, 16561, 3546, 10485, 6004, 65, 92594, 2904, 92489, 7830, 92405, 25780, 62579, 92385, 52774, 93604, 92666, 1697, 6624, 66, 52639, 196, 30338, 3461, 3531, 5338, 86381, 1664, 3461, 6315, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 2020, 5338, 27381, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®ä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»inputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„å®ä½“ç±»å‹è¿”å›ç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ã€‚\", \"schema\": [\"ä»»åŠ¡çŠ¶æ€\", \"ä»»åŠ¡ç±»å‹\", \"å†›ç§\", \"å•ä½\"], \"input\": \"å¥¥å·´é©¬æƒ³åœ¨æœªæ¥çš„æ ¸æ­¦å™¨å‰Šå‡è°ˆåˆ¤ä¸­å¼•å…¥æ–°çš„æ­¦å™¨åˆ†ç±»ï¼Œå¹¶å‡†å¤‡ä¸ä¿„ç½—æ–¯å°±æˆ˜æœ¯æ ¸æ­¦å™¨å’Œè´®å­˜å¼¹å¤´è¿›è¡Œè®¨è®ºã€‚\"}<reserved_107>{\"ä»»åŠ¡çŠ¶æ€\": [], \"ä»»åŠ¡ç±»å‹\": [], \"å†›ç§\": [], \"å•ä½\": []}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 3461, 3531, 5338, 86381, 1664, 3461, 6315, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 2020, 5338, 27381, 92795, 2]\n",
      "labels:\n",
      " {\"ä»»åŠ¡çŠ¶æ€\": [], \"ä»»åŠ¡ç±»å‹\": [], \"å†›ç§\": [], \"å•ä½\": []}</s>\n",
      "09/10/2024 04:32:17 - INFO - src.finetune - *** Train ***\n",
      "09/10/2024 04:32:17 - INFO - src.finetune - resume_from_checkpoint: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1712] 2024-09-10 04:32:18,124 >> ***** Running training *****\n",
      "[INFO|trainer.py:1713] 2024-09-10 04:32:18,125 >>   Num examples = 100\n",
      "[INFO|trainer.py:1714] 2024-09-10 04:32:18,126 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1715] 2024-09-10 04:32:18,127 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1718] 2024-09-10 04:32:18,127 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1719] 2024-09-10 04:32:18,128 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1720] 2024-09-10 04:32:18,129 >>   Total optimization steps = 120\n",
      "[INFO|trainer.py:1721] 2024-09-10 04:32:18,133 >>   Number of trainable parameters = 143,130,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 04:23, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.011504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.005489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.009388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.008988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.009032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.009112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3115] 2024-09-10 04:32:42,111 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:32:42,113 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:32:42,114 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:32:44,090 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-12\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:33:09,517 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:33:09,518 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:33:09,519 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:33:11,517 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-25\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:33:37,017 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:33:37,019 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:33:37,020 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:33:39,014 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-37\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:34:04,703 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:34:04,705 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:34:04,705 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:34:06,696 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-50\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:34:32,258 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:34:32,259 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:34:32,260 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:34:34,251 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-62\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:34:59,899 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:34:59,900 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:34:59,901 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:35:01,890 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-75\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:35:27,488 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:35:27,489 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:35:27,490 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:35:29,483 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-87\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:35:55,026 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:35:55,027 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:35:55,028 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:35:57,018 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-100\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:36:22,474 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:36:22,475 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:36:22,476 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:36:24,468 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-112\n",
      "[INFO|trainer.py:3115] 2024-09-10 04:36:40,640 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:36:40,641 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:36:40,642 >>   Batch size = 2\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:36:42,631 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910/checkpoint-120\n",
      "[INFO|trainer.py:1960] 2024-09-10 04:36:44,436 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2841] 2024-09-10 04:36:44,440 >> Saving model checkpoint to /root/data2text-oneke/lora/NER_lora_model_20240910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        9.6\n",
      "  total_flos               =  3116356GF\n",
      "  train_loss               =     0.0076\n",
      "  train_runtime            = 0:04:26.30\n",
      "  train_samples_per_second =      3.755\n",
      "  train_steps_per_second   =      0.451\n",
      "09/10/2024 04:36:45 - INFO - src.finetune - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3115] 2024-09-10 04:36:45,073 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3117] 2024-09-10 04:36:45,074 >>   Num examples = 25\n",
      "[INFO|trainer.py:3120] 2024-09-10 04:36:45,075 >>   Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        9.6\n",
      "  eval_loss               =     0.0091\n",
      "  eval_runtime            = 0:00:01.99\n",
      "  eval_samples_per_second =     12.517\n",
      "  eval_steps_per_second   =      6.509\n",
      "09/10/2024 04:36:47 - INFO - src.finetune - End Time: 2024:09:10 04:36:47\n"
     ]
    }
   ],
   "source": [
    "start_time, end_time, max_memory_allocated, gpu_index, lora_output_path = oneke_train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d29cd4-5343-465c-9c86-4057862a7503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T03:44:25.450197Z",
     "iopub.status.busy": "2024-09-05T03:44:25.449339Z",
     "iopub.status.idle": "2024-09-05T03:44:25.457407Z",
     "shell.execute_reply": "2024-09-05T03:44:25.456479Z",
     "shell.execute_reply.started": "2024-09-05T03:44:25.450136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05,03:38:19 2024-09-05,03:44:09 7218160640 1 /root/data2text-oneke/lora/NER_lora_model_20240905\n"
     ]
    }
   ],
   "source": [
    "print(start_time, end_time, max_memory_allocated, gpu_index, lora_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94830427-93bc-4390-972b-8b6ca3ec95a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
