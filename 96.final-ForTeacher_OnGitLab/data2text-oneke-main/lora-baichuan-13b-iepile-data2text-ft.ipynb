{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab89c7ef-a923-4393-a21a-6ce75ba2c865",
   "metadata": {},
   "source": [
    "# Baichuan2-13B iepile-data2text LoRAå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01246fe-bb6e-4e79-8174-c8922adc00b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:06:55.033036Z",
     "iopub.status.busy": "2024-08-14T10:06:55.031741Z",
     "iopub.status.idle": "2024-08-14T10:06:55.040235Z",
     "shell.execute_reply": "2024-08-14T10:06:55.039218Z",
     "shell.execute_reply.started": "2024-08-14T10:06:55.032990Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.6.0.17:8888\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf05eb9-1062-48a6-8fb7-f63364050fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T11:11:33.531764Z",
     "iopub.status.busy": "2024-08-13T11:11:33.531559Z",
     "iopub.status.idle": "2024-08-13T11:11:33.535718Z",
     "shell.execute_reply": "2024-08-13T11:11:33.534863Z",
     "shell.execute_reply.started": "2024-08-13T11:11:33.531744Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62305aee-615e-4722-abff-37c20b0434d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T13:04:17.934766Z",
     "iopub.status.busy": "2024-08-08T13:04:17.933867Z",
     "iopub.status.idle": "2024-08-08T13:04:20.131426Z",
     "shell.execute_reply": "2024-08-08T13:04:20.130396Z",
     "shell.execute_reply.started": "2024-08-08T13:04:17.934710Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'baichuan-inc/Baichuan2-7B-Chat'\n",
    "# lora_path = 'zjunlp/baichuan2-7b-iepile-lora'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05e79ea-88a3-45aa-a6f2-a3104bff4514",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-08-08T13:04:23.942770Z",
     "iopub.status.busy": "2024-08-08T13:04:23.941882Z",
     "iopub.status.idle": "2024-08-08T13:04:51.538179Z",
     "shell.execute_reply": "2024-08-08T13:04:51.536886Z",
     "shell.execute_reply.started": "2024-08-08T13:04:23.942710Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'zjunlp/baichuan2-7b-iepile-lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/zjunlp/baichuan2-7b-iepile-lora/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/utils/config.py:177\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66b4c273-236933bb51dfb318772ea7b1;d2607585-fc4b-4fee-9f39-0da95a5a6ff0)\n\nRepository Not Found for url: https://huggingface.co/zjunlp/baichuan2-7b-iepile-lora/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     model_path,\n\u001b[1;32m      3\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:244\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 244\u001b[0m         \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_peft_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubfolder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_auth_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    253\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/utils/config.py:183\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    178\u001b[0m             model_id,\n\u001b[1;32m    179\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    181\u001b[0m         )\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_attributes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'zjunlp/baichuan2-7b-iepile-lora'"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     lora_path,\n",
    "# )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df2be1-c920-4020-bbfb-38f65799a612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393dc0d-34e3-4013-a933-f8f0d025c19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T09:58:05.075926Z",
     "iopub.status.busy": "2024-08-14T09:58:05.075012Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.6.0.17'), PosixPath('http'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 17:58:09 - WARNING - args.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1332] 2024-08-14 17:58:09,837 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1764] 2024-08-14 17:58:09,837 >> PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "08/14/2024 17:58:09 - INFO - args.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "08/14/2024 17:58:09 - INFO - args.parser - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_17-58-09_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/14/2024 17:58:09 - INFO - __main__ - Start Time: 2024:08:14 17:58:09\n",
      "08/14/2024 17:58:09 - INFO - __main__ - model_args:ModelArguments(model_name_or_path='baichuan-inc/BaiChuan2-13B-Chat', model_name='baichuan', cache_dir=None, use_fast_tokenizer=True, trust_remote_code=True, use_auth_token=False, model_revision='main', split_special_tokens=False, bits=4, adam8bit=False, double_quant=True, quant_type='nf4', checkpoint_dir=['zjunlp/baichuan2-13b-iepile-lora'])\n",
      "data_args:DataArguments(train_file='data2text/iepile-ner-train-transformed.json', valid_file='data2text/iepile-ner-val-transformed.json', predict_file=None, preprocessing_num_workers=16, overwrite_cache=False, cache_path=None, template='baichuan2', system_prompt=None, max_source_length=400, max_target_length=300, cutoff_len=700, val_set_size=1000, pad_to_max_length=False, ignore_pad_token_for_loss=True, train_on_prompt=False, language='zh', id_text='input')\n",
      "training_args:TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_17-58-09_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "finetuning_args:FinetuningArguments(dpo_beta=0.1, ppo_logger=None, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_checkpoint=None, ref_model_quantization_bit=None, reward_model=None, reward_model_checkpoint=None, reward_model_quantization_bit=None, reward_model_type='lora', lora_r=64, lora_alpha=64.0, lora_dropout=0.05, lora_target_modules=['W_pack', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'], additional_target=None, resume_lora_training=True, num_layer_trainable=3, name_module_trainable=['mlp'], stage='sft', finetuning_type='lora', upcast_layernorm=False, neft_alpha=0, export_dir=None, plot_loss=False)\n",
      "generating_args:GenerationArguments(max_length=512, max_new_tokens=256, min_new_tokens=None, do_sample=False, num_beams=1, num_beam_groups=1, penalty_alpha=None, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n",
      "08/14/2024 17:58:09 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "trainer_class:<class 'transformers.trainer.Trainer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:58:11,264 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:58:11,264 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:58:11,264 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:58:11,264 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 17:58:11,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 17:58:13,082 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-08-14 17:58:13,085 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"_name_or_path\": \"baichuan-inc/BaiChuan2-13B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"baichuan-inc/BaiChuan2-13B-Chat--configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"baichuan-inc/BaiChuan2-13B-Chat--modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": [\n",
      "    false\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13696,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "08/14/2024 17:58:13 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.19)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "[INFO|modeling_utils.py:2857] 2024-08-14 17:58:13,698 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1200] 2024-08-14 17:58:13,700 >> Instantiating BaichuanForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 17:58:13,701 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2971] 2024-08-14 17:58:13,770 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:04<00:00, 21.62s/it]\n",
      "[INFO|modeling_utils.py:3643] 2024-08-14 17:59:18,780 >> All model checkpoint weights were used when initializing BaichuanForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3651] 2024-08-14 17:59:18,780 >> All the weights of BaichuanForCausalLM were initialized from the model checkpoint at baichuan-inc/BaiChuan2-13B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BaichuanForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:730] 2024-08-14 17:59:19,263 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--BaiChuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 17:59:19,264 >> Generate config GenerationConfig {\n",
      "  \"assistant_token_id\": 196,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.85,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"user_token_id\": 195\n",
      "}\n",
      "\n",
      "08/14/2024 17:59:19 - INFO - model.adapter - Gradient checkpointing enabled.\n",
      "08/14/2024 17:59:19 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/14/2024 17:59:19 - INFO - model.adapter - Resume model checkpoint(s): zjunlp/baichuan2-13b-iepile-lora .\n",
      "08/14/2024 18:00:46 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): zjunlp/baichuan2-13b-iepile-lora\n",
      "08/14/2024 18:00:46 - INFO - model.loader - trainable params: 223150080 || all params: 14119818240 || trainable%: 1.5804\n",
      "08/14/2024 18:00:46 - INFO - __main__ - BOS:1,<s>\tEOS:2,</s>\tPAD:0,<unk>\n",
      "Using custom data configuration default-230b0cd978e532d9\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Using custom data configuration default-c20be129830f148f\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:00:48,096 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-3d5ad7cdc04a108e_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 12915, 2925, 1664, 4956, 2925, 1664, 1967, 1650, 2925, 1664, 1650, 65226, 1664, 12269, 5338, 1664, 71109, 18648, 3346, 6866, 70480, 2392, 65, 21904, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 65, 14193, 92926, 93198, 94390, 18648, 4487, 17022, 2339, 94638, 92827, 93845, 93144, 93604, 11467, 39546, 96337, 77343, 88865, 65, 92391, 2849, 4330, 15890, 2274, 66, 4605, 7375, 7491, 17022, 5073, 92730, 92493, 11466, 93107, 65, 2802, 11466, 92919, 92373, 92362, 92338, 92335, 25409, 92424, 14623, 94575, 94515, 92986, 92843, 96467, 66, 52639, 196, 30338, 12915, 5338, 86381, 1664, 4956, 5338, 86381, 1664, 1967, 1650, 5338, 86381, 1664, 1650, 5338, 50154, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 19342, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"å¤‡æ³¨\", \"æ•°é‡\", \"å¼€å§‹æ—¶é—´\", \"æ—¶é—´\"], \"input\": \"æ®ç¾Žå›½ç©ºå†›å…¨çƒæ‰“å‡»å¸ä»¤éƒ¨è¡¨ç¤ºï¼Œå½“åœ°æ—¶é—´2æœˆ5æ—¥12æ—¶33åˆ†ï¼Œä»–ä»¬åœ¨èŒƒç™»å ¡ç©ºå†›åŸºåœ°å‘å°„äº†ä¸€æžšæœªæºè½½å¼¹å¤´çš„æ°‘å…µâ…¢æ´²é™…å¼¹é“å¯¼å¼¹ï¼Œä»¥è¿›ä¸€æ­¥æµ‹è¯•å¯¼å¼¹ç³»ç»Ÿã€‚æ­¤æ¬¡è¯•éªŒåŒæ­¥å‘å°„äº†ä¸€ä¸ªå†å…¥é£žè¡Œå™¨ï¼ŒæˆåŠŸé£žè¡Œçº¦6920åƒç±³åŽæŠµè¾¾å¤¸è´¾æž—çŽ¯ç¤ã€‚\"}<reserved_107>{\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 12915, 5338, 86381, 1664, 4956, 5338, 86381, 1664, 1967, 1650, 5338, 86381, 1664, 1650, 5338, 50154, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 19342, 92795, 2]\n",
      "labels:\n",
      " {\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}</s>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:00:48,186 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9f5cac70fa421c96_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 92397, 92620, 2925, 1664, 3461, 3531, 2925, 1664, 92892, 92534, 2925, 1664, 86096, 65226, 1664, 12269, 5338, 1664, 42008, 39549, 92639, 6675, 2296, 1719, 2998, 1748, 32077, 92888, 92402, 1860, 65, 43236, 92538, 20993, 32077, 3581, 65, 2244, 92538, 92888, 2019, 4349, 8398, 13087, 66, 41018, 65, 92627, 93518, 1719, 3205, 17812, 2296, 1719, 1697, 18988, 65, 2549, 4104, 1558, 3685, 1805, 65, 12775, 3142, 17526, 10314, 19135, 93110, 5105, 94129, 66, 52639, 196, 30338, 92397, 92620, 5338, 86381, 1664, 3461, 3531, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 86096, 5338, 27381, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"å›½åˆ«\", \"ä»»åŠ¡çŠ¶æ€\", \"å†›ç§\", \"æŒ‡æŒ¥å‘˜\"], \"input\": \"æ²™ç‰¹é˜¿æ‹‰ä¼¯å·²é‚€è¯·ç¾Žå›½å…¬å¸å‚ä¸Žå‘å±•æ°‘ç”¨æ ¸èƒ½é¡¹ç›®ï¼Œå¹¶è¡¨ç¤ºå°†ä¸“æ³¨äºŽæ°‘ç”¨é¢†åŸŸï¼Œä¸ä¼šå°†æ ¸æŠ€æœ¯ç”¨äºŽå†›äº‹ç”¨é€”ã€‚æ®æŠ¥é“ï¼Œè¥¿å±‹å…¬å¸æ­£åœ¨ä¸Žå…¶ä»–ç¾Žå›½å…¬å¸è¿›è¡Œè°ˆåˆ¤ï¼Œè®¡åˆ’æˆç«‹ä¸€ä¸ªè”åˆä¼ä¸šï¼Œå»ºé€ ä»·å€¼æ•°åäº¿ç¾Žå…ƒçš„ä¸¤åº§ååº”å †ã€‚\"}<reserved_107>{\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 92397, 92620, 5338, 86381, 1664, 3461, 3531, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 86096, 5338, 27381, 92795, 2]\n",
      "labels:\n",
      " {\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}</s>\n",
      "[INFO|trainer.py:403] 2024-08-14 18:00:48,244 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n",
      "08/14/2024 18:00:48 - INFO - __main__ - *** Train ***\n",
      "08/14/2024 18:00:48 - INFO - __main__ - resume_from_checkpoint: None\n",
      "[INFO|trainer.py:1712] 2024-08-14 18:00:49,763 >> ***** Running training *****\n",
      "[INFO|trainer.py:1713] 2024-08-14 18:00:49,764 >>   Num examples = 7,732\n",
      "[INFO|trainer.py:1714] 2024-08-14 18:00:49,764 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1715] 2024-08-14 18:00:49,764 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1718] 2024-08-14 18:00:49,764 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1719] 2024-08-14 18:00:49,764 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1720] 2024-08-14 18:00:49,764 >>   Total optimization steps = 9,660\n",
      "[INFO|trainer.py:1721] 2024-08-14 18:00:49,767 >>   Number of trainable parameters = 223,150,080\n",
      "{'loss': 0.56, 'learning_rate': 4.99896480331263e-05, 'epoch': 0.0}             \n",
      "{'loss': 0.3054, 'learning_rate': 4.997929606625259e-05, 'epoch': 0.0}          \n",
      "{'loss': 0.2441, 'learning_rate': 4.996894409937888e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.3077, 'learning_rate': 4.995859213250518e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.1238, 'learning_rate': 4.994824016563147e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.384, 'learning_rate': 4.993788819875777e-05, 'epoch': 0.01}          \n",
      "{'loss': 0.2504, 'learning_rate': 4.9927536231884056e-05, 'epoch': 0.01}        \n",
      "{'loss': 0.1511, 'learning_rate': 4.991718426501035e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1966, 'learning_rate': 4.9906832298136646e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.0912, 'learning_rate': 4.989648033126294e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1646, 'learning_rate': 4.988612836438924e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.0784, 'learning_rate': 4.9875776397515526e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1394, 'learning_rate': 4.986542443064182e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1533, 'learning_rate': 4.985507246376812e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.2036, 'learning_rate': 4.984472049689442e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1817, 'learning_rate': 4.983436853002071e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1209, 'learning_rate': 4.9824016563147e-05, 'epoch': 0.04}           \n",
      "{'loss': 0.0973, 'learning_rate': 4.98136645962733e-05, 'epoch': 0.04}          \n",
      "{'loss': 0.1197, 'learning_rate': 4.980331262939959e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.1342, 'learning_rate': 4.979296066252588e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.0885, 'learning_rate': 4.9782608695652176e-05, 'epoch': 0.04}        \n",
      "{'loss': 0.1127, 'learning_rate': 4.977225672877847e-05, 'epoch': 0.05}         \n",
      "  0%|â–                                     | 45/9660 [04:04<13:17:07,  4.97s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/finetune.py \\\n",
    "    --do_train --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_name_or_path 'baichuan-inc/BaiChuan2-13B-Chat' \\\n",
    "    --checkpoint_dir 'zjunlp/baichuan2-13b-iepile-lora' \\\n",
    "    --stage 'sft' \\\n",
    "    --model_name 'baichuan' \\\n",
    "    --template 'baichuan2' \\\n",
    "    --train_file 'data2text/iepile-ner-train-transformed.json' \\\n",
    "    --valid_file 'data2text/iepile-ner-val-transformed.json' \\\n",
    "    --output_dir='lora/test' \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --optim \"adamw_torch\" \\\n",
    "    --max_source_length 400 \\\n",
    "    --cutoff_len 700 \\\n",
    "    --max_target_length 300 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 10 \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --bf16 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad5345-49f5-49d9-91c8-417d61f78956",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨å¾®è°ƒæ¨¡åž‹å¯¹æµ‹è¯•é›†è¿›è¡ŒæŠ½å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ac50-2ba2-43fd-a682-38dc6341252e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:07:15.026986Z",
     "iopub.status.busy": "2024-08-14T10:07:15.026078Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//pypi.tuna.tsinghua.edu.cn/simple')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8888'), PosixPath('//10.6.0.17'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 18:07:19 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "08/14/2024 18:07:21 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.19)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:03<00:00, 21.05s/it]\n",
      "08/14/2024 18:08:25 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/14/2024 18:09:51 - INFO - model.adapter - Merged model checkpoint(s): ['lora/baichuan13B-iepile-data2text-continue'].\n",
      "08/14/2024 18:09:51 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): lora/baichuan13B-iepile-data2text-continue\n",
      "08/14/2024 18:09:51 - INFO - model.loader - trainable params: 0 || all params: 14119818240 || trainable%: 0.0000\n",
      "08/14/2024 18:09:51 - INFO - model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "BOS:1,<s>\tEOS:2,</s>\tPAD:0,<unk>\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1290/1290 [00:00<00:00, 2297.30 examples/s]\n",
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 4272, 2925, 1664, 12915, 2925, 1664, 6061, 6919, 2925, 1664, 3461, 6315, 65226, 1664, 12269, 5338, 1664, 92688, 7830, 10214, 28056, 2782, 92619, 10755, 65, 10214, 63, 92338, 92335, 92338, 92336, 4552, 4497, 38399, 92538, 7839, 92441, 14762, 45397, 15890, 92385, 17700, 66, 38399, 92364, 65, 19858, 93052, 92575, 94100, 2790, 37486, 6866, 92385, 3034, 16052, 65, 22995, 21235, 26705, 25780, 65, 1697, 25780, 53742, 2986, 66, 52639, 196]\n",
      "inputs:\n",
      "<reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"äººæ•°\", \"å¤‡æ³¨\", \"ä¸»ä½“åç§°\", \"ä»»åŠ¡ç±»åž‹\"], \"input\": \"æ®ä¿„ç½—æ–¯è¥¿éƒ¨å†›åŒºæ–°é—»å¤„é€éœ²ï¼Œè¥¿éƒ¨-2021æˆ˜ç•¥æŒ‡æŒ¥æ¼”ä¹ å°†èšç„¦äºŽå¯¹æŠ—å·¡èˆªå¯¼å¼¹å’Œæ— äººæœºã€‚æ¼”ä¹ ä¸­ï¼Œå°†å¯¹å‡æƒ³æ•Œå®žæ–½ç«åŠ›æ‰“å‡»å’Œç”µå­å¹²æ‰°ï¼Œå¹¶é€šè¿‡åž‚ç›´åŒ…å›´æˆ˜æœ¯ï¼Œè¿›è¡Œæˆ˜æœ¯ç©ºé™è¡ŒåŠ¨ã€‚\"}<reserved_107>\n",
      "{\"äººæ•°\": [], \"å¤‡æ³¨\": [], \"ä¸»ä½“åç§°\": [\"è¥¿éƒ¨å†›åŒº\"], \"ä»»åŠ¡ç±»åž‹\": []}\n",
      "{\"äººç‰©\": [], \"åœ°ç‚¹\": [], \"è£…å¤‡ç±»åž‹\": [], \"è£…å¤‡\": []}\n",
      "{\"æŒ‡æŒ¥å‘˜\": [], \"å›½å®¶\": [], \"ä¸»ä½“ç±»åž‹\": [\"å†›äº‹æ¼”ä¹ \"], \"ç»„ç»‡æœºæž„\": [\"è¥¿éƒ¨å†›åŒºæ–°é—»å¤„\"]}\n",
      "{\"æ•°é‡\": [], \"å›½åˆ«\": [], \"è£…å¤‡åç§°\": [], \"å•ä½\": []}\n",
      "{\"ä¸»ä½“æ•°é‡\": [\"1\"], \"å†›ç§\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"è£…å¤‡æ•°é‡\": []}\n",
      "{\"æ—¶é—´\": [], \"å¼€å§‹æ—¶é—´\": []}\n",
      "{\"äººæ•°\": [], \"å¤‡æ³¨\": [], \"ä¸»ä½“åç§°\": [\"é˜¿æ–¯ç‰¹é‡Œå§†å…¬å¸\"], \"ä»»åŠ¡ç±»åž‹\": []}\n",
      "{\"äººç‰©\": [], \"åœ°ç‚¹\": [], \"è£…å¤‡ç±»åž‹\": [], \"è£…å¤‡\": []}\n",
      "{\"æŒ‡æŒ¥å‘˜\": [], \"å›½å®¶\": [], \"ä¸»ä½“ç±»åž‹\": [\"æ‰«é›·èˆ°\"], \"ç»„ç»‡æœºæž„\": [\"çš‡å®¶æµ·å†›\"]}\n",
      "{\"æ•°é‡\": [], \"å›½åˆ«\": [], \"è£…å¤‡åç§°\": [], \"å•ä½\": []}\n",
      "{\"ä¸»ä½“æ•°é‡\": [\"å…­è‰˜\"], \"å†›ç§\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"è£…å¤‡æ•°é‡\": []}\n",
      "{\"æ—¶é—´\": [\"äº”å¹´æœŸ\"], \"å¼€å§‹æ—¶é—´\": []}\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path 'baichuan-inc/Baichuan2-13B-Chat' \\\n",
    "    --checkpoint_dir 'lora/baichuan13B-iepile-data2text-continue' \\\n",
    "    --model_name 'baichuan' \\\n",
    "    --template 'baichuan2' \\\n",
    "    --do_predict \\\n",
    "    --input_file 'data2text/iepile-ner-test-transformed.json' \\\n",
    "    --output_file 'results/test.json' \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir 'lora/oneke-continue-test' \\\n",
    "    --predict_with_generate \\\n",
    "    --cutoff_len 512 \\\n",
    "    --bf16 \\\n",
    "    --max_new_tokens 300 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543df8ab-d1a7-4a88-b5e5-ee135fc12b89",
   "metadata": {},
   "source": [
    "### å¯¹æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3e7345-301c-4673-b19d-ad566dbb9186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T07:50:43.472124Z",
     "iopub.status.busy": "2024-08-09T07:50:43.471318Z",
     "iopub.status.idle": "2024-08-09T07:50:44.122534Z",
     "shell.execute_reply": "2024-08-09T07:50:44.121038Z",
     "shell.execute_reply.started": "2024-08-09T07:50:43.472065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': {'æ€»æ ·æœ¬æ•°': 215, 'é”™è¯¯æ•°': 0, 'P': 45.35, 'R': 45.8, 'F1': 45.57}}\n"
     ]
    }
   ],
   "source": [
    "!python ie2instruction/eval_func.py \\\n",
    "  --path1 results/baichuan7B-ft-test-output.json \\\n",
    "  --task NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a61b23-ac55-42e5-a6ae-6494380e8c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
