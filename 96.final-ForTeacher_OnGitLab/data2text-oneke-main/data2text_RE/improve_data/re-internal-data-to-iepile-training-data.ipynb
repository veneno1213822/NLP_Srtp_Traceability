{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048196ed-19b0-4b37-ae1a-e2f1a8e762a5",
   "metadata": {},
   "source": [
    "# 训练数据生成\n",
    "进行内部数据转换，生成模型微调所需训练和测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23685558-5054-47f1-9445-20fa15a8b1f4",
   "metadata": {},
   "source": [
    "## 功能性函数封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e790993c-a4e0-4b43-aac3-d6cde5506bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:14:07.456351Z",
     "iopub.status.busy": "2024-08-29T07:14:07.454786Z",
     "iopub.status.idle": "2024-08-29T07:14:07.838629Z",
     "shell.execute_reply": "2024-08-29T07:14:07.837222Z",
     "shell.execute_reply.started": "2024-08-29T07:14:07.456279Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "\n",
    "# 构造标准列名映射\n",
    "schema_mapping = {\n",
    "    \"国别\": \"国家\",\n",
    "    \"主体名称\": \"相关主体\",\n",
    "    \"主体类型\": \"类型\",\n",
    "    \"主体数量\": \"数量\",\n",
    "    \"装备名称\": \"相关主体\",\n",
    "    \"装备类型\": \"类型\",\n",
    "    \"装备数量\": \"数量\",\n",
    "    \"装备\": \"相关主体\",\n",
    "    # \"数量\": \"数量\",\n",
    "    \"开始时间\": \"时间\",\n",
    "    \"任务类型\": \"任务\"\n",
    "}\n",
    "\n",
    "# 构造头实体备选列\n",
    "header_schema = [\"任务\", \"行动\", \"事件\"]\n",
    "\n",
    "# 构造低质量及不需要的schema排除列\n",
    "remove_list = [\"国家\", \"人物\", \"备注\", \"国别\", \"任务\", \"行动\", \"事件\", \"类型\", \"数量\", \"人数\"]\n",
    "\n",
    "retain_list = [\"军种\",\"单位\",\"任务状态\"]\n",
    "\n",
    "# 判断表格内容是否有效（可保留）\n",
    "def retain_entity(value):\n",
    "    if value == \"\" or (\"未提及\" in value) or (\"未明确\" in value) or (\"未提供\" in value) or (\"未知\" in value):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 判断表格内容是不是包含“，、”的长句子\n",
    "def not_sentence(value):\n",
    "    if \"，\" in value or \"、\" in value:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 获取动词最多的分句\n",
    "def get_verb_head(text):\n",
    "    #  使用正则表达式同时根据逗号和顿号拆分句子\n",
    "    # 正则表达式中的 | 表示逻辑或，[^ ] 表示匹配括号内的任意字符\n",
    "    sentences = re.split(r'，|、', text)\n",
    "    # 初始化变量，用于记录动词最多的分句\n",
    "    max_verb_count = 0\n",
    "    verb_sent = \"\"\n",
    "    # 对每个句子进行处理\n",
    "    for sent in sentences:\n",
    "        # 使用jieba进行分词和词性标注\n",
    "        words = pseg.cut(sent)\n",
    "        # 统计当前句子的动词数量\n",
    "        verb_count = sum(1 for word, flag in words if (flag == 'v' or flag == 'vn')) \n",
    "        # 更新动词最多的分句\n",
    "        if verb_count > max_verb_count:\n",
    "            max_verb_count = verb_count\n",
    "            verb_sent = sent     \n",
    "    # 返回动词含量最多的分句\n",
    "    return verb_sent\n",
    "\n",
    "# 判断是不是相同的关系\n",
    "def are_objects_equal(obj1, obj2):\n",
    "    if sorted(obj1.items()) == sorted(obj2.items()):\n",
    "        return True\n",
    "\n",
    "# 去除低质量的数量关系（加和那种）\n",
    "def remove_add_relation(relation_array):\n",
    "    # 结果列表\n",
    "    unique_list = []\n",
    "    # 用于记录已经检查过的head和relation的组合\n",
    "    checked_combinations = set()\n",
    "    # 遍历原始列表\n",
    "    for item in relation_array:\n",
    "        # 检查是否满足剔除条件：head和relation相同，relation为\"数量\"，tail不同\n",
    "        combination = (item[\"head\"], item[\"relation\"])\n",
    "        if item[\"relation\"] == \"数量\" and combination in checked_combinations:\n",
    "            # 如果tail不同，则不添加任何对象\n",
    "            continue\n",
    "        # 添加对象到结果列表\n",
    "        unique_list.append(item)\n",
    "        # 记录这个组合，无论是否满足剔除条件\n",
    "        checked_combinations.add(combination)\n",
    "        # 如果relation为\"数量\"，检查是否有相同head和relation但不同tail的对象\n",
    "        if item[\"relation\"] == \"数量\":\n",
    "            for other_item in relation_array:\n",
    "                if other_item != item and (other_item[\"head\"], other_item[\"relation\"]) == combination and other_item[\"tail\"] != item[\"tail\"]:\n",
    "                    # 找到相同head和relation但不同tail的对象，剔除\n",
    "                    unique_list.pop()\n",
    "                    break   \n",
    "    return unique_list\n",
    "\n",
    "# 在去除加和劣质数据基础上，去除重复关系得到最终的关系数组\n",
    "def get_final_relations(relation_array):\n",
    "    temp_list = remove_add_relation(relation_array)\n",
    "    seen = []\n",
    "    unique_list = []\n",
    "    for item in temp_list:\n",
    "        if not any(are_objects_equal(item, seen_item) for seen_item in seen):\n",
    "            seen.append(item)\n",
    "            unique_list.append(item)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b182a6-98a6-4352-a1d0-51a04caa75e9",
   "metadata": {},
   "source": [
    "## 标注数据文件地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8273dc35-b80d-4f61-aa80-6d33f00225fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:33:37.287324Z",
     "iopub.status.busy": "2024-08-29T07:33:37.286589Z",
     "iopub.status.idle": "2024-08-29T07:33:37.294280Z",
     "shell.execute_reply": "2024-08-29T07:33:37.293077Z",
     "shell.execute_reply.started": "2024-08-29T07:33:37.287272Z"
    }
   },
   "outputs": [],
   "source": [
    "# 从 JSON 文件中读取数据\n",
    "input_file_path = 'data2text-test-remove-rare.json'\n",
    "\n",
    "train_output_file_path = 'remove-rare-train-val.json'\n",
    "\n",
    "test_output_file_path = 'remove-rare-test.json'\n",
    "\n",
    "update_sample_path = 'RE-update-all-v4.json'\n",
    "\n",
    "rare_input_path = 'rare_data.json'\n",
    "\n",
    "rare_sample_path = 'rare-all.json'\n",
    "\n",
    "remove_rare_path = 'remove-rare-all.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce0ac9-f5e9-466b-baa0-264be359faa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 统计不同的表头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802f458-6a82-4a39-bf3c-c0be06df4005",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "表头类型统计\n",
    "表头1：[\"时间\",\"人物\",\"国家\",\"装备名称\",\"装备类型\",\"装备数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\"] 1161\n",
    "- 处理规则：\n",
    "  - [ ] 装备相关schema改为“主体”\n",
    "  - [ ] 以“行动”内容作为事件触发词（“行动”有些内容也是长句子的表述，或者没有意义）\n",
    "    - 规则1：“行动”表格内容可能没有意义，此时转为判断“任务”表格内容是否有意义\n",
    "    - 规则2：“行动”有些内容也是长句子的表述，判断字符串内如果有逗号，则关注“任务”表格内容\n",
    "    - 规则3：接续规则2，若“行动”是长句表述，但“任务”为空，则把“行动”根据逗号分割，取出其中一串作为触发词\n",
    "\n",
    "表头2：[\"时间\",\"人物\",\"国家\",\"主体名称\",\"主体类型\",\"主体数量\",\"组织机构\",\"地点\",\"行动\",\"事件\"] 967\n",
    "- 处理规则：\n",
    "  - [ ] 以“行动”内容作为事件触发词（“行动”有些内容也是长句子的表述，或者没有意义）\n",
    "    - 规则1：“行动”表格内容可能没有意义，此时舍弃该条数据\n",
    "    - 规则2：“行动”有些内容也是长句子的表述，判断字符串内如果有逗号，则把“行动”根据逗号分割，取出其中一串作为触发词\n",
    "\n",
    "表头3：[\"国别\", \"军种\", \"单位\", \"装备\", \"人数\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"] 7\n",
    "- 处理规则：\n",
    "  - [ ] 以“行动”内容作为事件触发词（“行动”有些内容也是长句子的表述，或者没有意义）\n",
    "    - 规则1：“行动”表格内容可能没有意义，此时舍弃该条数据\n",
    "    - 规则2：“行动”有些内容也是长句子的表述，判断字符串内如果有逗号，则把“行动”根据逗号分割，取出其中一串作为触发词\n",
    "  - [ ] “装备”表格内容需要按照分号、逗号进行拆分，分割成多个独立的记录\n",
    "  - [ ] “指挥员”表格内容暂未发现\n",
    "表头4：['国别', '军种', '装备数量', '装备', '地点', '行动', '开始时间', '任务状态'] 2\n",
    "\n",
    "表头5：['国别', '军种', '单位', '装备', '装备数量', '指挥员', '地点', '行动', '开始时间', '任务状态', '任务类型', '备注'] 4\n",
    "\n",
    "表头6：['军种', '数量', '装备', '地点', '行动', '任务状态'] 5\n",
    "\n",
    "表头7：['国别', '军种', '单位', '装备', '数量', '指挥员', '地点', '行动', '开始时间', '任务状态', '任务类型', '备注'] 1\n",
    "\n",
    "表头8：['国别', '军种', '数量', '装备', '地点', '行动', '开始时间', '任务状态'] 1\n",
    "\n",
    "\n",
    "### 需要对单元格内容做分隔处理的数据\n",
    "表头3：[\"国别\", \"军种\", \"单位\", \"装备\", \"人数\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "\n",
    "{   \"text\": \"7月7日，空军第九运输机师的运-20运输机（代号KJ-2000）8架，前往青藏高原某机场进行高原起降训练。此次训练旨在提高运输机在高原复杂环境下的操作能力，保障部队快速机动。7月8日，海军第六驱逐舰支队的大连舰（052D驱逐舰，舷号110）和综保支队的微山湖舰（综合补给舰，舷号887）共328人，前往夏威夷参加环太平洋-2024演习。演习期间，大连舰进行了反舰导弹发射训练。\", \n",
    "    \"event\": [{\"event_trigger\": \"环太平洋-2024演习\", \"event_type\": \"行动\", \"arguments\": [{\"argument\": \"1艘\", \"role\": \"主体数量\"}, {\"argument\": \"052D驱逐舰\", \"role\": \"主体名称\"}]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85bea9f1-08da-46cd-8374-673603946586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T12:55:32.296209Z",
     "iopub.status.busy": "2024-08-11T12:55:32.295740Z",
     "iopub.status.idle": "2024-08-11T12:55:32.344212Z",
     "shell.execute_reply": "2024-08-11T12:55:32.343034Z",
     "shell.execute_reply.started": "2024-08-11T12:55:32.296186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表头8数量： 1\n",
      "数据总数： 2138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "headers = []\n",
    "\n",
    "# headers.append([\"时间\",\"人物\",\"国家\",\"装备名称\",\"装备类型\",\"装备数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\"])\n",
    "count=0\n",
    "count_all=0\n",
    "for data in input_data:\n",
    "    count_all+=1\n",
    "    # 1161\n",
    "    array1 = [\"时间\",\"人物\",\"国家\",\"装备名称\",\"装备类型\",\"装备数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\"]\n",
    "    a1 = np.array(array1)\n",
    "    # 967\n",
    "    array2 = [\"时间\",\"人物\",\"国家\",\"主体名称\",\"主体类型\",\"主体数量\",\"组织机构\",\"地点\",\"行动\",\"事件\"]\n",
    "    a2 = np.array(array2)\n",
    "    # 7\n",
    "    array3 = [\"国别\", \"军种\", \"单位\", \"装备\", \"人数\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "    a3 = np.array(array3)\n",
    "    # 2\n",
    "    array5 = ['国别', '军种', '装备数量', '装备', '地点', '行动', '开始时间', '任务状态']\n",
    "    a5 = np.array(array5)\n",
    "    # 4\n",
    "    array6 = ['国别', '军种', '单位', '装备', '装备数量', '指挥员', '地点', '行动', '开始时间', '任务状态', '任务类型', '备注']\n",
    "    a6 = np.array(array6)\n",
    "    # 5\n",
    "    array7 = ['军种', '数量', '装备', '地点', '行动', '任务状态']\n",
    "    a7 = np.array(array7)\n",
    "    # 1\n",
    "    array8 = ['国别', '军种', '单位', '装备', '数量', '指挥员', '地点', '行动', '开始时间', '任务状态', '任务类型', '备注']\n",
    "    a8 = np.array(array8)\n",
    "    # 1\n",
    "    array9 = ['国别', '军种', '数量', '装备', '地点', '行动', '开始时间', '任务状态']\n",
    "    a9 = np.array(array9)\n",
    "    if isinstance(data, list):\n",
    "        for row in data:\n",
    "            array4 = row['table']['header']\n",
    "            a4 = np.array(array4)\n",
    "            if np.array_equal(a9, a4):\n",
    "                count+=1\n",
    "            # if np.array_equal(a1, a4) or np.array_equal(a2, a4) or np.array_equal(a3, a4) or np.array_equal(a5, a4) or np.array_equal(a6, a4) or np.array_equal(a7, a4):\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     headers.append(array4)\n",
    "\n",
    "\n",
    "\n",
    "                # array1 = array2\n",
    "                # a1 = np.array(array1)\n",
    "    # if isinstance(data, list):\n",
    "    #     for row in data:\n",
    "    #         array4 = row['table']['header']\n",
    "    #         a4 = np.array(array4)\n",
    "    #         if np.array_equal(a1, a4):\n",
    "    #             pass\n",
    "    #         else:\n",
    "    #             headers.append(array4)\n",
    "    #             array1 = array4.copy()\n",
    "    #             a1 = np.array(array1)     \n",
    "\n",
    "\n",
    "# 打印集合内容\n",
    "# print(\"不同的 表头 有：\", headers)\n",
    "print(\"表头8数量：\", count)\n",
    "print(\"数据总数：\", count_all)\n",
    "# 打印总共有多少种不同的 entity_type\n",
    "# print(\"共有\", len(entity_types), \"种不同的 entity_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77036a6-92b5-4eba-b94d-a19251b42fc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 生成训练集和测试集（EE）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823f0163-7cca-4d11-be8f-97b4546d7d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T08:33:43.245339Z",
     "iopub.status.busy": "2024-08-12T08:33:43.243913Z",
     "iopub.status.idle": "2024-08-12T08:33:43.503021Z",
     "shell.execute_reply": "2024-08-12T08:33:43.502071Z",
     "shell.execute_reply.started": "2024-08-12T08:33:43.245273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_val_set = []\n",
    "test_set = []\n",
    "all_set = []\n",
    "\n",
    "header1 = [\"时间\",\"人物\",\"国家\",\"装备名称\",\"装备类型\",\"装备数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\"]\n",
    "header2 = [\"时间\",\"人物\",\"国家\",\"主体名称\",\"主体类型\",\"主体数量\",\"组织机构\",\"地点\",\"行动\",\"事件\"]\n",
    "header3 = [\"国别\", \"军种\", \"单位\", \"装备\", \"数量\", \"人数\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "header4 = [\"国别\", \"军种\", \"装备数量\", \"装备\", \"地点\", \"行动\", \"开始时间\", \"任务状态\"]\n",
    "header5 = [\"国别\", \"军种\", \"单位\", \"装备\", \"装备数量\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "header6 = [\"军种\", \"数量\", \"装备\", \"地点\", \"行动\", \"任务状态\"]\n",
    "header7 = [\"国别\", \"军种\", \"单位\", \"装备\", \"数量\", \"指挥员\", \"地点\", \"行动\", \"开始时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "header8 = [\"国别\", \"军种\", \"数量\", \"装备\", \"地点\", \"行动\", \"开始时间\", \"任务状态\"]\n",
    "\n",
    "# 遍历输入数据中的每个data项\n",
    "for excel in input_data:\n",
    "    for sheet in excel:\n",
    "        events = []\n",
    "        relation_type = \"\"\n",
    "        update_header = [\"时间\",\"人物\",\"国家\",\"主体名称\",\"主体类型\",\"主体数量\",\"组织机构\",\"地点\",\"行动\",\"事件\"]\n",
    "        # 判断当前的表头类型\n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header1)):\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            update_header = [\"时间\",\"人物\",\"国家\",\"主体名称\",\"主体类型\",\"主体数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\"]\n",
    "            # 获取可能的中心词 行动、任务 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            head_index_task = sheet['table']['header'].index(\"任务\")\n",
    "\n",
    "            # 遍历data中的每一行\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 、 任务 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                task = row_value[head_index_task]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         # 行动 有意义但是长句子，则先看一下 任务 单元格内容是否有效\n",
    "                         if retain_entity(task) and not_sentence(task):\n",
    "                             # 任务 单元格有效且不是长句子，则把任务内容作为trigger\n",
    "                             event_trigger = task\n",
    "                         else:\n",
    "                             # 任务 单元格无效或是长句子则回头拆分 行动 单元格内容,近似选择一段字符串\n",
    "                             items = action.split('，')\n",
    "                             event_trigger = items[0]\n",
    "                # 行动单元格无意义，判断 任务 单元格内容\n",
    "                else:\n",
    "                    if retain_entity(task):\n",
    "                        if not_sentence(task):\n",
    "                             # 任务 单元格有效且不是长句子，则把任务内容作为trigger\n",
    "                             event_trigger = task\n",
    "                        else:\n",
    "                             # 任务 单元格有效但是长句子，分割后任务内容作为trigger\n",
    "                             items = task.split('，')\n",
    "                             event_trigger = items[0]\n",
    "                    else:\n",
    "                        # 行动和任务单元格都没意义，这条数据舍弃\n",
    "                        pass\n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国家\", \"人物\", \"任务\", \"行动\", \"事件\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "        \n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header2)):\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            # 获取可能的中心词 行动 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         items = action.split('，')\n",
    "                         event_trigger = items[0]\n",
    "                else:\n",
    "                    # 行动单元格都没意义，这条数据舍弃\n",
    "                    pass\n",
    "    \n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国家\", \"人物\", \"任务\", \"行动\", \"事件\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "        \n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header3)):\n",
    "            update_header = [\"国别\", \"军种\", \"单位\", \"主体名称\", \"主体数量\", \"人数\", \"指挥员\", \"地点\", \"行动\", \"时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            # 获取可能的中心词 行动 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         items = action.split('，')\n",
    "                         event_trigger = items[0]\n",
    "                else:\n",
    "                    # 行动单元格都没意义，这条数据舍弃\n",
    "                    pass\n",
    "    \n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国别\", \"人物\", \"任务\", \"行动\", \"事件\", \"备注\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "        \n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header4)):\n",
    "            update_header = [\"国别\", \"军种\", \"主体数量\", \"主体名称\", \"地点\", \"行动\", \"时间\", \"任务状态\"]\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            # 获取可能的中心词 行动 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         items = action.split('，')\n",
    "                         event_trigger = items[0]\n",
    "                else:\n",
    "                    # 行动单元格都没意义，这条数据舍弃\n",
    "                    pass\n",
    "    \n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国别\", \"人物\", \"任务\", \"行动\", \"事件\", \"备注\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "\n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header5)):\n",
    "            update_header = [\"国别\", \"军种\", \"单位\", \"主体名称\", \"主体数量\", \"指挥员\", \"地点\", \"行动\", \"时间\", \"任务状态\", \"任务类型\", \"备注\"]\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            # 获取可能的中心词 行动 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         items = action.split('，')\n",
    "                         event_trigger = items[0]\n",
    "                else:\n",
    "                    # 行动单元格都没意义，这条数据舍弃\n",
    "                    pass\n",
    "    \n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国别\", \"人物\", \"任务\", \"行动\", \"事件\", \"备注\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "\n",
    "        if np.array_equal(np.array(sheet['table']['header']),np.array(header6)):\n",
    "            update_header = [\"军种\", \"主体数量\", \"主体名称\", \"地点\", \"行动\", \"任务状态\"]\n",
    "            # 统一一下装备和主体的命名，尽量减少实体数量，但整体结构不会变动\n",
    "            # 获取可能的中心词 行动 的下标\n",
    "            head_index_action = sheet['table']['header'].index(\"行动\")\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                arguments = []\n",
    "                event_trigger = \"\"\n",
    "                event_type = \"事件\"\n",
    "                # 获取 行动 的表格内容\n",
    "                action = row_value[head_index_action]\n",
    "                # 判断 行动 表格内容是否有意义\n",
    "                if retain_entity(action):\n",
    "                    # 有意义则进一步判断是不是长句子\n",
    "                     if not_sentence(action):\n",
    "                         # 不是长句子，直接把内容作为trigger\n",
    "                         event_trigger = action\n",
    "                     else:\n",
    "                         items = action.split('，')\n",
    "                         event_trigger = items[0]\n",
    "                else:\n",
    "                    # 行动单元格都没意义，这条数据舍弃\n",
    "                    pass\n",
    "    \n",
    "                if event_trigger != \"\":\n",
    "                    # 确定触发词有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        header_index = row_value.index(value)\n",
    "                        entity_type = update_header[header_index]\n",
    "                        if entity_type not in [\"国别\", \"人物\", \"任务\", \"行动\", \"事件\", \"备注\"]:\n",
    "                            if retain_entity(value):\n",
    "                                arguments.append({\n",
    "                                    \"argument\": value,\n",
    "                                    \"role\": entity_type})\n",
    "                    events.append({\n",
    "                        \"event_trigger\": event_trigger,\n",
    "                        \"event_type\": event_type,\n",
    "                        \"arguments\": arguments\n",
    "                    })\n",
    "                # 创建输出格式\n",
    "                if len(events):\n",
    "                    output = {\"text\": sheet[\"text\"], \"event\": events}\n",
    "                    all_set.append(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打乱数组顺序\n",
    "np.random.shuffle(all_set)\n",
    "\n",
    "# 计算分割点\n",
    "split_index = int(len(all_set) * 0.9)\n",
    "print(len(all_set))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_val_set = all_set[:split_index]\n",
    "test_set = all_set[split_index:]\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "output_file_path = os.path.join(current_path, '../iepile-ee-train-val.json')\n",
    "test_file_path = os.path.join(current_path, '../iepile-ee-test.json')\n",
    "\n",
    "# 打开输出路径\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "    json.dump(train_val_set, file1, ensure_ascii=False, indent=1)\n",
    "with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    json.dump(test_set, file2, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e618eb-b74c-4dd6-99bd-c1c701df5e28",
   "metadata": {},
   "source": [
    "## 生成测试数据函数封装-RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a84f08-373c-4718-b9ae-afa1a5081276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:40:11.254230Z",
     "iopub.status.busy": "2024-08-29T07:40:11.253191Z",
     "iopub.status.idle": "2024-08-29T07:40:11.273457Z",
     "shell.execute_reply": "2024-08-29T07:40:11.272697Z",
     "shell.execute_reply.started": "2024-08-29T07:40:11.254152Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_premanage(input_path, standard_schema_map, header_schema, remove_list, train_output_path, test_output_path):\n",
    "# def data_premanage(input_path, standard_schema_map, header_schema, remove_list, train_output_path):\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        input_data = json.load(file)\n",
    "\n",
    "    train_val_set = []\n",
    "    test_set = []\n",
    "    all_set = []\n",
    "\n",
    "    # 遍历输入数据中的每个data项\n",
    "    for excel in input_data:\n",
    "        for sheet in excel:\n",
    "            # 获取原始表头，并根据标准列映射进行替换，更新得到标准表头\n",
    "            header = sheet['table']['header']\n",
    "            for i in range(len(header)):\n",
    "                for key, value in standard_schema_map.items():\n",
    "                    if header[i] == key:\n",
    "                        header[i] = value\n",
    "            relations = []\n",
    "            # 遍历data中的每一行\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                head_type = \"\",\n",
    "                head_value = \"\",\n",
    "                # 获取可能的中心词的下标( 任务 > 行动 > 事件)\n",
    "                for i in range(len(header_schema)):\n",
    "                # 从任务开始，如果数据表头中有任务，就先取出任务内容进行判断，后面不行再用i+1\n",
    "                    if header_schema[i] in header:\n",
    "                    # 第一个能对应上的候选头实体，拿到其 类型type 和 在当前表头中的数组下标index\n",
    "                        head_index = header.index(header_schema[i])\n",
    "                        head_type = header_schema[i]\n",
    "                        # 判断 候选头实体的初始值value 是否有意义\n",
    "                        if retain_entity(head_value):\n",
    "                        # 有意义，则用刚才找到的index对应找到 候选头实体的初始值value\n",
    "                            head_value = row_value[head_index]\n",
    "                            # 进一步判断是不是长句子\n",
    "                            if not_sentence(head_value):\n",
    "                                # 不是长句子，不用再做其他处理，value可直接作为头实体\n",
    "                                pass\n",
    "                            else:\n",
    "                                # 候选头实体的初始值value 有意义但是长句子，则分句后选择动词最多的字段作为头实体\n",
    "                                head_value = get_verb_head(head_value)\n",
    "                        else:\n",
    "                            pass\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                # 开始构造数据\n",
    "                if retain_entity(head_value):\n",
    "                     # 确定头实体有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        tail_type_index = row_value.index(value)\n",
    "                        tail_type = header[tail_type_index]\n",
    "                        if tail_type not in remove_list:\n",
    "                            if retain_entity(value):\n",
    "                                if tail_type == \"相关主体\":\n",
    "                                    if \"，\" in value or \"；\" in value:\n",
    "                                        records = re.split(r'；', value)\n",
    "                                        for record in records:\n",
    "                                            single_name_value = re.split(r'，', record)[0]\n",
    "                                            single_num_value = re.split(r'，', record)[1]\n",
    "                                            relations.append({\n",
    "                                                \"head\": head_value,\n",
    "                                                \"relation\": tail_type,\n",
    "                                                \"tail\": single_name_value})\n",
    "                                            # relations.append({\n",
    "                                            #     \"head\": single_name_value,\n",
    "                                            #     \"relation\": \"数量\",\n",
    "                                            #     \"tail\": single_num_value})\n",
    "                                    else:\n",
    "                                        relations.append({\n",
    "                                                \"head\": head_value,\n",
    "                                                \"relation\": tail_type,\n",
    "                                                \"tail\": value})\n",
    "                                elif tail_type == \"时间\":\n",
    "                                    if value in sheet[\"text\"]:\n",
    "                                        relations.append({\n",
    "                                            \"head\": head_value,\n",
    "                                            \"relation\": tail_type,\n",
    "                                            \"tail\": value})\n",
    "                                else:    \n",
    "                                    relations.append({\n",
    "                                        \"head\": head_value,\n",
    "                                        \"relation\": tail_type,\n",
    "                                        \"tail\": value})\n",
    "                \n",
    "                # 对于 主体名称-主体数量、主体名称-主体类型 两个关系，单独进行构造\n",
    "                # 先拿到 主体名称 表格内容\n",
    "                subject_index = header.index(\"相关主体\")\n",
    "                subject_value = row_value[subject_index]\n",
    "                # 如果 主体名称 有效，进一步考虑构造\n",
    "                if retain_entity(subject_value):\n",
    "                    # 依次获取两个尾实体表格内容\n",
    "                    for subject_tail in [\"类型\", \"数量\"]:\n",
    "                        if subject_tail in header:\n",
    "                            subject_tail_index = header.index(subject_tail)\n",
    "                            subject_tail_value = row_value[subject_tail_index]\n",
    "                            # 表格内容有效则构建一条关系\n",
    "                            if retain_entity(subject_tail_value):\n",
    "                                if subject_tail == \"类型\":\n",
    "                                    relations.append({\n",
    "                                            \"head\": subject_value,\n",
    "                                            \"relation\": subject_tail,\n",
    "                                            \"tail\": subject_tail_value})\n",
    "                                # if subject_tail == \"数量\" and subject_tail_value in sheet[\"text\"]:\n",
    "                                #     relations.append({\n",
    "                                #             \"head\": subject_value,\n",
    "                                #             \"relation\": subject_tail,\n",
    "                                #             \"tail\": subject_tail_value})\n",
    "\n",
    "            final_relations = get_final_relations(relations)\n",
    "            # 创建输出格式\n",
    "            if len(relations):\n",
    "                output = {\"text\": sheet[\"text\"], \"relation\": final_relations}\n",
    "                all_set.append(output)\n",
    "\n",
    "    # 打乱数组顺序\n",
    "    np.random.shuffle(all_set)\n",
    "\n",
    "    # 计算分割点\n",
    "    split_index = int(len(all_set) * 0.9)\n",
    "    print(len(all_set))\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    train_val_set = all_set[:split_index]\n",
    "    test_set = all_set[split_index:]\n",
    "\n",
    "    # 获取当前工作目录\n",
    "    current_path = os.getcwd()\n",
    "    output_file_path = os.path.join(current_path, train_output_path)\n",
    "    test_file_path = os.path.join(current_path, test_output_path)\n",
    "\n",
    "    # 打开输出路径\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "        json.dump(train_val_set, file1, ensure_ascii=False, indent=1)\n",
    "    with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "        json.dump(test_set, file2, ensure_ascii=False, indent=1)\n",
    "\n",
    "\n",
    "\n",
    "    print(len(all_set))\n",
    "    # # 获取当前工作目录\n",
    "    # current_path = os.getcwd()\n",
    "    # output_file_path = os.path.join(current_path, train_output_path)\n",
    "    # # test_file_path = os.path.join(current_path, test_output_path)\n",
    "\n",
    "    # # 打开输出路径\n",
    "    # with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "    #     json.dump(all_set, file1, ensure_ascii=False, indent=1)\n",
    "    # # with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    # #     json.dump(test_set, file2, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78afdb-b92a-41a9-a9ca-5d0a3d37ed42",
   "metadata": {},
   "source": [
    "## rare_data处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78648d2b-1d12-4856-888f-15831e04951f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:15:52.586475Z",
     "iopub.status.busy": "2024-08-29T07:15:52.585532Z",
     "iopub.status.idle": "2024-08-29T07:15:52.604045Z",
     "shell.execute_reply": "2024-08-29T07:15:52.603179Z",
     "shell.execute_reply.started": "2024-08-29T07:15:52.586413Z"
    }
   },
   "outputs": [],
   "source": [
    "# def data_premanage(input_path, standard_schema_map, header_schema, remove_list, train_output_path, test_output_path):\n",
    "def data_premanage(input_path, standard_schema_map, header_schema, retain_list, train_output_path):\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as file:\n",
    "        input_data = json.load(file)\n",
    "\n",
    "    train_val_set = []\n",
    "    test_set = []\n",
    "    all_set = []\n",
    "\n",
    "    # 遍历输入数据中的每个data项\n",
    "    for excel in input_data:\n",
    "        for sheet in excel:\n",
    "            # 获取原始表头，并根据标准列映射进行替换，更新得到标准表头\n",
    "            header = sheet['table']['header']\n",
    "            for i in range(len(header)):\n",
    "                for key, value in standard_schema_map.items():\n",
    "                    if header[i] == key:\n",
    "                        header[i] = value\n",
    "            relations = []\n",
    "            # 遍历data中的每一行\n",
    "            for row, row_value in sheet['table']['data'].items(): \n",
    "                head_type = \"\",\n",
    "                head_value = \"\",\n",
    "                # 获取可能的中心词的下标( 任务 > 行动 > 事件)\n",
    "                for i in range(len(header_schema)):\n",
    "                # 从任务开始，如果数据表头中有任务，就先取出任务内容进行判断，后面不行再用i+1\n",
    "                    if header_schema[i] in header:\n",
    "                    # 第一个能对应上的候选头实体，拿到其 类型type 和 在当前表头中的数组下标index\n",
    "                        head_index = header.index(header_schema[i])\n",
    "                        head_type = header_schema[i]\n",
    "                        # 判断 候选头实体的初始值value 是否有意义\n",
    "                        if retain_entity(head_value):\n",
    "                        # 有意义，则用刚才找到的index对应找到 候选头实体的初始值value\n",
    "                            head_value = row_value[head_index]\n",
    "                            # 进一步判断是不是长句子\n",
    "                            if not_sentence(head_value):\n",
    "                                # 不是长句子，不用再做其他处理，value可直接作为头实体\n",
    "                                pass\n",
    "                            else:\n",
    "                                # 候选头实体的初始值value 有意义但是长句子，则分句后选择动词最多的字段作为头实体\n",
    "                                head_value = get_verb_head(head_value)\n",
    "                        else:\n",
    "                            pass\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                # 开始构造数据\n",
    "                if retain_entity(head_value):\n",
    "                     # 确定头实体有效才开始构建数据\n",
    "                    for value in row_value:\n",
    "                        # 对应schema和单元格内容value\n",
    "                        tail_type_index = row_value.index(value)\n",
    "                        tail_type = header[tail_type_index]\n",
    "                        if tail_type in retain_list:\n",
    "                            if retain_entity(value):  \n",
    "                                relations.append({\n",
    "                                    \"head\": head_value,\n",
    "                                    \"relation\": tail_type,\n",
    "                                    \"tail\": value})\n",
    "            \n",
    "            final_relations = get_final_relations(relations)\n",
    "            # 创建输出格式\n",
    "            if len(relations):\n",
    "                output = {\"text\": sheet[\"text\"], \"relation\": final_relations}\n",
    "                all_set.append(output)\n",
    "\n",
    "    # # 打乱数组顺序\n",
    "    # np.random.shuffle(all_set)\n",
    "\n",
    "    # # 计算分割点\n",
    "    # split_index = int(len(all_set) * 0.9)\n",
    "    # print(len(all_set))\n",
    "\n",
    "    # # 划分训练集和测试集\n",
    "    # train_val_set = all_set[:split_index]\n",
    "    # test_set = all_set[split_index:]\n",
    "\n",
    "    # # 获取当前工作目录\n",
    "    # current_path = os.getcwd()\n",
    "    # output_file_path = os.path.join(current_path, train_output_path)\n",
    "    # test_file_path = os.path.join(current_path, test_output_path)\n",
    "\n",
    "    # # 打开输出路径\n",
    "    # with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "    #     json.dump(train_val_set, file1, ensure_ascii=False, indent=1)\n",
    "    # with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    #     json.dump(test_set, file2, ensure_ascii=False, indent=1)\n",
    "\n",
    "\n",
    "\n",
    "    print(len(all_set))\n",
    "    # 获取当前工作目录\n",
    "    current_path = os.getcwd()\n",
    "    output_file_path = os.path.join(current_path, train_output_path)\n",
    "    # test_file_path = os.path.join(current_path, test_output_path)\n",
    "\n",
    "    # 打开输出路径\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "        json.dump(all_set, file1, ensure_ascii=False, indent=1)\n",
    "    # with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    #     json.dump(test_set, file2, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e1f03a-4dd2-4836-bdcd-07f05e1a29b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:11:58.382102Z",
     "iopub.status.busy": "2024-08-29T08:11:58.380935Z",
     "iopub.status.idle": "2024-08-29T08:11:58.406698Z",
     "shell.execute_reply": "2024-08-29T08:11:58.405154Z",
     "shell.execute_reply.started": "2024-08-29T08:11:58.382033Z"
    }
   },
   "outputs": [],
   "source": [
    "input_path = 'rare-test.json'\n",
    "output_file_path = 'enhance-rare-test-all.json'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "np.random.shuffle(input_data)\n",
    "# copied_data = []\n",
    "\n",
    "# for item in input_data:\n",
    "#     for _ in range(56):\n",
    "#         copied_data.append(item)\n",
    "\n",
    "# print(len(copied_data))\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "    json.dump(input_data, file1, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c8db9-3e8e-4ac8-8948-e7d374ca7e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "660de070-4f10-4e0c-b635-08b8b627f88f",
   "metadata": {},
   "source": [
    "## 运行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de473b70-2fb1-4436-a41d-9edd312872e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:40:27.504823Z",
     "iopub.status.busy": "2024-08-29T07:40:27.503942Z",
     "iopub.status.idle": "2024-08-29T07:40:27.733432Z",
     "shell.execute_reply": "2024-08-29T07:40:27.732625Z",
     "shell.execute_reply.started": "2024-08-29T07:40:27.504759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763\n",
      "1763\n"
     ]
    }
   ],
   "source": [
    "data_premanage(input_file_path, schema_mapping, header_schema, remove_list, train_output_file_path, test_output_file_path)\n",
    "\n",
    "# data_premanage(input_file_path, schema_mapping, header_schema, remove_list, remove_rare_path)\n",
    "\n",
    "#data_premanage(rare_input_path, schema_mapping, header_schema, retain_list, rare_sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd784a-c922-4b49-b692-b6f8d4da08ad",
   "metadata": {},
   "source": [
    "## 转变为jsonl格式并直接重命名为json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f0bd245-6e83-456d-ab3a-2c0045433ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:34:10.405694Z",
     "iopub.status.busy": "2024-08-29T08:34:10.404835Z",
     "iopub.status.idle": "2024-08-29T08:34:10.468055Z",
     "shell.execute_reply": "2024-08-29T08:34:10.467306Z",
     "shell.execute_reply.started": "2024-08-29T08:34:10.405640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renamed from enhance-rare-train-val-all.jsonl to enhance-rare-train-val-json.json\n",
      "File renamed from enhance-rare-test-all.jsonl to enhance-rare-test-json.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "\n",
    "def rename_file(old_name, new_name):\n",
    "    # 确保旧文件存在\n",
    "    if os.path.exists(old_name):\n",
    "        # 重命名文件\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f\"File renamed from {old_name} to {new_name}\")\n",
    "    else:\n",
    "        print(f\"File {old_name} does not exist.\")\n",
    "\n",
    "\n",
    "train_json_file = \"enhance-rare-train-val-all.json\"\n",
    "train_jsonl_file = \"enhance-rare-train-val-all.jsonl\"\n",
    "test_json_file = \"enhance-rare-test-all.json\"\n",
    "test_jsonl_file = \"enhance-rare-test-all.jsonl\"\n",
    "\n",
    "with open(train_json_file, \"r\", encoding='utf-8') as file:\n",
    "    with jsonlines.open(train_jsonl_file, 'w') as writer:\n",
    "        input_data = json.load(file)\n",
    "        for data in input_data:\n",
    "            writer.write(data)\n",
    "\n",
    "with open(test_json_file, \"r\", encoding='utf-8') as file:\n",
    "    with jsonlines.open(test_jsonl_file, 'w') as writer:\n",
    "        input_data = json.load(file)\n",
    "        for data in input_data:\n",
    "            writer.write(data)\n",
    "\n",
    "rename_file(\"enhance-rare-train-val-all.jsonl\", \"enhance-rare-train-val-json.json\")\n",
    "rename_file(\"enhance-rare-test-all.jsonl\", \"enhance-rare-test-json.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a7689-2849-4be7-980a-dcc51554f857",
   "metadata": {},
   "source": [
    "## 生成测试集（按照单个schema）\n",
    "1. 按照单个schema生成测试集，目前其他测试方法优先级更高，此处代码保留但不作运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07463eab-999b-4dfb-b084-69e47262b065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:06:30.274110Z",
     "iopub.status.busy": "2024-08-02T08:06:30.273342Z",
     "iopub.status.idle": "2024-08-02T08:06:30.355517Z",
     "shell.execute_reply": "2024-08-02T08:06:30.354641Z",
     "shell.execute_reply.started": "2024-08-02T08:06:30.274070Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_file_path_test = 'NERsample_test.json'\n",
    "\n",
    "with open(input_file_path_test, 'r', encoding='utf-8') as file:\n",
    "    input_data_test = json.load(file)\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "output_file_path = os.path.join(current_path, 'schema_NERsample_test.json')\n",
    "\n",
    "output_data_test = []\n",
    "# 遍历输入数据中的每个data项\n",
    "for entity_type in [\"时间\", \"人物\", \"国家\", \"装备名称\", \"装备类型\", \"装备数量\", \"组织机构\", \"地点\", \"任务\", \"行动\", \"事件\"]:\n",
    "    output_file_path = os.path.join(current_path, entity_type + 'NERsample_test.json')\n",
    "    for data in input_data_test:\n",
    "        entities = []\n",
    "        for item in data['entity']:\n",
    "            if item[\"entity_type\"] == entity_type:\n",
    "                entities.append(item)\n",
    "                output = {\"text\": data[\"text\"], \"entity\": entities}\n",
    "                output_data_test.append(output)\n",
    "    # 打开输出路径\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output_data_test, file, ensure_ascii=False,indent=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac02515-d67e-4429-ac93-34cebbd024b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:32:31.099713Z",
     "iopub.status.busy": "2024-08-02T08:32:31.098863Z",
     "iopub.status.idle": "2024-08-02T08:32:31.113046Z",
     "shell.execute_reply": "2024-08-02T08:32:31.112169Z",
     "shell.execute_reply.started": "2024-08-02T08:32:31.099653Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_file_path_test = 'NERsample_test.json'\n",
    "\n",
    "with open(input_file_path_test, 'r', encoding='utf-8') as file:\n",
    "    input_data_test = json.load(file)\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "\n",
    "output_data_test = []\n",
    "# 遍历输入数据中的每个data项\n",
    "\n",
    "output_file_path = os.path.join(current_path,  '装备数量NERsample_test.json')\n",
    "for data in input_data_test:\n",
    "    for item in data['entity']:\n",
    "        if item[\"entity_type\"] == \"装备数量\":\n",
    "            entities = []\n",
    "            entities.append(item)\n",
    "            output = {\"text\": data[\"text\"], \"entity\": entities}\n",
    "            output_data_test.append(output)\n",
    "# 打开输出路径\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(output_data_test, file, ensure_ascii=False,indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b6ed53-fce6-4287-a47e-072fe8036bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T06:36:08.019618Z",
     "iopub.status.busy": "2024-08-02T06:36:08.018733Z",
     "iopub.status.idle": "2024-08-02T06:36:08.029005Z",
     "shell.execute_reply": "2024-08-02T06:36:08.028498Z",
     "shell.execute_reply.started": "2024-08-02T06:36:08.019556Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, 23)]\n"
     ]
    }
   ],
   "source": [
    "def find_all_spans(source_text, substring):\n",
    "    length = len(substring)\n",
    "    spans = []\n",
    "    for i in range(len(source_text) - length + 1):\n",
    "        if source_text[i:i+length] == substring:\n",
    "            spans.append((i, i + length))\n",
    "    return spans\n",
    "\n",
    "\n",
    "source_text = \"这是一个测试文本，这包含一些用于测试的子字符串。\"\n",
    "substring = \"字符串\"\n",
    "\n",
    "spans = find_all_spans(source_text, substring)\n",
    "print(spans)\n",
    "# span = find_span(source_text, substring)\n",
    "# print(span)  # 输出子字符串的位置span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98428d-231c-4782-aa8b-dd69bc202d18",
   "metadata": {},
   "source": [
    "### 将训练数据转换为模型微调使用的数据格式\n",
    "运行格式转换脚本进行格式转换，其中指令含义如下：\n",
    "- --src_path data2text/iepile-ner-train-val-json.json\n",
    "   - 训练数据的路径\n",
    "- --tgt_path data2text/iepile-ner-train-val-transformed.json\n",
    "   - 数据转换之后文件输出的路径\n",
    "- --language zh\n",
    "   - 选择zh语言\n",
    "- --task NER\n",
    "   - 选择实体抽取任务\n",
    "- --split_num 4\n",
    "   - 定义单个指令中可包含的最大schema数目\n",
    "- --random_sort\n",
    "   - 是否对指令中的schema随机排序，默认为false\n",
    "- --split train\n",
    "   - 指定数据集类型为train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db1583d-ee4a-487c-bedc-d687a04280de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:39:33.508322Z",
     "iopub.status.busy": "2024-08-29T08:39:33.507410Z",
     "iopub.status.idle": "2024-08-29T08:39:33.905350Z",
     "shell.execute_reply": "2024-08-29T08:39:33.903608Z",
     "shell.execute_reply.started": "2024-08-29T08:39:33.508252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/data2text-oneke/data2text_RE/improve_data/../../ie2instruction/convert_func.py\", line 170, in <module>\n",
      "    process(options)\n",
      "  File \"/root/data2text-oneke/data2text_RE/improve_data/../../ie2instruction/convert_func.py\", line 107, in process\n",
      "    options.source = options.src_path.split('/')[-2]  # 用源路径的最后一个文件夹名作为source\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python ../../ie2instruction/convert_func.py \\\n",
    "    --src_path improve_data/enhance-rare-train-val-json.json \\\n",
    "    --tgt_path improve_data/enhance-rare-train-val-transformed.json \\\n",
    "    --schema_path improve_data/schema.json \\\n",
    "    --language zh \\\n",
    "    --task RE \\\n",
    "    --split_num 6 \\\n",
    "    --random_sort \\\n",
    "    --split train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fe99e-8a03-42b5-9177-269ffb2f014c",
   "metadata": {},
   "source": [
    "### 拆分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aeac3ec-7259-49eb-8e39-35912461af01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:34:56.213340Z",
     "iopub.status.busy": "2024-08-18T11:34:56.212496Z",
     "iopub.status.idle": "2024-08-18T11:34:56.305387Z",
     "shell.execute_reply": "2024-08-18T11:34:56.304929Z",
     "shell.execute_reply.started": "2024-08-18T11:34:56.213279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和验证集已生成：\n",
      "训练集大小：3491\n",
      "验证集大小：873\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# 读取JSONL文件\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    random.shuffle(data)  # 随机打乱数据\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "# 写入JSONL文件\n",
    "def write_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for entry in data:\n",
    "            file.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(input_file, train_file, val_file):\n",
    "    data = read_jsonl(input_file)\n",
    "    train_data, val_data = split_data(data)\n",
    "    write_jsonl(train_data, train_file)\n",
    "    write_jsonl(val_data, val_file)\n",
    "    print(f\"训练集和验证集已生成：\\n训练集大小：{len(train_data)}\\n验证集大小：{len(val_data)}\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "input_file = 'newdata/iepile-re-train-val-transformed.json'  # 输入的JSONL文件\n",
    "train_file = 'newdata/iepile-re-train-transformed.json'  # 训练集文件\n",
    "val_file = 'newdata/iepile-re-val-transformed.json'      # 验证集文件\n",
    "\n",
    "main(input_file, train_file, val_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37506ec2-e4bc-4d31-944b-7b7530d4bba0",
   "metadata": {},
   "source": [
    "### 将测试数据转换为测试使用的数据格式\n",
    "运行格式转换脚本进行格式转换，其中指令含义如下：\n",
    "- --src_path data2text/iepile-ner-test-json.json\n",
    "   - 训练数据的路径\n",
    "- --tgt_path data2text/iepile-ner-test-transformed.json\n",
    "   - 数据转换之后文件输出的路径\n",
    "- --language zh\n",
    "   - 选择zh语言\n",
    "- --task NER\n",
    "   - 选择实体抽取任务\n",
    "- --split_num 4\n",
    "   - 定义单个指令中可包含的最大schema数目\n",
    "- --random_sort\n",
    "   - 是否对指令中的schema随机排序，默认为false\n",
    "- --split test\n",
    "   - 指定数据集类型为train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2541ae1a-61ef-4a5a-8fab-f42cb65d88e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:19:23.521278Z",
     "iopub.status.busy": "2024-08-19T13:19:23.520001Z",
     "iopub.status.idle": "2024-08-19T13:19:23.919631Z",
     "shell.execute_reply": "2024-08-19T13:19:23.917734Z",
     "shell.execute_reply.started": "2024-08-19T13:19:23.521217Z"
    }
   },
   "outputs": [],
   "source": [
    "!python ../../ie2instruction/convert_func.py \\\n",
    "    --src_path newdata/test_demo.json \\\n",
    "    --tgt_path newdata/test_demo-transformed.json \\\n",
    "    --schema_path newdata/schema_test.json \\\n",
    "    --language zh \\\n",
    "    --task RE \\\n",
    "    --split_num 6 \\\n",
    "    --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a9b3d-4aaa-458e-838c-b3cdf94a531b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 将测试输出每6条合并\n",
    "测试集的输出因为split_num = 4，故每条文本被拆分为6条数据输出，我们需要将其合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a557c2ab-f8b7-4d63-a890-db72335ebd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T12:50:34.921147Z",
     "iopub.status.busy": "2024-08-09T12:50:34.920492Z",
     "iopub.status.idle": "2024-08-09T12:50:34.961238Z",
     "shell.execute_reply": "2024-08-09T12:50:34.960345Z",
     "shell.execute_reply.started": "2024-08-09T12:50:34.921108Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def merge_dicts(dict_list):\n",
    "    \"\"\"合并多个字典，将相同键的值合并为一个列表\"\"\"\n",
    "    merged_dict = {}\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            if key in merged_dict:\n",
    "                merged_dict[key].extend(value)\n",
    "            else:\n",
    "                merged_dict[key] = value\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def process_jsonl_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    result = []\n",
    "    temp_dict_list = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        data = json.loads(line)\n",
    "        output_data = json.loads(data['output'])\n",
    "        temp_dict_list.append(output_data)\n",
    "\n",
    "        # 每6条数据合并一次\n",
    "        if (i + 1) % 6 == 0 or i == len(lines) - 1:\n",
    "            merged_output = merge_dicts(temp_dict_list)\n",
    "            result.append({\"result\": merged_output})\n",
    "            temp_dict_list = []\n",
    "\n",
    "    # 将结果写入JSON文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "input_file = 'results/baichuan7B-ft-test-output.json'\n",
    "output_file = 'results/baichuan7B-ft-test-output-merged.json'\n",
    "\n",
    "process_jsonl_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa17f4-db30-4045-8f61-c6bacdbc530f",
   "metadata": {},
   "source": [
    "## 单条分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29069a-9dfa-433e-8765-e4cf6b99416b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2d110-aa27-49aa-814e-6b338fdb2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "            \"table\": {\n",
    "                \"header\": [\n",
    "                    \"时间\",\n",
    "                    \"人物\",\n",
    "                    \"国家\",\n",
    "                    \"装备名称\",\n",
    "                    \"装备类型\",\n",
    "                    \"装备数量\",\n",
    "                    \"组织机构\",\n",
    "                    \"地点\",\n",
    "                    \"任务\",\n",
    "                    \"行动\",\n",
    "                    \"事件\"\n",
    "                ],\n",
    "                \"data\": {\n",
    "                    \"2\": [\n",
    "                        \"未提供具体时间\",\n",
    "                        \"谢尔盖・里亚布科夫\",\n",
    "                        \"俄罗斯\",\n",
    "                        \"新型高超声速武器\",\n",
    "                        \"高超声速武器\",\n",
    "                        \"未提供数量\",\n",
    "                        \"俄罗斯外交部\",\n",
    "                        \"未提供具体地点\",\n",
    "                        \"与美国展开对话\",\n",
    "                        \"就新型高超声速武器展开对话\",\n",
    "                        \"俄罗斯愿意与美国就高超声速武器展开对话\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"origin\": {},\n",
    "            \"text\": \"俄罗斯外交部副部长谢尔盖・里亚布科夫表示，俄罗斯愿意与美国就俄罗斯研发的新型高超声速武器展开对话，前提条件是美国必须就其高超声速项目、全球反导系统以及在太空部署武器的计划进行详谈。里亚布科夫指出，俄罗斯对美国的上述计划感到担忧。\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27938f-105e-4320-9a73-4e3fbf24ed6a",
   "metadata": {},
   "source": [
    "### 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dc008-32a5-424b-8b93-1e6418e166ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"主体名称-主体类型\": [{\"head\": \"高超声速项目、全球反导系统、太空部署武器\", \"tail\": \"高超声速项目、反导系统、太空武器\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d975dfdb-053d-453d-a46d-ed79f39405cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T13:54:02.265908Z",
     "iopub.status.busy": "2024-08-19T13:54:02.265120Z",
     "iopub.status.idle": "2024-08-19T13:54:02.275398Z",
     "shell.execute_reply": "2024-08-19T13:54:02.274144Z",
     "shell.execute_reply.started": "2024-08-19T13:54:02.265858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '在黑海水域中，俄罗斯黑海舰队导弹炮兵编队和堡垒机动式岸基反舰导弹系统小队进行了水面舰艇探测和打击的演习。在演习过程中，舞会和堡垒机动式岸基反舰导弹系统小队前往指定阵地，并对水面目标进行了识别和跟踪。根据指挥中心的指令，舞会和堡垒机动式岸基反舰导弹系统进入了战斗状态，并采用电子发射方式实施了对假想敌舰艇的导弹打击。', 'relation': [{'head': '黑海舰队导弹炮兵编队、堡垒机动式岸基反舰导弹系统小队', 'relation': '主体名称-主体类型', 'tail': '军事编队、军事小队'}, {'head': '黑海舰队导弹炮兵编队、堡垒机动式岸基反舰导弹系统小队', 'relation': '主体名称-主体数量', 'tail': '1、1'}]}\n",
      "{'text': '瑞典国防军在哥特兰岛进行军事防御演练，指挥官艾米丽在两天的演习中指挥共15辆CV90步兵战车参与岛屿防御演习和快速部署行动，以加强岛屿的防御体系。', 'relation': [{'head': '岛屿防御演习', 'relation': '核心事件-时间', 'tail': '2024年8月6日'}, {'head': '岛屿防御演习', 'relation': '核心事件-组织机构', 'tail': '瑞典国防军'}, {'head': '岛屿防御演习', 'relation': '核心事件-地点', 'tail': '哥特兰岛'}, {'head': 'CV90步兵战车', 'relation': '主体名称-主体类型', 'tail': '装甲战斗车辆'}, {'head': 'CV90步兵战车', 'relation': '主体名称-主体数量', 'tail': '7'}]}\n",
      "{'text': '预计今年年底，北德文斯克号将成为第一艘交付海军的亚森级潜艇，而第二艘喀山号则已于2009年开始建造，目前正在北方造船厂进行。', 'relation': [{'head': '交付', 'relation': '核心事件-时间', 'tail': '今年年底'}, {'head': '交付', 'relation': '核心事件-组织机构', 'tail': '海军'}, {'head': '亚森级潜艇', 'relation': '主体名称-主体类型', 'tail': '潜艇'}, {'head': '亚森级潜艇', 'relation': '主体名称-主体数量', 'tail': '1'}]}\n",
      "{'text': '美国空军中央司令部第336远征战斗机中队的F-15E攻击机，参加了在阿拉伯湾举行的水面战一体化演习的空战部分。海军中央司令部的冈萨雷斯号和P-3猎户座也参与了演习。此次演习旨在强化美国空军和海军部队之间的协作，以支持美国海军的水面作战舰艇。', 'relation': [{'head': '参加水面战一体化演习的空战部分', 'relation': '核心事件-组织机构', 'tail': '美国空军中央司令部'}, {'head': '参加水面战一体化演习的空战部分', 'relation': '核心事件-地点', 'tail': '阿拉伯湾'}, {'head': '第336远征战斗机中队', 'relation': '主体名称-主体类型', 'tail': 'F-15E攻击机'}]}\n",
      "{'text': '南美战略联盟在指挥官艾伦的指挥下，在南美热带雨林举行了雨林机动演习，两天内共出动了13架战术运输直升机，执行快速部署和空中机动任务。', 'relation': [{'head': '快速部署任务', 'relation': '核心事件-时间', 'tail': '2024年6月18日'}, {'head': '快速部署任务', 'relation': '核心事件-组织机构', 'tail': '南美战略联盟'}, {'head': '快速部署任务', 'relation': '核心事件-地点', 'tail': '南美热带雨林'}, {'head': '战术运输直升机', 'relation': '主体名称-主体类型', 'tail': '空中运输工具'}, {'head': '战术运输直升机', 'relation': '主体名称-主体数量', 'tail': '6'}]}\n",
      "{'text': '美国卡尼号阿利.伯克级导弹驱逐舰于6月30日抵达乌克兰奥德萨进行港口访问，这是2019年海风多国联合演习的组成部分。该舰参加演习旨在加强与北约盟国和伙伴国在黑海上的协同行动，并彰显在大西洋决心行动下解决黑海安全问题的方案。', 'relation': [{'head': '港口访问', 'relation': '核心事件-时间', 'tail': '6月30日'}, {'head': '港口访问', 'relation': '核心事件-组织机构', 'tail': '北约'}, {'head': '港口访问', 'relation': '核心事件-地点', 'tail': '乌克兰奥德萨'}, {'head': '卡尼号', 'relation': '主体名称-主体类型', 'tail': '阿利.伯克级导弹驱逐舰'}, {'head': '卡尼号', 'relation': '主体名称-主体数量', 'tail': '1'}]}\n",
      "{'text': '北约发言人詹姆斯·阿帕苏莱伊在一场记者招待会上宣布，北约成员国代表已经批准了再次在索马里海域执行打击海盗行动的计划。北约将派遣7艘舰艇前往该地区，执行打击海盗任务的是北约常备海上舰群1组。该舰群组将于3月19日通过苏伊士运河前往该地区，这项行动将于7月初结束。在此期间，该舰群将暂停打击海盗的行动，转而进行常规港口访问。', 'relation': [{'head': '常规港口访问', 'relation': '核心事件-时间', 'tail': '7月初'}]}\n",
      "{'text': '乌克兰海军派出了包括斯塔罗普斯克号岛级巡逻艇在内的多艘舰艇参与与美国海军罗斯福号驱逐舰的联合演习。斯塔罗普斯克号原为美国海岸警卫队所有，于2019年移交给乌克兰海军。', 'relation': [{'head': '联合演习', 'relation': '核心事件-时间', 'tail': '2019年'}, {'head': '联合演习', 'relation': '核心事件-组织机构', 'tail': '美国海岸警卫队'}, {'head': '斯塔罗普斯克号', 'relation': '主体名称-主体类型', 'tail': '岛级巡逻艇'}, {'head': '斯塔罗普斯克号', 'relation': '主体名称-主体数量', 'tail': '1'}]}\n",
      "{'text': '在北约联合军演期间，英国皇家陆军在威尔士军事基地举行了连续两天的联合军事演习。将军约翰逊指挥了包括挑战者2主战坦克和武士步兵战车在内的11辆装甲车辆，进行了夜间渗透和城市作战演练等行动。', 'relation': [{'head': '联合军事演习', 'relation': '核心事件-时间', 'tail': '2024年5月10日'}, {'head': '联合军事演习', 'relation': '核心事件-组织机构', 'tail': '英国皇家陆军'}, {'head': '联合军事演习', 'relation': '核心事件-地点', 'tail': '威尔士军事基地'}, {'head': '挑战者2主战坦克', 'relation': '主体名称-主体类型', 'tail': '主战坦克'}, {'head': '挑战者2主战坦克', 'relation': '主体名称-主体数量', 'tail': '7辆'}]}\n",
      "{'text': '中东反恐联盟在指挥官威尔斯的领导下，在中东沙漠地区举行了反恐实战演练，两天内共出动了13架特种作战直升机，执行反恐快速反应和空中突击任务。', 'relation': [{'head': '反恐快速反应', 'relation': '核心事件-时间', 'tail': '2024年4月3日'}, {'head': '反恐快速反应', 'relation': '核心事件-组织机构', 'tail': '中东反恐联盟'}, {'head': '反恐快速反应', 'relation': '核心事件-地点', 'tail': '中东沙漠地区'}, {'head': '特种作战直升机', 'relation': '主体名称-主体类型', 'tail': '低空快速打击平台'}, {'head': '特种作战直升机', 'relation': '主体名称-主体数量', 'tail': '7'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input = {}\n",
    "output = {}\n",
    "# 打开JSONL文件\n",
    "with open('newdata/test_demo.json', 'r', encoding='utf-8') as file:\n",
    "    # 逐行读取文件\n",
    "    for line in file:\n",
    "        # 去除行尾的换行符并解析JSON\n",
    "        data = json.loads(line.strip())\n",
    "        # 处理解析后的字典\n",
    "        \n",
    "        print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
