{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6c0d12-dc09-4eaf-b9e4-fa18c4f6f558",
   "metadata": {},
   "source": [
    "# Baichuan2-13B data2text LoRAå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01246fe-bb6e-4e79-8174-c8922adc00b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T09:43:21.029093Z",
     "iopub.status.busy": "2024-08-14T09:43:21.028280Z",
     "iopub.status.idle": "2024-08-14T09:43:21.035442Z",
     "shell.execute_reply": "2024-08-14T09:43:21.034206Z",
     "shell.execute_reply.started": "2024-08-14T09:43:21.029043Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.6.0.17:8888\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf05eb9-1062-48a6-8fb7-f63364050fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T14:46:13.234819Z",
     "iopub.status.busy": "2024-08-12T14:46:13.233984Z",
     "iopub.status.idle": "2024-08-12T14:46:13.241082Z",
     "shell.execute_reply": "2024-08-12T14:46:13.239926Z",
     "shell.execute_reply.started": "2024-08-12T14:46:13.234756Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62305aee-615e-4722-abff-37c20b0434d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T07:20:30.934735Z",
     "iopub.status.busy": "2024-08-11T07:20:30.933712Z",
     "iopub.status.idle": "2024-08-11T07:20:43.306501Z",
     "shell.execute_reply": "2024-08-11T07:20:43.305468Z",
     "shell.execute_reply.started": "2024-08-11T07:20:30.934672Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//pypi.tuna.tsinghua.edu.cn/simple')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'baichuan-inc/Baichuan2-13B-Chat'\n",
    "lora_path = 'zjunlp/baichuan2-13b-iepile-lora'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05e79ea-88a3-45aa-a6f2-a3104bff4514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T10:58:36.728608Z",
     "iopub.status.busy": "2024-08-10T10:58:36.727772Z",
     "iopub.status.idle": "2024-08-10T10:58:38.394026Z",
     "shell.execute_reply": "2024-08-10T10:58:38.392975Z",
     "shell.execute_reply.started": "2024-08-10T10:58:36.728547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_137368/3559238702.py\", line 1, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py\", line 558, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/c8d877c7ca596d9aeff429d43bff06e288684f45/modeling_baichuan.py\", line 670, in from_pretrained\n",
      "    return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path, *model_args,\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\", line 2954, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/c8d877c7ca596d9aeff429d43bff06e288684f45/modeling_baichuan.py\", line 539, in __init__\n",
      "    self.model = BaichuanModel(config)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/c8d877c7ca596d9aeff429d43bff06e288684f45/modeling_baichuan.py\", line 296, in __init__\n",
      "    self.embed_tokens = torch.nn.Embedding(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\", line 146, in __init__\n",
      "    self.weight = Parameter(_weight)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\", line 155, in reset_parameters\n",
      "    if self.padding_idx is not None:\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/init.py\", line 176, in normal_\n",
      "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/init.py\", line 21, in _no_grad_normal_\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_refs/__init__.py\", line 6091, in normal_\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_prims_common/wrappers.py\", line 266, in _fn\n",
      "    _fn.__signature__ = inspect.Signature(  # type: ignore[attr-defined]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_prims_common/wrappers.py\", line 138, in _fn\n",
      "    \"An output with one or more elements was resized since it had shape {0} \"\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/_refs/__init__.py\", line 6051, in normal\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/__init__.py\", line 1353, in _check\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/__init__.py\", line 1317, in _check_with\n",
      "ImportError: cannot import name 'expect_true' from 'torch.fx.experimental.symbolic_shapes' (/usr/local/lib/python3.9/dist-packages/torch/fx/experimental/symbolic_shapes.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a80ae3-f5eb-44ee-bfdc-171758f11907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T12:20:51.987602Z",
     "iopub.status.busy": "2024-08-11T12:20:51.986710Z",
     "iopub.status.idle": "2024-08-11T12:20:51.993516Z",
     "shell.execute_reply": "2024-08-11T12:20:51.992466Z",
     "shell.execute_reply.started": "2024-08-11T12:20:51.987539Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17df2be1-c920-4020-bbfb-38f65799a612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T14:46:52.333101Z",
     "iopub.status.busy": "2024-08-12T14:46:52.332191Z",
     "iopub.status.idle": "2024-08-12T14:46:53.598657Z",
     "shell.execute_reply": "2024-08-12T14:46:53.597425Z",
     "shell.execute_reply.started": "2024-08-12T14:46:52.333046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# é‡ç½®æœ€å¤§å†…å­˜è®¡æ•°å™¨\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2393dc0d-34e3-4013-a933-f8f0d025c19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T09:43:27.026712Z",
     "iopub.status.busy": "2024-08-14T09:43:27.025921Z",
     "iopub.status.idle": "2024-08-14T09:53:09.642015Z",
     "shell.execute_reply": "2024-08-14T09:53:09.640664Z",
     "shell.execute_reply.started": "2024-08-14T09:43:27.026654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('https'), PosixPath('//pypi.tuna.tsinghua.edu.cn/simple')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.6.0.17'), PosixPath('8888'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 17:43:30 - WARNING - args.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1332] 2024-08-14 17:43:30,806 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1764] 2024-08-14 17:43:30,806 >> PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "08/14/2024 17:43:30 - INFO - args.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "08/14/2024 17:43:30 - INFO - args.parser - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/baichuan13B-data2text-continue/runs/Aug14_17-43-30_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/baichuan13B-data2text-continue,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/baichuan13B-data2text-continue,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/14/2024 17:43:30 - INFO - __main__ - Start Time: 2024:08:14 17:43:30\n",
      "08/14/2024 17:43:30 - INFO - __main__ - model_args:ModelArguments(model_name_or_path='baichuan-inc/Baichuan2-13B-Chat', model_name='baichuan', cache_dir=None, use_fast_tokenizer=True, trust_remote_code=True, use_auth_token=False, model_revision='main', split_special_tokens=False, bits=4, adam8bit=False, double_quant=True, quant_type='nf4', checkpoint_dir=None)\n",
      "data_args:DataArguments(train_file='data2text/iepile-ner-train-transformed.json', valid_file='data2text/iepile-ner-val-transformed.json', predict_file=None, preprocessing_num_workers=16, overwrite_cache=False, cache_path=None, template='baichuan2', system_prompt=None, max_source_length=400, max_target_length=300, cutoff_len=700, val_set_size=1000, pad_to_max_length=False, ignore_pad_token_for_loss=True, train_on_prompt=False, language='zh', id_text='input')\n",
      "training_args:TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/baichuan13B-data2text-continue/runs/Aug14_17-43-30_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/baichuan13B-data2text-continue,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/baichuan13B-data2text-continue,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "finetuning_args:FinetuningArguments(dpo_beta=0.1, ppo_logger=None, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_checkpoint=None, ref_model_quantization_bit=None, reward_model=None, reward_model_checkpoint=None, reward_model_quantization_bit=None, reward_model_type='lora', lora_r=64, lora_alpha=64.0, lora_dropout=0.05, lora_target_modules=['W_pack', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'], additional_target=None, resume_lora_training=True, num_layer_trainable=3, name_module_trainable=['mlp'], stage='sft', finetuning_type='lora', upcast_layernorm=False, neft_alpha=0, export_dir=None, plot_loss=False)\n",
      "generating_args:GenerationArguments(max_length=512, max_new_tokens=256, min_new_tokens=None, do_sample=False, num_beams=1, num_beam_groups=1, penalty_alpha=None, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n",
      "08/14/2024 17:43:30 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "trainer_class:<class 'transformers.trainer.Trainer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:43:31,897 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:43:31,897 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:43:31,897 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 17:43:31,897 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 17:43:32,215 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 17:43:32,713 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-08-14 17:43:32,715 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"_name_or_path\": \"baichuan-inc/Baichuan2-13B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"baichuan-inc/Baichuan2-13B-Chat--configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"baichuan-inc/Baichuan2-13B-Chat--modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": [\n",
      "    false\n",
      "  ],\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13696,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "08/14/2024 17:43:32 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.19)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "[INFO|modeling_utils.py:2857] 2024-08-14 17:43:33,117 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1200] 2024-08-14 17:43:33,120 >> Instantiating BaichuanForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 17:43:33,121 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2971] 2024-08-14 17:43:33,192 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:04<00:00, 21.50s/it]\n",
      "[INFO|modeling_utils.py:3643] 2024-08-14 17:44:37,843 >> All model checkpoint weights were used when initializing BaichuanForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3651] 2024-08-14 17:44:37,843 >> All the weights of BaichuanForCausalLM were initialized from the model checkpoint at baichuan-inc/Baichuan2-13B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BaichuanForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:730] 2024-08-14 17:44:38,121 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--baichuan-inc--Baichuan2-13B-Chat/snapshots/c8d877c7ca596d9aeff429d43bff06e288684f45/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 17:44:38,122 >> Generate config GenerationConfig {\n",
      "  \"assistant_token_id\": 196,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.85,\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"user_token_id\": 195\n",
      "}\n",
      "\n",
      "08/14/2024 17:44:38 - INFO - model.adapter - Gradient checkpointing enabled.\n",
      "08/14/2024 17:44:38 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/14/2024 17:46:02 - INFO - model.loader - trainable params: 223150080 || all params: 14119818240 || trainable%: 1.5804\n",
      "08/14/2024 17:46:02 - INFO - __main__ - BOS:1,<s>\tEOS:2,</s>\tPAD:0,<unk>\n",
      "Using custom data configuration default-230b0cd978e532d9\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Using custom data configuration default-c20be129830f148f\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 17:46:04,066 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-da6b80c4ff0c104c_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 12915, 2925, 1664, 4956, 2925, 1664, 1967, 1650, 2925, 1664, 1650, 65226, 1664, 12269, 5338, 1664, 71109, 18648, 3346, 6866, 70480, 2392, 65, 21904, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 65, 14193, 92926, 93198, 94390, 18648, 4487, 17022, 2339, 94638, 92827, 93845, 93144, 93604, 11467, 39546, 96337, 77343, 88865, 65, 92391, 2849, 4330, 15890, 2274, 66, 4605, 7375, 7491, 17022, 5073, 92730, 92493, 11466, 93107, 65, 2802, 11466, 92919, 92373, 92362, 92338, 92335, 25409, 92424, 14623, 94575, 94515, 92986, 92843, 96467, 66, 52639, 196, 30338, 12915, 5338, 86381, 1664, 4956, 5338, 86381, 1664, 1967, 1650, 5338, 86381, 1664, 1650, 5338, 50154, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 19342, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®ä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»inputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„å®ä½“ç±»å‹è¿”å›ç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ã€‚\", \"schema\": [\"å¤‡æ³¨\", \"æ•°é‡\", \"å¼€å§‹æ—¶é—´\", \"æ—¶é—´\"], \"input\": \"æ®ç¾å›½ç©ºå†›å…¨çƒæ‰“å‡»å¸ä»¤éƒ¨è¡¨ç¤ºï¼Œå½“åœ°æ—¶é—´2æœˆ5æ—¥12æ—¶33åˆ†ï¼Œä»–ä»¬åœ¨èŒƒç™»å ¡ç©ºå†›åŸºåœ°å‘å°„äº†ä¸€æšæœªæºè½½å¼¹å¤´çš„æ°‘å…µâ…¢æ´²é™…å¼¹é“å¯¼å¼¹ï¼Œä»¥è¿›ä¸€æ­¥æµ‹è¯•å¯¼å¼¹ç³»ç»Ÿã€‚æ­¤æ¬¡è¯•éªŒåŒæ­¥å‘å°„äº†ä¸€ä¸ªå†å…¥é£è¡Œå™¨ï¼ŒæˆåŠŸé£è¡Œçº¦6920åƒç±³åæŠµè¾¾å¤¸è´¾æ—ç¯ç¤ã€‚\"}<reserved_107>{\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 12915, 5338, 86381, 1664, 4956, 5338, 86381, 1664, 1967, 1650, 5338, 86381, 1664, 1650, 5338, 50154, 92338, 92443, 92358, 92420, 92336, 92338, 92380, 92354, 92354, 92431, 19342, 92795, 2]\n",
      "labels:\n",
      " {\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}</s>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 17:46:04,226 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-26fbf8a3d740b323_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[195, 30338, 9571, 4256, 5338, 1664, 8225, 7048, 1697, 12242, 29090, 86566, 66, 92676, 92569, 12269, 92364, 93689, 24352, 3550, 46272, 12316, 92333, 12242, 65, 92349, 13601, 12242, 6315, 10078, 92842, 27303, 66, 92676, 2592, 38498, 59124, 94200, 92333, 14499, 7293, 66, 2925, 1664, 46272, 5338, 50154, 92397, 92620, 2925, 1664, 3461, 3531, 2925, 1664, 92892, 92534, 2925, 1664, 86096, 65226, 1664, 12269, 5338, 1664, 42008, 39549, 92639, 6675, 2296, 1719, 2998, 1748, 32077, 92888, 92402, 1860, 65, 43236, 92538, 20993, 32077, 3581, 65, 2244, 92538, 92888, 2019, 4349, 8398, 13087, 66, 41018, 65, 92627, 93518, 1719, 3205, 17812, 2296, 1719, 1697, 18988, 65, 2549, 4104, 1558, 3685, 1805, 65, 12775, 3142, 17526, 10314, 19135, 93110, 5105, 94129, 66, 52639, 196, 30338, 92397, 92620, 5338, 86381, 1664, 3461, 3531, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 86096, 5338, 27381, 92795, 2]\n",
      "inputs:\n",
      " <reserved_106>{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®ä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»inputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„å®ä½“ç±»å‹è¿”å›ç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›ç­”ã€‚\", \"schema\": [\"å›½åˆ«\", \"ä»»åŠ¡çŠ¶æ€\", \"å†›ç§\", \"æŒ‡æŒ¥å‘˜\"], \"input\": \"æ²™ç‰¹é˜¿æ‹‰ä¼¯å·²é‚€è¯·ç¾å›½å…¬å¸å‚ä¸å‘å±•æ°‘ç”¨æ ¸èƒ½é¡¹ç›®ï¼Œå¹¶è¡¨ç¤ºå°†ä¸“æ³¨äºæ°‘ç”¨é¢†åŸŸï¼Œä¸ä¼šå°†æ ¸æŠ€æœ¯ç”¨äºå†›äº‹ç”¨é€”ã€‚æ®æŠ¥é“ï¼Œè¥¿å±‹å…¬å¸æ­£åœ¨ä¸å…¶ä»–ç¾å›½å…¬å¸è¿›è¡Œè°ˆåˆ¤ï¼Œè®¡åˆ’æˆç«‹ä¸€ä¸ªè”åˆä¼ä¸šï¼Œå»ºé€ ä»·å€¼æ•°åäº¿ç¾å…ƒçš„ä¸¤åº§ååº”å †ã€‚\"}<reserved_107>{\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30338, 92397, 92620, 5338, 86381, 1664, 3461, 3531, 5338, 86381, 1664, 92892, 92534, 5338, 86381, 1664, 86096, 5338, 27381, 92795, 2]\n",
      "labels:\n",
      " {\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}</s>\n",
      "[INFO|trainer.py:403] 2024-08-14 17:46:04,311 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n",
      "08/14/2024 17:46:04 - INFO - __main__ - *** Train ***\n",
      "08/14/2024 17:46:04 - INFO - __main__ - resume_from_checkpoint: None\n",
      "[INFO|trainer.py:1712] 2024-08-14 17:46:06,047 >> ***** Running training *****\n",
      "[INFO|trainer.py:1713] 2024-08-14 17:46:06,048 >>   Num examples = 7,732\n",
      "[INFO|trainer.py:1714] 2024-08-14 17:46:06,048 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1715] 2024-08-14 17:46:06,048 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1718] 2024-08-14 17:46:06,048 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1719] 2024-08-14 17:46:06,048 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1720] 2024-08-14 17:46:06,048 >>   Total optimization steps = 9,660\n",
      "[INFO|trainer.py:1721] 2024-08-14 17:46:06,051 >>   Number of trainable parameters = 223,150,080\n",
      "{'loss': 0.9276, 'learning_rate': 4.99896480331263e-05, 'epoch': 0.0}           \n",
      "{'loss': 0.6075, 'learning_rate': 4.997929606625259e-05, 'epoch': 0.0}          \n",
      "{'loss': 0.3482, 'learning_rate': 4.996894409937888e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.4708, 'learning_rate': 4.995859213250518e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2039, 'learning_rate': 4.994824016563147e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.4217, 'learning_rate': 4.993788819875777e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2545, 'learning_rate': 4.9927536231884056e-05, 'epoch': 0.01}        \n",
      "{'loss': 0.1774, 'learning_rate': 4.991718426501035e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.2546, 'learning_rate': 4.9906832298136646e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1323, 'learning_rate': 4.989648033126294e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.2358, 'learning_rate': 4.988612836438924e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1317, 'learning_rate': 4.9875776397515526e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1505, 'learning_rate': 4.986542443064182e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1626, 'learning_rate': 4.985507246376812e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.2001, 'learning_rate': 4.984472049689442e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1979, 'learning_rate': 4.983436853002071e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1404, 'learning_rate': 4.9824016563147e-05, 'epoch': 0.04}           \n",
      "{'loss': 0.106, 'learning_rate': 4.98136645962733e-05, 'epoch': 0.04}           \n",
      "{'loss': 0.1569, 'learning_rate': 4.980331262939959e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.1678, 'learning_rate': 4.979296066252588e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.1256, 'learning_rate': 4.9782608695652176e-05, 'epoch': 0.04}        \n",
      "{'loss': 0.1279, 'learning_rate': 4.977225672877847e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0769, 'learning_rate': 4.976190476190477e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0475, 'learning_rate': 4.975155279503106e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0934, 'learning_rate': 4.974120082815735e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.2765, 'learning_rate': 4.9730848861283646e-05, 'epoch': 0.05}        \n",
      "{'loss': 0.2181, 'learning_rate': 4.972049689440994e-05, 'epoch': 0.06}         \n",
      "{'loss': 0.1741, 'learning_rate': 4.9710144927536237e-05, 'epoch': 0.06}        \n",
      "{'loss': 0.1981, 'learning_rate': 4.9699792960662525e-05, 'epoch': 0.06}        \n",
      "{'loss': 0.163, 'learning_rate': 4.968944099378882e-05, 'epoch': 0.06}          \n",
      "{'loss': 0.1377, 'learning_rate': 4.9679089026915116e-05, 'epoch': 0.06}        \n",
      "{'loss': 0.2515, 'learning_rate': 4.966873706004141e-05, 'epoch': 0.07}         \n",
      "{'loss': 0.1498, 'learning_rate': 4.9658385093167706e-05, 'epoch': 0.07}        \n",
      "{'loss': 0.1156, 'learning_rate': 4.9648033126293995e-05, 'epoch': 0.07}        \n",
      "{'loss': 0.2631, 'learning_rate': 4.963768115942029e-05, 'epoch': 0.07}         \n",
      "{'loss': 0.1871, 'learning_rate': 4.9627329192546585e-05, 'epoch': 0.07}        \n",
      "{'loss': 0.2195, 'learning_rate': 4.961697722567288e-05, 'epoch': 0.08}         \n",
      "{'loss': 0.1372, 'learning_rate': 4.960662525879917e-05, 'epoch': 0.08}         \n",
      "  1%|â–                                     | 76/9660 [06:59<14:51:27,  5.58s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/finetune.py \\\n",
    "    --do_train --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_name_or_path 'baichuan-inc/Baichuan2-13B-Chat' \\\n",
    "    --stage 'sft' \\\n",
    "    --model_name 'baichuan' \\\n",
    "    --template 'baichuan2' \\\n",
    "    --train_file 'data2text/iepile-ner-train-transformed.json' \\\n",
    "    --valid_file 'data2text/iepile-ner-val-transformed.json' \\\n",
    "    --output_dir='lora/baichuan13B-data2text-continue' \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --optim \"adamw_torch\" \\\n",
    "    --max_source_length 400 \\\n",
    "    --cutoff_len 700 \\\n",
    "    --max_target_length 300 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 10 \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --bf16 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad5345-49f5-49d9-91c8-417d61f78956",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨å¾®è°ƒæ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡ŒæŠ½å–\n",
    "1. åœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œmodel_name, template, å’Œ bf16 å¿…é¡»ä¸è®­ç»ƒæ—¶çš„è®¾ç½®ç›¸åŒã€‚\n",
    "2. model_name_or_path: æŒ‡å®šæ‰€ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¿…é¡»ä¸ç›¸åº”çš„LoRAæ¨¡å‹åŒ¹é…ã€‚\n",
    "3. checkpoint_dir: LoRAçš„æƒé‡æ–‡ä»¶è·¯å¾„ã€‚\n",
    "4. output_dir: æ­¤å‚æ•°åœ¨æ¨ç†æ—¶ä¸èµ·ä½œç”¨ï¼Œå¯ä»¥éšæ„æŒ‡å®šä¸€ä¸ªè·¯å¾„ã€‚\n",
    "5. input_file, output_file: åˆ†åˆ«æŒ‡å®šè¾“å…¥çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„å’Œé¢„æµ‹ç»“æœçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚\n",
    "6. cutoff_len, max_new_tokens: è®¾ç½®æœ€å¤§çš„è¾“å…¥é•¿åº¦å’Œç”Ÿæˆçš„æ–°tokenæ•°é‡ï¼Œæ ¹æ®æ˜¾å­˜å¤§å°è¿›è¡Œè°ƒæ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ac50-2ba2-43fd-a682-38dc6341252e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T09:54:05.075566Z",
     "iopub.status.busy": "2024-08-14T09:54:05.074487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8888'), PosixPath('//10.6.0.17')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 17:54:08 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "08/14/2024 17:54:10 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.0.1+cu117)\n",
      "    Python  3.9.18 (you have 3.9.19)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:02<00:00, 20.95s/it]\n",
      "08/14/2024 17:55:14 - INFO - model.adapter - Fine-tuning method: LoRA\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/inference.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path 'baichuan-inc/Baichuan2-13B-Chat' \\\n",
    "    --checkpoint_dir 'lora/baichuan13B-data2text-continue' \\\n",
    "    --model_name 'baichuan' \\\n",
    "    --template 'baichuan2' \\\n",
    "    --do_predict \\\n",
    "    --input_file 'data2text/iepile-ner-test-transformed.json' \\\n",
    "    --output_file 'results/test.json' \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir 'lora/oneke-continue-test' \\\n",
    "    --predict_with_generate \\\n",
    "    --cutoff_len 512 \\\n",
    "    --bf16 \\\n",
    "    --max_new_tokens 300 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543df8ab-d1a7-4a88-b5e5-ee135fc12b89",
   "metadata": {},
   "source": [
    "### å¯¹æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3e7345-301c-4673-b19d-ad566dbb9186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T04:34:05.574190Z",
     "iopub.status.busy": "2024-08-08T04:34:05.573600Z",
     "iopub.status.idle": "2024-08-08T04:34:05.969137Z",
     "shell.execute_reply": "2024-08-08T04:34:05.967768Z",
     "shell.execute_reply.started": "2024-08-08T04:34:05.574152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': {'æ€»æ ·æœ¬æ•°': 215, 'é”™è¯¯æ•°': 0, 'P': 45.2, 'R': 43.83, 'F1': 44.5}}\n"
     ]
    }
   ],
   "source": [
    "!python ie2instruction/eval_func.py \\\n",
    "  --path1 results/baichuan-ft-test-output.json \\\n",
    "  --task NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2eb02c-55a4-44c5-ad94-d04bdea187f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
