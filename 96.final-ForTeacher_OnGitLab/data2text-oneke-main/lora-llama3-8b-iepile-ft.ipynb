{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8873a6f-abab-478d-8485-86ad234b3264",
   "metadata": {},
   "source": [
    "# Llama-8B IEPile-data2text LoRAå¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44102d9-9b87-4ea9-a5f6-e3ab0505fb38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:07:37.013026Z",
     "iopub.status.busy": "2024-08-20T12:07:37.012266Z",
     "iopub.status.idle": "2024-08-20T12:07:37.022846Z",
     "shell.execute_reply": "2024-08-20T12:07:37.022014Z",
     "shell.execute_reply.started": "2024-08-20T12:07:37.012983Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.6.0.17:8888\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b14750-157d-4659-9d5e-b9a64292f6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:07:38.472355Z",
     "iopub.status.busy": "2024-08-20T12:07:38.471620Z",
     "iopub.status.idle": "2024-08-20T12:07:38.478556Z",
     "shell.execute_reply": "2024-08-20T12:07:38.477383Z",
     "shell.execute_reply.started": "2024-08-20T12:07:38.472301Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28150793-57cf-45c3-a9cc-97def0e42420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:05:48.474884Z",
     "iopub.status.busy": "2024-08-20T12:05:48.474059Z",
     "iopub.status.idle": "2024-08-20T12:05:53.308214Z",
     "shell.execute_reply": "2024-08-20T12:05:53.307150Z",
     "shell.execute_reply.started": "2024-08-20T12:05:48.474831Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8888'), PosixPath('//10.6.0.17'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import _utils\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B'\n",
    "lora_path = 'lora/llama3-8B-iepile-data2text-continue'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9bc06e-be22-4b25-b0cb-7e74f1b124dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:05:57.479397Z",
     "iopub.status.busy": "2024-08-20T12:05:57.478628Z",
     "iopub.status.idle": "2024-08-20T12:07:16.330267Z",
     "shell.execute_reply": "2024-08-20T12:07:16.328975Z",
     "shell.execute_reply.started": "2024-08-20T12:05:57.479345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:32<00:00,  8.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     model_path,\n\u001b[1;32m      3\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:270\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[38;5;241m=\u001b[39mis_trainable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:893\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:112\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:180\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# transformers models have a .config attribute, whose presence is assumed later on\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:194\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for all adapters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:352\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m         target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    345\u001b[0m             adapter_name,\n\u001b[1;32m    346\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39mr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39minit_lora_weights,\n\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:309\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, only `torch.nn.Linear` and `Conv1D` are supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:765\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, adapter_name, in_features, out_features, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    753\u001b[0m     adapter_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    762\u001b[0m ):\n\u001b[1;32m    763\u001b[0m     init_lora_weights \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_lora_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 765\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     LoraLayer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features\u001b[38;5;241m=\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39mout_features)\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Freezing the pre-trained weight matrix\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce7f22-9a31-4184-8380-06ae97e25d5c",
   "metadata": {},
   "source": [
    "## å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c8a924-f103-44a6-b909-ff93c7b078ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:12:47.128856Z",
     "iopub.status.busy": "2024-08-14T10:12:47.128047Z",
     "iopub.status.idle": "2024-08-14T10:17:03.437003Z",
     "shell.execute_reply": "2024-08-14T10:17:03.435440Z",
     "shell.execute_reply.started": "2024-08-14T10:12:47.128817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//10.6.0.17'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 18:12:50 - WARNING - args.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1332] 2024-08-14 18:12:50,995 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1764] 2024-08-14 18:12:50,995 >> PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "08/14/2024 18:12:50 - INFO - args.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "08/14/2024 18:12:50 - INFO - args.parser - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_18-12-50_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/14/2024 18:12:50 - INFO - __main__ - Start Time: 2024:08:14 18:12:50\n",
      "08/14/2024 18:12:50 - INFO - __main__ - model_args:ModelArguments(model_name_or_path='meta-llama/Meta-Llama-3-8B', model_name='llama', cache_dir=None, use_fast_tokenizer=True, trust_remote_code=True, use_auth_token=False, model_revision='main', split_special_tokens=False, bits=4, adam8bit=False, double_quant=True, quant_type='nf4', checkpoint_dir=['zjunlp/llama3-8b-iepile-lora'])\n",
      "data_args:DataArguments(train_file='data2text/iepile-ner-train-transformed.json', valid_file='data2text/iepile-ner-val-transformed.json', predict_file=None, preprocessing_num_workers=16, overwrite_cache=False, cache_path=None, template='alpaca', system_prompt=None, max_source_length=400, max_target_length=300, cutoff_len=700, val_set_size=1000, pad_to_max_length=False, ignore_pad_token_for_loss=True, train_on_prompt=False, language='zh', id_text='input')\n",
      "training_args:TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_18-12-50_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "finetuning_args:FinetuningArguments(dpo_beta=0.1, ppo_logger=None, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_checkpoint=None, ref_model_quantization_bit=None, reward_model=None, reward_model_checkpoint=None, reward_model_quantization_bit=None, reward_model_type='lora', lora_r=64, lora_alpha=64.0, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], additional_target=None, resume_lora_training=True, num_layer_trainable=3, name_module_trainable=['mlp'], stage='sft', finetuning_type='lora', upcast_layernorm=False, neft_alpha=0, export_dir=None, plot_loss=False)\n",
      "generating_args:GenerationArguments(max_length=512, max_new_tokens=256, min_new_tokens=None, do_sample=False, num_beams=1, num_beam_groups=1, penalty_alpha=None, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n",
      "08/14/2024 18:12:50 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "trainer_class:<class 'transformers.trainer.Trainer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,786 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 18:12:52,392 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-08-14 18:12:52,396 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "08/14/2024 18:12:52 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:2857] 2024-08-14 18:12:52,444 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1200] 2024-08-14 18:12:52,447 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 18:12:52,448 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2971] 2024-08-14 18:12:52,707 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:34<00:00,  8.70s/it]\n",
      "[INFO|modeling_utils.py:3643] 2024-08-14 18:13:27,772 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3651] 2024-08-14 18:13:27,772 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:730] 2024-08-14 18:13:28,075 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 18:13:28,076 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Gradient checkpointing enabled.\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Resume model checkpoint(s): zjunlp/llama3-8b-iepile-lora .\n",
      "08/14/2024 18:14:20 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): zjunlp/llama3-8b-iepile-lora\n",
      "08/14/2024 18:14:20 - INFO - model.loader - trainable params: 167772160 || all params: 8198033408 || trainable%: 2.0465\n",
      "[ERROR|tokenization_utils_base.py:1061] 2024-08-14 18:14:20,146 >> Using pad_token, but it is not set yet.\n",
      "08/14/2024 18:14:20 - INFO - __main__ - BOS:128000,<|begin_of_text|>\tEOS:128001,<|end_of_text|>\tPAD:None,None\n",
      "Using custom data configuration default-230b0cd978e532d9\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Using custom data configuration default-c20be129830f148f\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/14/2024 18:14:22 - INFO - datamodule.template - Add pad token: <|end_of_text|>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:14:22,175 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 98390, 498, 330, 82900, 498, 330, 56386, 21082, 498, 330, 21082, 8073, 330, 1379, 794, 330, 16423, 109615, 35894, 103626, 117805, 76537, 53826, 65743, 103507, 34048, 52563, 119491, 30590, 21082, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 117925, 19000, 104463, 29741, 114618, 35894, 103626, 124426, 29391, 105644, 104430, 109577, 39442, 120988, 28466, 115039, 65455, 9554, 70821, 104443, 71567, 95, 102776, 104153, 115039, 45893, 65820, 115039, 105610, 126841, 82805, 65820, 115039, 73548, 122471, 33671, 42421, 42462, 119298, 29391, 105644, 35287, 48044, 88356, 17701, 107163, 23039, 32648, 3922, 19653, 107163, 23039, 95337, 25073, 15, 103633, 73361, 34547, 116875, 94588, 8192, 116, 13647, 122, 102138, 87412, 17920, 223, 1811, 64259, 14711, 6075, 512, 5018, 98390, 794, 10277, 330, 82900, 794, 10277, 330, 56386, 21082, 794, 10277, 330, 21082, 794, 4482, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 93546, 128001]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"å¤‡æ³¨\", \"æ•°é‡\", \"å¼€å§‹æ—¶é—´\", \"æ—¶é—´\"], \"input\": \"æ®ç¾Žå›½ç©ºå†›å…¨çƒæ‰“å‡»å¸ä»¤éƒ¨è¡¨ç¤ºï¼Œå½“åœ°æ—¶é—´2æœˆ5æ—¥12æ—¶33åˆ†ï¼Œä»–ä»¬åœ¨èŒƒç™»å ¡ç©ºå†›åŸºåœ°å‘å°„äº†ä¸€æžšæœªæºè½½å¼¹å¤´çš„æ°‘å…µâ…¢æ´²é™…å¼¹é“å¯¼å¼¹ï¼Œä»¥è¿›ä¸€æ­¥æµ‹è¯•å¯¼å¼¹ç³»ç»Ÿã€‚æ­¤æ¬¡è¯•éªŒåŒæ­¥å‘å°„äº†ä¸€ä¸ªå†å…¥é£žè¡Œå™¨ï¼ŒæˆåŠŸé£žè¡Œçº¦6920åƒç±³åŽæŠµè¾¾å¤¸è´¾æž—çŽ¯ç¤ã€‚\"}\n",
      "\n",
      "### Response:\n",
      "{\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}<|end_of_text|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 98390, 794, 10277, 330, 82900, 794, 10277, 330, 56386, 21082, 794, 10277, 330, 21082, 794, 4482, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 93546, 128001]\n",
      "labels:\n",
      "{\"å¤‡æ³¨\": [], \"æ•°é‡\": [], \"å¼€å§‹æ—¶é—´\": [], \"æ—¶é—´\": [\"2æœˆ5æ—¥12æ—¶33åˆ†\"]}<|end_of_text|>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:14:22,391 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 29504, 64022, 498, 330, 89902, 45191, 498, 330, 103626, 87502, 498, 330, 64467, 115683, 43323, 8073, 330, 1379, 794, 330, 105351, 66378, 103478, 73325, 107024, 37767, 45932, 222, 15225, 109615, 74318, 124080, 106246, 70821, 11883, 72237, 27327, 74445, 91495, 52563, 45163, 96511, 26130, 35304, 70821, 11883, 124177, 102836, 38093, 45163, 72237, 104696, 114593, 103626, 30926, 11883, 108968, 1811, 16423, 87327, 3922, 61786, 102571, 74318, 97655, 58318, 93994, 109615, 74318, 72917, 110695, 106053, 3922, 109703, 116166, 48044, 122933, 105231, 3922, 26892, 67178, 121637, 9039, 95598, 54456, 119645, 9554, 78640, 103605, 121287, 125985, 1811, 64259, 14711, 6075, 512, 5018, 29504, 64022, 794, 10277, 330, 89902, 45191, 794, 10277, 330, 103626, 87502, 794, 10277, 330, 64467, 115683, 43323, 794, 3132, 92, 128001]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"å›½åˆ«\", \"ä»»åŠ¡çŠ¶æ€\", \"å†›ç§\", \"æŒ‡æŒ¥å‘˜\"], \"input\": \"æ²™ç‰¹é˜¿æ‹‰ä¼¯å·²é‚€è¯·ç¾Žå›½å…¬å¸å‚ä¸Žå‘å±•æ°‘ç”¨æ ¸èƒ½é¡¹ç›®ï¼Œå¹¶è¡¨ç¤ºå°†ä¸“æ³¨äºŽæ°‘ç”¨é¢†åŸŸï¼Œä¸ä¼šå°†æ ¸æŠ€æœ¯ç”¨äºŽå†›äº‹ç”¨é€”ã€‚æ®æŠ¥é“ï¼Œè¥¿å±‹å…¬å¸æ­£åœ¨ä¸Žå…¶ä»–ç¾Žå›½å…¬å¸è¿›è¡Œè°ˆåˆ¤ï¼Œè®¡åˆ’æˆç«‹ä¸€ä¸ªè”åˆä¼ä¸šï¼Œå»ºé€ ä»·å€¼æ•°åäº¿ç¾Žå…ƒçš„ä¸¤åº§ååº”å †ã€‚\"}\n",
      "\n",
      "### Response:\n",
      "{\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}<|end_of_text|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 29504, 64022, 794, 10277, 330, 89902, 45191, 794, 10277, 330, 103626, 87502, 794, 10277, 330, 64467, 115683, 43323, 794, 3132, 92, 128001]\n",
      "labels:\n",
      "{\"å›½åˆ«\": [], \"ä»»åŠ¡çŠ¶æ€\": [], \"å†›ç§\": [], \"æŒ‡æŒ¥å‘˜\": []}<|end_of_text|>\n",
      "[INFO|trainer.py:403] 2024-08-14 18:14:22,566 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n",
      "08/14/2024 18:14:22 - INFO - __main__ - *** Train ***\n",
      "08/14/2024 18:14:22 - INFO - __main__ - resume_from_checkpoint: None\n",
      "[INFO|trainer.py:1712] 2024-08-14 18:14:24,452 >> ***** Running training *****\n",
      "[INFO|trainer.py:1713] 2024-08-14 18:14:24,452 >>   Num examples = 7,732\n",
      "[INFO|trainer.py:1714] 2024-08-14 18:14:24,452 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1715] 2024-08-14 18:14:24,452 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1718] 2024-08-14 18:14:24,452 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1719] 2024-08-14 18:14:24,452 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1720] 2024-08-14 18:14:24,452 >>   Total optimization steps = 9,660\n",
      "[INFO|trainer.py:1721] 2024-08-14 18:14:24,456 >>   Number of trainable parameters = 167,772,160\n",
      "  0%|                                                  | 0/9660 [00:00<?, ?it/s][WARNING|logging.py:290] 2024-08-14 18:14:24,468 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.3696, 'learning_rate': 4.99896480331263e-05, 'epoch': 0.0}           \n",
      "{'loss': 0.2623, 'learning_rate': 4.997929606625259e-05, 'epoch': 0.0}          \n",
      "{'loss': 0.2194, 'learning_rate': 4.996894409937888e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.1604, 'learning_rate': 4.995859213250518e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2534, 'learning_rate': 4.994824016563147e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2674, 'learning_rate': 4.993788819875777e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2519, 'learning_rate': 4.9927536231884056e-05, 'epoch': 0.01}        \n",
      "{'loss': 0.1824, 'learning_rate': 4.991718426501035e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.0952, 'learning_rate': 4.9906832298136646e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1857, 'learning_rate': 4.989648033126294e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1155, 'learning_rate': 4.988612836438924e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1317, 'learning_rate': 4.9875776397515526e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1345, 'learning_rate': 4.986542443064182e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1447, 'learning_rate': 4.985507246376812e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1324, 'learning_rate': 4.984472049689442e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1444, 'learning_rate': 4.983436853002071e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.127, 'learning_rate': 4.9824016563147e-05, 'epoch': 0.04}            \n",
      "{'loss': 0.0527, 'learning_rate': 4.98136645962733e-05, 'epoch': 0.04}          \n",
      "{'loss': 0.1662, 'learning_rate': 4.980331262939959e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.0884, 'learning_rate': 4.979296066252588e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.0684, 'learning_rate': 4.9782608695652176e-05, 'epoch': 0.04}        \n",
      "{'loss': 0.0963, 'learning_rate': 4.977225672877847e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0674, 'learning_rate': 4.976190476190477e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0644, 'learning_rate': 4.975155279503106e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0634, 'learning_rate': 4.974120082815735e-05, 'epoch': 0.05}         \n",
      "  1%|â–                                      | 51/9660 [02:35<7:54:04,  2.96s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3 python src/finetune.py \\\n",
    "    --do_train --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_name_or_path 'meta-llama/Meta-Llama-3-8B' \\\n",
    "    --checkpoint_dir 'zjunlp/llama3-8b-iepile-lora' \\\n",
    "    --stage 'sft' \\\n",
    "    --model_name 'llama' \\\n",
    "    --template 'alpaca' \\\n",
    "    --train_file 'data2text/iepile-ner-train-transformed.json' \\\n",
    "    --valid_file 'data2text/iepile-ner-val-transformed.json' \\\n",
    "    --output_dir='lora/test' \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --optim \"adamw_torch\" \\\n",
    "    --max_source_length 400 \\\n",
    "    --cutoff_len 700 \\\n",
    "    --max_target_length 300 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 10 \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --bf16 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4ff6a-29d0-4f86-b2a5-b34df59d2f72",
   "metadata": {},
   "source": [
    "## æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c1065b-3a9d-49aa-9b09-9e5b3d69fda4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:13:46.519335Z",
     "iopub.status.busy": "2024-08-20T12:13:46.518591Z",
     "iopub.status.idle": "2024-08-20T12:15:44.576390Z",
     "shell.execute_reply": "2024-08-20T12:15:44.574864Z",
     "shell.execute_reply.started": "2024-08-20T12:13:46.519282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.6.0.17'), PosixPath('http'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/20/2024 20:13:52 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "08/20/2024 20:13:54 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:35<00:00,  8.81s/it]\n",
      "08/20/2024 20:14:30 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/20/2024 20:15:18 - INFO - model.adapter - Merged model checkpoint(s): ['lora/llama3-8B-iepile-data2text-continue'].\n",
      "08/20/2024 20:15:18 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): lora/llama3-8B-iepile-data2text-continue\n",
      "08/20/2024 20:15:18 - INFO - model.loader - trainable params: 0 || all params: 8198033408 || trainable%: 0.0000\n",
      "08/20/2024 20:15:18 - INFO - model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "Using pad_token, but it is not set yet.\n",
      "BOS:128000,<|begin_of_text|>\tEOS:128001,<|end_of_text|>\tPAD:None,None\n",
      "08/20/2024 20:15:18 - INFO - datamodule.template - Add pad token: <|end_of_text|>\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 289.12 examples/s]\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 82900, 498, 330, 21082, 498, 330, 89902, 33005, 498, 330, 36668, 33014, 31091, 8073, 330, 1379, 794, 330, 22, 11, 806, 9953, 508, 9080, 11, 101597, 29504, 42, 55273, 74318, 11, 101597, 29504, 11, 44510, 117, 17, 36668, 102778, 116187, 101545, 11, 36668, 102778, 116187, 101545, 11, 1041, 15, 13, 15, 11, 101597, 29504, 42, 55273, 74318, 11, 39442, 29172, 82317, 11, 108142, 104611, 44388, 69253, 118249, 36668, 102778, 116187, 101545, 6744, 252, 21990, 1272, 123000, 11, 108448, 103225, 104430, 83324, 64022, 30867, 21990, 115070, 21990, 9080, 38093, 11, 44510, 117, 17, 109474, 116187, 101545, 6744, 252, 21990, 1272, 123000, 11, 1272, 8107, 37507, 3922, 44510, 117, 17, 109474, 116187, 101545, 112736, 21043, 61786, 24273, 46091, 31640, 116187, 101545, 9554, 31944, 14276, 228, 91495, 102987, 106145, 9554, 109829, 35287, 102433, 1954, 29430, 116187, 101545, 5486, 104371, 27479, 103478, 20321, 120, 116187, 101545, 5486, 74090, 108870, 42246, 103478, 100179, 107784, 116187, 101545, 50667, 93994, 106444, 46091, 31640, 36668, 102778, 116187, 101545, 9554, 106246, 11, 23, 11, 20, 9953, 845, 9080, 11, 21589, 250, 101545, 105417, 11, 113231, 127037, 11, 73361, 12, 1758, 44, 103787, 99741, 74245, 105062, 33748, 11, 103787, 99741, 74245, 105062, 33748, 11, 16, 11, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 11, 110206, 101011, 70626, 11, 77413, 20834, 11, 74843, 77413, 11, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 30867, 106633, 11, 40265, 9080, 3922, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 19000, 110206, 101011, 70626, 30867, 106633, 3922, 55999, 107246, 73686, 124714, 972, 19483, 106444, 9554, 10866, 46729, 108174, 33122, 74843, 77413, 120793, 108399, 104959, 102776, 74245, 105062, 33748, 74318, 5486, 103478, 102491, 101011, 106499, 34208, 113231, 127037, 74245, 105062, 33748, 74318, 50667, 102616, 74245, 105062, 33748, 118329, 109098, 65455, 11, 24, 11, 46091, 103178, 117759, 36827, 11, 106499, 55038, 64467, 115683, 43323, 11, 39442, 31958, 35056, 11, 74245, 105062, 33748, 11, 39442, 31958, 35056, 11, 605, 13, 15, 11, 30537, 2495, 113911, 103626, 107611, 98245, 104743, 104412, 11, 68464, 49409, 107611, 127767, 11, 119662, 27384, 121790, 33748, 104336, 106297, 123796, 47770, 83324, 107163, 23039, 11, 127288, 35894, 69636, 72718, 11, 115890, 34048, 83266, 19000, 120288, 59464, 114223, 43240, 75140, 102146, 47523, 77195, 17297, 104987, 103181, 106556, 11, 46091, 103178, 117759, 36827, 68464, 49409, 107611, 127767, 102283, 94588, 15568, 108, 106101, 96, 3922, 106161, 100815, 106499, 55038, 64467, 115683, 43323, 15120, 71174, 103507, 17297, 3922, 30537, 2495, 113911, 103626, 107611, 98245, 104743, 104412, 9039, 95598, 107145, 74245, 105062, 33748, 103963, 33671, 127288, 35894, 69636, 72718, 107163, 104198, 103633, 70349, 55030, 48915, 9554, 45736, 53229, 107611, 127767, 3922, 119662, 27384, 121790, 33748, 104336, 106297, 123796, 47770, 83324, 107163, 23039, 3922, 115890, 34048, 83266, 19000, 120288, 59464, 114223, 43240, 75140, 102146, 47523, 77195, 17297, 104987, 103181, 106556, 1359, 633, 14711, 6075, 512]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"ä½ æ˜¯ä¸“é—¨è¿›è¡Œå®žä½“æŠ½å–çš„ä¸“å®¶ã€‚è¯·ä»Žinputä¸­æŠ½å–å‡ºç¬¦åˆschemaå®šä¹‰çš„å®žä½“ï¼Œä¸å­˜åœ¨çš„å®žä½“ç±»åž‹è¿”å›žç©ºåˆ—è¡¨ã€‚è¯·æŒ‰ç…§JSONå­—ç¬¦ä¸²çš„æ ¼å¼å›žç­”ã€‚\", \"schema\": [\"æ•°é‡\", \"æ—¶é—´\", \"ä»»åŠ¡ç±»åž‹\", \"ä¸»ä½“åç§°\"], \"input\": \"7,11æœˆ20æ—¥,å¾·å›½KMWå…¬å¸,å¾·å›½,è±¹2ä¸»æˆ˜å¦å…‹,ä¸»æˆ˜å¦å…‹,1000.0,å¾·å›½KMWå…¬å¸,æœªæåŠ,çºªå¿µè¿™æ¬¾ä¼ å¥‡ä¸»æˆ˜å¦å…‹è¯žç”Ÿ40å‘¨å¹´,ä¸¾åŠžäº†ä¸€åœºåˆ«å¼€ç”Ÿé¢çš„ç”Ÿæ—¥ä¼š,è±¹2ç³»åˆ—å¦å…‹è¯žç”Ÿ40å‘¨å¹´,40å¹´æ¥ï¼Œè±¹2ç³»åˆ—å¦å…‹ä¸€ç›´æ˜¯è¥¿æ–¹ä¸‰ä»£å¦å…‹çš„æ ‡æ†ï¼Œå¹¶æ·±åˆ»çš„å½±å“äº†æ—¥æœ¬90å¼å¦å…‹ã€å°åº¦é˜¿ç¼å¦å…‹ã€åœŸè€³å…¶é˜¿å°”æ³°å¦å…‹ç­‰å…¶ä»–å›½å®¶ä¸‰ä»£ä¸»æˆ˜å¦å…‹çš„å‘å±•,8,5æœˆ16æ—¥,å§œå…‹çº¢,ä¿„ç½—æ–¯,ç±³-35Mæ­¦è£…ç›´å‡æœº,æ­¦è£…ç›´å‡æœº,1,ç¬¬6å±Šä¿„ç½—æ–¯å›½é™…ç›´å‡æœºå·¥ä¸šå±•,èŽ«æ–¯ç§‘,å±•å‡º,å‚å±•,ç¬¬6å±Šä¿„ç½—æ–¯å›½é™…ç›´å‡æœºå·¥ä¸šå±•å¼€å¹•,å½“æ—¥ï¼Œç¬¬6å±Šä¿„ç½—æ–¯å›½é™…ç›´å‡æœºå·¥ä¸šå±•åœ¨èŽ«æ–¯ç§‘å¼€å¹•ï¼Œå…±å¸å¼•æ¥è‡ª18ä¸ªå›½å®¶çš„205å®¶åŽ‚å•†å‚å±•ï¼Œå…¶ä¸­åŒ…æ‹¬æ¬§æ´²ç›´å‡æœºå…¬å¸ã€é˜¿å¤æ–¯å¡”å’Œä¿„ç½—æ–¯ç›´å‡æœºå…¬å¸ç­‰ä¸–ç•Œç›´å‡æœºåˆ¶é€ å·¨å¤´,9,ä¸‰ä¹å¯’å¤©,å¡”å°æŒ‡æŒ¥å‘˜,æœªæ˜Žç¡®,ç›´å‡æœº,æœªæ˜Žç¡®,10.0,ç¬¬78é›†å›¢å†›æŸé™†èˆªæ—…,ä¸œåŒ—æŸæœºåœº,å¼€å±•å¤§è§„æ¨¡æœºç¾¤è¿œè·ç¦»è½¬åœºé£žè¡Œ,è…¾ç©ºè€Œèµ·,æé«˜éƒ¨é˜Ÿåœ¨å„ç§å¤æ‚å¤šå˜æ°”è±¡æ¡ä»¶ä¸‹æ”»é˜²èƒ½åŠ›,ä¸‰ä¹å¯’å¤©ä¸œåŒ—æŸæœºåœºé©¬è¾¾è½°é¸£ï¼Œéšç€å¡”å°æŒ‡æŒ¥å‘˜ä¸€å£°ä»¤ä¸‹ï¼Œç¬¬78é›†å›¢å†›æŸé™†èˆªæ—…æ•°åæž¶ç›´å‡æœºä¾æ¬¡è…¾ç©ºè€Œèµ·é£žå¾€åƒé‡Œä¹‹å¤–çš„é«˜åŽŸæŸæœºåœºï¼Œå¼€å±•å¤§è§„æ¨¡æœºç¾¤è¿œè·ç¦»è½¬åœºé£žè¡Œï¼Œæé«˜éƒ¨é˜Ÿåœ¨å„ç§å¤æ‚å¤šå˜æ°”è±¡æ¡ä»¶ä¸‹æ”»é˜²èƒ½åŠ›,\"}\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\"æ•°é‡\": [], \"æ—¶é—´\": [\"7,11æœˆ20æ—¥\", \"8,5æœˆ16æ—¥\", \"9\", \"10.0\"], \"ä»»åŠ¡ç±»åž‹\": [], \"ä¸»ä½“åç§°\": [\"KMWå…¬å¸\", \"æ¬§æ´²ç›´å‡æœºå…¬å¸ã€é˜¿å¤æ–¯å¡”ã€ä¿„ç½—æ–¯ç›´å‡æœºå…¬å¸\", \"ç¬¬78é›†å›¢å†›æŸé™†èˆªæ—…\"]}\n",
      "{\"åœ°ç‚¹\": [\"ä¸œåŒ—æŸæœºåœº\", \"é«˜åŽŸæŸæœºåœº\"], \"äººç‰©\": [], \"å¼€å§‹æ—¶é—´\": [], \"ä»»åŠ¡çŠ¶æ€\": []}\n",
      "{\"æŒ‡æŒ¥å‘˜\": [], \"è£…å¤‡\": [], \"ä¸»ä½“ç±»åž‹\": [\"å…¬å¸\", \"é™†èˆªæ—…\"], \"ç»„ç»‡æœºæž„\": []}\n",
      "{\"å›½å®¶\": [\"å¾·å›½\"], \"å›½åˆ«\": [], \"å†›ç§\": [], \"å¤‡æ³¨\": []}\n",
      "{\"è£…å¤‡åç§°\": [], \"å•ä½\": [], \"äººæ•°\": [], \"è£…å¤‡æ•°é‡\": []}\n",
      "{\"ä¸»ä½“æ•°é‡\": [\"1\", \"æ•°åæž¶\"], \"è£…å¤‡ç±»åž‹\": []}\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python src/inference.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path 'meta-llama/Meta-Llama-3-8B' \\\n",
    "    --checkpoint_dir 'lora/llama3-8B-iepile-data2text-continue' \\\n",
    "    --model_name 'llama' \\\n",
    "    --template 'alpaca' \\\n",
    "    --do_predict \\\n",
    "    --input_file 'data2text/data2text-test.json' \\\n",
    "    --output_file 'results/test-useless.json' \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir 'lora/oneke-continue-test' \\\n",
    "    --predict_with_generate \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --bf16 \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --bits 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c5c0-d783-4405-978b-fdf0ba393d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
