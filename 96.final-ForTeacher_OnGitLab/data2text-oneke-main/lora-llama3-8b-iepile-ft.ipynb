{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8873a6f-abab-478d-8485-86ad234b3264",
   "metadata": {},
   "source": [
    "# Llama-8B IEPile-data2text LoRA微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44102d9-9b87-4ea9-a5f6-e3ab0505fb38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:07:37.013026Z",
     "iopub.status.busy": "2024-08-20T12:07:37.012266Z",
     "iopub.status.idle": "2024-08-20T12:07:37.022846Z",
     "shell.execute_reply": "2024-08-20T12:07:37.022014Z",
     "shell.execute_reply.started": "2024-08-20T12:07:37.012983Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.6.0.17:8888\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b14750-157d-4659-9d5e-b9a64292f6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:07:38.472355Z",
     "iopub.status.busy": "2024-08-20T12:07:38.471620Z",
     "iopub.status.idle": "2024-08-20T12:07:38.478556Z",
     "shell.execute_reply": "2024-08-20T12:07:38.477383Z",
     "shell.execute_reply.started": "2024-08-20T12:07:38.472301Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28150793-57cf-45c3-a9cc-97def0e42420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:05:48.474884Z",
     "iopub.status.busy": "2024-08-20T12:05:48.474059Z",
     "iopub.status.idle": "2024-08-20T12:05:53.308214Z",
     "shell.execute_reply": "2024-08-20T12:05:53.307150Z",
     "shell.execute_reply.started": "2024-08-20T12:05:48.474831Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8888'), PosixPath('//10.6.0.17'), PosixPath('http')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import _utils\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B'\n",
    "lora_path = 'lora/llama3-8B-iepile-data2text-continue'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9bc06e-be22-4b25-b0cb-7e74f1b124dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:05:57.479397Z",
     "iopub.status.busy": "2024-08-20T12:05:57.478628Z",
     "iopub.status.idle": "2024-08-20T12:07:16.330267Z",
     "shell.execute_reply": "2024-08-20T12:07:16.328975Z",
     "shell.execute_reply.started": "2024-08-20T12:05:57.479345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     model_path,\n\u001b[1;32m      3\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:270\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, config, adapter_name)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m model\u001b[38;5;241m.\u001b[39mload_adapter(model_id, adapter_name, is_trainable\u001b[38;5;241m=\u001b[39mis_trainable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:893\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, peft_config: PeftConfig, adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/peft_model.py:112\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m peft_config\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mPEFT_TYPE_TO_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:180\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# transformers models have a .config attribute, whose presence is assumed later on\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:194\u001b[0m, in \u001b[0;36mLoraModel.add_adapter\u001b[0;34m(self, adapter_name, config)\u001b[0m\n\u001b[1;32m    192\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_lora_config(config, model_config)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for all adapters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:352\u001b[0m, in \u001b[0;36mLoraModel._find_and_replace\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m         target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    345\u001b[0m             adapter_name,\n\u001b[1;32m    346\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39mr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m             lora_config\u001b[38;5;241m.\u001b[39minit_lora_weights,\n\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, new_module, target)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:309\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(self, lora_config, adapter_name, target)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, only `torch.nn.Linear` and `Conv1D` are supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/peft/tuners/lora.py:765\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, adapter_name, in_features, out_features, r, lora_alpha, lora_dropout, fan_in_fan_out, is_target_conv_1d_layer, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    753\u001b[0m     adapter_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    762\u001b[0m ):\n\u001b[1;32m    763\u001b[0m     init_lora_weights \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_lora_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 765\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     LoraLayer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features\u001b[38;5;241m=\u001b[39min_features, out_features\u001b[38;5;241m=\u001b[39mout_features)\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# Freezing the pre-trained weight matrix\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce7f22-9a31-4184-8380-06ae97e25d5c",
   "metadata": {},
   "source": [
    "## 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c8a924-f103-44a6-b909-ff93c7b078ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T10:12:47.128856Z",
     "iopub.status.busy": "2024-08-14T10:12:47.128047Z",
     "iopub.status.idle": "2024-08-14T10:17:03.437003Z",
     "shell.execute_reply": "2024-08-14T10:17:03.435440Z",
     "shell.execute_reply.started": "2024-08-14T10:12:47.128817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//10.6.0.17'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/14/2024 18:12:50 - WARNING - args.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1332] 2024-08-14 18:12:50,995 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1764] 2024-08-14 18:12:50,995 >> PyTorch: setting up devices\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "08/14/2024 18:12:50 - INFO - args.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "08/14/2024 18:12:50 - INFO - args.parser - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_18-12-50_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/14/2024 18:12:50 - INFO - __main__ - Start Time: 2024:08:14 18:12:50\n",
      "08/14/2024 18:12:50 - INFO - __main__ - model_args:ModelArguments(model_name_or_path='meta-llama/Meta-Llama-3-8B', model_name='llama', cache_dir=None, use_fast_tokenizer=True, trust_remote_code=True, use_auth_token=False, model_revision='main', split_special_tokens=False, bits=4, adam8bit=False, double_quant=True, quant_type='nf4', checkpoint_dir=['zjunlp/llama3-8b-iepile-lora'])\n",
      "data_args:DataArguments(train_file='data2text/iepile-ner-train-transformed.json', valid_file='data2text/iepile-ner-val-transformed.json', predict_file=None, preprocessing_num_workers=16, overwrite_cache=False, cache_path=None, template='alpaca', system_prompt=None, max_source_length=400, max_target_length=300, cutoff_len=700, val_set_size=1000, pad_to_max_length=False, ignore_pad_token_for_loss=True, train_on_prompt=False, language='zh', id_text='input')\n",
      "training_args:TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=lora/test/runs/Aug14_18-12-50_a51f78e3e7ee,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=2,\n",
      "logging_strategy=steps,\n",
      "loss_scale=1.0,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=lora/test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=lora/test,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=10,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "finetuning_args:FinetuningArguments(dpo_beta=0.1, ppo_logger=None, ppo_score_norm=False, ppo_target=6.0, ppo_whiten_rewards=False, ref_model=None, ref_model_checkpoint=None, ref_model_quantization_bit=None, reward_model=None, reward_model_checkpoint=None, reward_model_quantization_bit=None, reward_model_type='lora', lora_r=64, lora_alpha=64.0, lora_dropout=0.05, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], additional_target=None, resume_lora_training=True, num_layer_trainable=3, name_module_trainable=['mlp'], stage='sft', finetuning_type='lora', upcast_layernorm=False, neft_alpha=0, export_dir=None, plot_loss=False)\n",
      "generating_args:GenerationArguments(max_length=512, max_new_tokens=256, min_new_tokens=None, do_sample=False, num_beams=1, num_beam_groups=1, penalty_alpha=None, use_cache=True, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, diversity_penalty=0.0, repetition_penalty=1.0, length_penalty=1.0, no_repeat_ngram_size=0)\n",
      "08/14/2024 18:12:50 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "trainer_class:<class 'transformers.trainer.Trainer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,785 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2024-08-14 18:12:51,786 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:715] 2024-08-14 18:12:52,392 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-08-14 18:12:52,396 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.33.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "08/14/2024 18:12:52 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:2857] 2024-08-14 18:12:52,444 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1200] 2024-08-14 18:12:52,447 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 18:12:52,448 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2971] 2024-08-14 18:12:52,707 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:34<00:00,  8.70s/it]\n",
      "[INFO|modeling_utils.py:3643] 2024-08-14 18:13:27,772 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3651] 2024-08-14 18:13:27,772 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:730] 2024-08-14 18:13:28,075 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2024-08-14 18:13:28,076 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.33.0\"\n",
      "}\n",
      "\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Gradient checkpointing enabled.\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/14/2024 18:13:28 - INFO - model.adapter - Resume model checkpoint(s): zjunlp/llama3-8b-iepile-lora .\n",
      "08/14/2024 18:14:20 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): zjunlp/llama3-8b-iepile-lora\n",
      "08/14/2024 18:14:20 - INFO - model.loader - trainable params: 167772160 || all params: 8198033408 || trainable%: 2.0465\n",
      "[ERROR|tokenization_utils_base.py:1061] 2024-08-14 18:14:20,146 >> Using pad_token, but it is not set yet.\n",
      "08/14/2024 18:14:20 - INFO - __main__ - BOS:128000,<|begin_of_text|>\tEOS:128001,<|end_of_text|>\tPAD:None,None\n",
      "Using custom data configuration default-230b0cd978e532d9\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Using custom data configuration default-c20be129830f148f\n",
      "Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "08/14/2024 18:14:22 - INFO - datamodule.template - Add pad token: <|end_of_text|>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:14:22,175 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-230b0cd978e532d9/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d06569e973064b69_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 98390, 498, 330, 82900, 498, 330, 56386, 21082, 498, 330, 21082, 8073, 330, 1379, 794, 330, 16423, 109615, 35894, 103626, 117805, 76537, 53826, 65743, 103507, 34048, 52563, 119491, 30590, 21082, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 117925, 19000, 104463, 29741, 114618, 35894, 103626, 124426, 29391, 105644, 104430, 109577, 39442, 120988, 28466, 115039, 65455, 9554, 70821, 104443, 71567, 95, 102776, 104153, 115039, 45893, 65820, 115039, 105610, 126841, 82805, 65820, 115039, 73548, 122471, 33671, 42421, 42462, 119298, 29391, 105644, 35287, 48044, 88356, 17701, 107163, 23039, 32648, 3922, 19653, 107163, 23039, 95337, 25073, 15, 103633, 73361, 34547, 116875, 94588, 8192, 116, 13647, 122, 102138, 87412, 17920, 223, 1811, 64259, 14711, 6075, 512, 5018, 98390, 794, 10277, 330, 82900, 794, 10277, 330, 56386, 21082, 794, 10277, 330, 21082, 794, 4482, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 93546, 128001]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\", \"schema\": [\"备注\", \"数量\", \"开始时间\", \"时间\"], \"input\": \"据美国空军全球打击司令部表示，当地时间2月5日12时33分，他们在范登堡空军基地发射了一枚未携载弹头的民兵Ⅲ洲际弹道导弹，以进一步测试导弹系统。此次试验同步发射了一个再入飞行器，成功飞行约6920千米后抵达夸贾林环礁。\"}\n",
      "\n",
      "### Response:\n",
      "{\"备注\": [], \"数量\": [], \"开始时间\": [], \"时间\": [\"2月5日12时33分\"]}<|end_of_text|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 98390, 794, 10277, 330, 82900, 794, 10277, 330, 56386, 21082, 794, 10277, 330, 21082, 794, 4482, 17, 9953, 20, 9080, 717, 13646, 1644, 17620, 93546, 128001]\n",
      "labels:\n",
      "{\"备注\": [], \"数量\": [], \"开始时间\": [], \"时间\": [\"2月5日12时33分\"]}<|end_of_text|>\n",
      "[INFO|tokenization_utils_base.py:926] 2024-08-14 18:14:22,391 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00000_of_00016.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00001_of_00016.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00002_of_00016.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00003_of_00016.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00004_of_00016.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00005_of_00016.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00006_of_00016.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00007_of_00016.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00008_of_00016.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00009_of_00016.arrow\n",
      "Process #10 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00010_of_00016.arrow\n",
      "Process #11 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00011_of_00016.arrow\n",
      "Process #12 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00012_of_00016.arrow\n",
      "Process #13 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00013_of_00016.arrow\n",
      "Process #14 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00014_of_00016.arrow\n",
      "Process #15 will write at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_00015_of_00016.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-c20be129830f148f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-414af73419a5cb2c_*_of_00016.arrow\n",
      "Concatenating 16 shards\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 29504, 64022, 498, 330, 89902, 45191, 498, 330, 103626, 87502, 498, 330, 64467, 115683, 43323, 8073, 330, 1379, 794, 330, 105351, 66378, 103478, 73325, 107024, 37767, 45932, 222, 15225, 109615, 74318, 124080, 106246, 70821, 11883, 72237, 27327, 74445, 91495, 52563, 45163, 96511, 26130, 35304, 70821, 11883, 124177, 102836, 38093, 45163, 72237, 104696, 114593, 103626, 30926, 11883, 108968, 1811, 16423, 87327, 3922, 61786, 102571, 74318, 97655, 58318, 93994, 109615, 74318, 72917, 110695, 106053, 3922, 109703, 116166, 48044, 122933, 105231, 3922, 26892, 67178, 121637, 9039, 95598, 54456, 119645, 9554, 78640, 103605, 121287, 125985, 1811, 64259, 14711, 6075, 512, 5018, 29504, 64022, 794, 10277, 330, 89902, 45191, 794, 10277, 330, 103626, 87502, 794, 10277, 330, 64467, 115683, 43323, 794, 3132, 92, 128001]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\", \"schema\": [\"国别\", \"任务状态\", \"军种\", \"指挥员\"], \"input\": \"沙特阿拉伯已邀请美国公司参与发展民用核能项目，并表示将专注于民用领域，不会将核技术用于军事用途。据报道，西屋公司正在与其他美国公司进行谈判，计划成立一个联合企业，建造价值数十亿美元的两座反应堆。\"}\n",
      "\n",
      "### Response:\n",
      "{\"国别\": [], \"任务状态\": [], \"军种\": [], \"指挥员\": []}<|end_of_text|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 29504, 64022, 794, 10277, 330, 89902, 45191, 794, 10277, 330, 103626, 87502, 794, 10277, 330, 64467, 115683, 43323, 794, 3132, 92, 128001]\n",
      "labels:\n",
      "{\"国别\": [], \"任务状态\": [], \"军种\": [], \"指挥员\": []}<|end_of_text|>\n",
      "[INFO|trainer.py:403] 2024-08-14 18:14:22,566 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.\n",
      "08/14/2024 18:14:22 - INFO - __main__ - *** Train ***\n",
      "08/14/2024 18:14:22 - INFO - __main__ - resume_from_checkpoint: None\n",
      "[INFO|trainer.py:1712] 2024-08-14 18:14:24,452 >> ***** Running training *****\n",
      "[INFO|trainer.py:1713] 2024-08-14 18:14:24,452 >>   Num examples = 7,732\n",
      "[INFO|trainer.py:1714] 2024-08-14 18:14:24,452 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1715] 2024-08-14 18:14:24,452 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1718] 2024-08-14 18:14:24,452 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1719] 2024-08-14 18:14:24,452 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1720] 2024-08-14 18:14:24,452 >>   Total optimization steps = 9,660\n",
      "[INFO|trainer.py:1721] 2024-08-14 18:14:24,456 >>   Number of trainable parameters = 167,772,160\n",
      "  0%|                                                  | 0/9660 [00:00<?, ?it/s][WARNING|logging.py:290] 2024-08-14 18:14:24,468 >> You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.3696, 'learning_rate': 4.99896480331263e-05, 'epoch': 0.0}           \n",
      "{'loss': 0.2623, 'learning_rate': 4.997929606625259e-05, 'epoch': 0.0}          \n",
      "{'loss': 0.2194, 'learning_rate': 4.996894409937888e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.1604, 'learning_rate': 4.995859213250518e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2534, 'learning_rate': 4.994824016563147e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2674, 'learning_rate': 4.993788819875777e-05, 'epoch': 0.01}         \n",
      "{'loss': 0.2519, 'learning_rate': 4.9927536231884056e-05, 'epoch': 0.01}        \n",
      "{'loss': 0.1824, 'learning_rate': 4.991718426501035e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.0952, 'learning_rate': 4.9906832298136646e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1857, 'learning_rate': 4.989648033126294e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1155, 'learning_rate': 4.988612836438924e-05, 'epoch': 0.02}         \n",
      "{'loss': 0.1317, 'learning_rate': 4.9875776397515526e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.1345, 'learning_rate': 4.986542443064182e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1447, 'learning_rate': 4.985507246376812e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1324, 'learning_rate': 4.984472049689442e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.1444, 'learning_rate': 4.983436853002071e-05, 'epoch': 0.03}         \n",
      "{'loss': 0.127, 'learning_rate': 4.9824016563147e-05, 'epoch': 0.04}            \n",
      "{'loss': 0.0527, 'learning_rate': 4.98136645962733e-05, 'epoch': 0.04}          \n",
      "{'loss': 0.1662, 'learning_rate': 4.980331262939959e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.0884, 'learning_rate': 4.979296066252588e-05, 'epoch': 0.04}         \n",
      "{'loss': 0.0684, 'learning_rate': 4.9782608695652176e-05, 'epoch': 0.04}        \n",
      "{'loss': 0.0963, 'learning_rate': 4.977225672877847e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0674, 'learning_rate': 4.976190476190477e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0644, 'learning_rate': 4.975155279503106e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.0634, 'learning_rate': 4.974120082815735e-05, 'epoch': 0.05}         \n",
      "  1%|▏                                      | 51/9660 [02:35<7:54:04,  2.96s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3 python src/finetune.py \\\n",
    "    --do_train --do_eval \\\n",
    "    --overwrite_output_dir \\\n",
    "    --model_name_or_path 'meta-llama/Meta-Llama-3-8B' \\\n",
    "    --checkpoint_dir 'zjunlp/llama3-8b-iepile-lora' \\\n",
    "    --stage 'sft' \\\n",
    "    --model_name 'llama' \\\n",
    "    --template 'alpaca' \\\n",
    "    --train_file 'data2text/iepile-ner-train-transformed.json' \\\n",
    "    --valid_file 'data2text/iepile-ner-val-transformed.json' \\\n",
    "    --output_dir='lora/test' \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_grad_norm 0.5 \\\n",
    "    --optim \"adamw_torch\" \\\n",
    "    --max_source_length 400 \\\n",
    "    --cutoff_len 700 \\\n",
    "    --max_target_length 300 \\\n",
    "    --evaluation_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 10 \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 64 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --bf16 \\\n",
    "    --bits 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4ff6a-29d0-4f86-b2a5-b34df59d2f72",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c1065b-3a9d-49aa-9b09-9e5b3d69fda4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:13:46.519335Z",
     "iopub.status.busy": "2024-08-20T12:13:46.518591Z",
     "iopub.status.idle": "2024-08-20T12:15:44.576390Z",
     "shell.execute_reply": "2024-08-20T12:15:44.574864Z",
     "shell.execute_reply.started": "2024-08-20T12:13:46.519282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.6.0.17'), PosixPath('http'), PosixPath('8888')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "08/20/2024 20:13:52 - INFO - __main__ - model_class:<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>\n",
      "tokenizer_class:<class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>\n",
      "\n",
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "08/20/2024 20:13:54 - INFO - model.loader - Quantizing model to 4 bit.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:35<00:00,  8.81s/it]\n",
      "08/20/2024 20:14:30 - INFO - model.adapter - Fine-tuning method: LoRA\n",
      "08/20/2024 20:15:18 - INFO - model.adapter - Merged model checkpoint(s): ['lora/llama3-8B-iepile-data2text-continue'].\n",
      "08/20/2024 20:15:18 - INFO - model.adapter - Loaded fine-tuned model from checkpoint(s): lora/llama3-8B-iepile-data2text-continue\n",
      "08/20/2024 20:15:18 - INFO - model.loader - trainable params: 0 || all params: 8198033408 || trainable%: 0.0000\n",
      "08/20/2024 20:15:18 - INFO - model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "Using pad_token, but it is not set yet.\n",
      "BOS:128000,<|begin_of_text|>\tEOS:128001,<|end_of_text|>\tPAD:None,None\n",
      "08/20/2024 20:15:18 - INFO - datamodule.template - Add pad token: <|end_of_text|>\n",
      "Map: 100%|████████████████████████████████| 6/6 [00:00<00:00, 289.12 examples/s]\n",
      "input_ids:\n",
      "[128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 13, 271, 14711, 30151, 512, 5018, 56074, 794, 330, 57668, 21043, 96511, 65789, 72917, 41073, 33014, 116602, 18655, 9554, 96511, 46729, 1811, 15225, 46281, 1379, 16325, 116602, 18655, 20834, 119292, 17801, 92382, 9554, 41073, 33014, 3922, 70284, 9554, 41073, 33014, 33005, 32626, 35894, 45277, 1811, 15225, 117026, 5483, 67658, 9554, 69905, 113925, 1811, 498, 330, 17801, 794, 4482, 82900, 498, 330, 21082, 498, 330, 89902, 33005, 498, 330, 36668, 33014, 31091, 8073, 330, 1379, 794, 330, 22, 11, 806, 9953, 508, 9080, 11, 101597, 29504, 42, 55273, 74318, 11, 101597, 29504, 11, 44510, 117, 17, 36668, 102778, 116187, 101545, 11, 36668, 102778, 116187, 101545, 11, 1041, 15, 13, 15, 11, 101597, 29504, 42, 55273, 74318, 11, 39442, 29172, 82317, 11, 108142, 104611, 44388, 69253, 118249, 36668, 102778, 116187, 101545, 6744, 252, 21990, 1272, 123000, 11, 108448, 103225, 104430, 83324, 64022, 30867, 21990, 115070, 21990, 9080, 38093, 11, 44510, 117, 17, 109474, 116187, 101545, 6744, 252, 21990, 1272, 123000, 11, 1272, 8107, 37507, 3922, 44510, 117, 17, 109474, 116187, 101545, 112736, 21043, 61786, 24273, 46091, 31640, 116187, 101545, 9554, 31944, 14276, 228, 91495, 102987, 106145, 9554, 109829, 35287, 102433, 1954, 29430, 116187, 101545, 5486, 104371, 27479, 103478, 20321, 120, 116187, 101545, 5486, 74090, 108870, 42246, 103478, 100179, 107784, 116187, 101545, 50667, 93994, 106444, 46091, 31640, 36668, 102778, 116187, 101545, 9554, 106246, 11, 23, 11, 20, 9953, 845, 9080, 11, 21589, 250, 101545, 105417, 11, 113231, 127037, 11, 73361, 12, 1758, 44, 103787, 99741, 74245, 105062, 33748, 11, 103787, 99741, 74245, 105062, 33748, 11, 16, 11, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 11, 110206, 101011, 70626, 11, 77413, 20834, 11, 74843, 77413, 11, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 30867, 106633, 11, 40265, 9080, 3922, 30537, 21, 109948, 113231, 127037, 107604, 74245, 105062, 33748, 116788, 77413, 19000, 110206, 101011, 70626, 30867, 106633, 3922, 55999, 107246, 73686, 124714, 972, 19483, 106444, 9554, 10866, 46729, 108174, 33122, 74843, 77413, 120793, 108399, 104959, 102776, 74245, 105062, 33748, 74318, 5486, 103478, 102491, 101011, 106499, 34208, 113231, 127037, 74245, 105062, 33748, 74318, 50667, 102616, 74245, 105062, 33748, 118329, 109098, 65455, 11, 24, 11, 46091, 103178, 117759, 36827, 11, 106499, 55038, 64467, 115683, 43323, 11, 39442, 31958, 35056, 11, 74245, 105062, 33748, 11, 39442, 31958, 35056, 11, 605, 13, 15, 11, 30537, 2495, 113911, 103626, 107611, 98245, 104743, 104412, 11, 68464, 49409, 107611, 127767, 11, 119662, 27384, 121790, 33748, 104336, 106297, 123796, 47770, 83324, 107163, 23039, 11, 127288, 35894, 69636, 72718, 11, 115890, 34048, 83266, 19000, 120288, 59464, 114223, 43240, 75140, 102146, 47523, 77195, 17297, 104987, 103181, 106556, 11, 46091, 103178, 117759, 36827, 68464, 49409, 107611, 127767, 102283, 94588, 15568, 108, 106101, 96, 3922, 106161, 100815, 106499, 55038, 64467, 115683, 43323, 15120, 71174, 103507, 17297, 3922, 30537, 2495, 113911, 103626, 107611, 98245, 104743, 104412, 9039, 95598, 107145, 74245, 105062, 33748, 103963, 33671, 127288, 35894, 69636, 72718, 107163, 104198, 103633, 70349, 55030, 48915, 9554, 45736, 53229, 107611, 127767, 3922, 119662, 27384, 121790, 33748, 104336, 106297, 123796, 47770, 83324, 107163, 23039, 3922, 115890, 34048, 83266, 19000, 120288, 59464, 114223, 43240, 75140, 102146, 47523, 77195, 17297, 104987, 103181, 106556, 1359, 633, 14711, 6075, 512]\n",
      "inputs:\n",
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "{\"instruction\": \"你是专门进行实体抽取的专家。请从input中抽取出符合schema定义的实体，不存在的实体类型返回空列表。请按照JSON字符串的格式回答。\", \"schema\": [\"数量\", \"时间\", \"任务类型\", \"主体名称\"], \"input\": \"7,11月20日,德国KMW公司,德国,豹2主战坦克,主战坦克,1000.0,德国KMW公司,未提及,纪念这款传奇主战坦克诞生40周年,举办了一场别开生面的生日会,豹2系列坦克诞生40周年,40年来，豹2系列坦克一直是西方三代坦克的标杆，并深刻的影响了日本90式坦克、印度阿琼坦克、土耳其阿尔泰坦克等其他国家三代主战坦克的发展,8,5月16日,姜克红,俄罗斯,米-35M武装直升机,武装直升机,1,第6届俄罗斯国际直升机工业展,莫斯科,展出,参展,第6届俄罗斯国际直升机工业展开幕,当日，第6届俄罗斯国际直升机工业展在莫斯科开幕，共吸引来自18个国家的205家厂商参展，其中包括欧洲直升机公司、阿古斯塔和俄罗斯直升机公司等世界直升机制造巨头,9,三九寒天,塔台指挥员,未明确,直升机,未明确,10.0,第78集团军某陆航旅,东北某机场,开展大规模机群远距离转场飞行,腾空而起,提高部队在各种复杂多变气象条件下攻防能力,三九寒天东北某机场马达轰鸣，随着塔台指挥员一声令下，第78集团军某陆航旅数十架直升机依次腾空而起飞往千里之外的高原某机场，开展大规模机群远距离转场飞行，提高部队在各种复杂多变气象条件下攻防能力,\"}\n",
      "\n",
      "### Response:\n",
      "\n",
      "{\"数量\": [], \"时间\": [\"7,11月20日\", \"8,5月16日\", \"9\", \"10.0\"], \"任务类型\": [], \"主体名称\": [\"KMW公司\", \"欧洲直升机公司、阿古斯塔、俄罗斯直升机公司\", \"第78集团军某陆航旅\"]}\n",
      "{\"地点\": [\"东北某机场\", \"高原某机场\"], \"人物\": [], \"开始时间\": [], \"任务状态\": []}\n",
      "{\"指挥员\": [], \"装备\": [], \"主体类型\": [\"公司\", \"陆航旅\"], \"组织机构\": []}\n",
      "{\"国家\": [\"德国\"], \"国别\": [], \"军种\": [], \"备注\": []}\n",
      "{\"装备名称\": [], \"单位\": [], \"人数\": [], \"装备数量\": []}\n",
      "{\"主体数量\": [\"1\", \"数十架\"], \"装备类型\": []}\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python src/inference.py \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path 'meta-llama/Meta-Llama-3-8B' \\\n",
    "    --checkpoint_dir 'lora/llama3-8B-iepile-data2text-continue' \\\n",
    "    --model_name 'llama' \\\n",
    "    --template 'alpaca' \\\n",
    "    --do_predict \\\n",
    "    --input_file 'data2text/data2text-test.json' \\\n",
    "    --output_file 'results/test-useless.json' \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir 'lora/oneke-continue-test' \\\n",
    "    --predict_with_generate \\\n",
    "    --cutoff_len 2048 \\\n",
    "    --bf16 \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --bits 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c5c0-d783-4405-978b-fdf0ba393d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
