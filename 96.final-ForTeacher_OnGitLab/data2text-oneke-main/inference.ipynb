{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9843f8cf-ed41-4612-897d-4b352e8e270b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T05:15:52.904482Z",
     "iopub.status.busy": "2024-08-16T05:15:52.903502Z",
     "iopub.status.idle": "2024-08-16T05:15:52.912470Z",
     "shell.execute_reply": "2024-08-16T05:15:52.911776Z",
     "shell.execute_reply.started": "2024-08-16T05:15:52.904423Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.6.0.17:8888\"\n",
    "os.environ[\"HF_TOKEN\"] = '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'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f5e0d6-1adc-4089-979f-7733a2364e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T05:15:56.825195Z",
     "iopub.status.busy": "2024-08-16T05:15:56.824344Z",
     "iopub.status.idle": "2024-08-16T05:15:59.846836Z",
     "shell.execute_reply": "2024-08-16T05:15:59.845871Z",
     "shell.execute_reply.started": "2024-08-16T05:15:56.825132Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//pypi.tuna.tsinghua.edu.cn/simple'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//10.6.0.17'), PosixPath('http'), PosixPath('8888')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d44e85-68cd-484f-9447-a4ee5365ba50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T05:16:08.824401Z",
     "iopub.status.busy": "2024-08-16T05:16:08.823642Z",
     "iopub.status.idle": "2024-08-16T05:16:10.063015Z",
     "shell.execute_reply": "2024-08-16T05:16:10.062498Z",
     "shell.execute_reply.started": "2024-08-16T05:16:08.824347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'meta-llama/Meta-Llama-3-8B' #选择你下载的模型存储在本地的位置\n",
    "# lora_path = 'zjunlp/llama3-8b-iepile-lora'\n",
    "lora_path = 'lora/llama3-8B-iepile-data2text-continue'\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78eb0468-703e-4f20-aefc-3c413f7bc108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T02:54:46.820340Z",
     "iopub.status.busy": "2024-08-16T02:54:46.819452Z",
     "iopub.status.idle": "2024-08-16T02:54:46.829488Z",
     "shell.execute_reply": "2024-08-16T02:54:46.828578Z",
     "shell.execute_reply.started": "2024-08-16T02:54:46.820279Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 4bit量化\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a7e003-8ac5-488e-8d68-565e6eb4f066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T05:16:14.824264Z",
     "iopub.status.busy": "2024-08-16T05:16:14.823467Z",
     "iopub.status.idle": "2024-08-16T05:17:44.099709Z",
     "shell.execute_reply": "2024-08-16T05:17:44.098174Z",
     "shell.execute_reply.started": "2024-08-16T05:16:14.824224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=1024, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(\n",
       "                in_features=4096, out_features=14336, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear(\n",
       "                in_features=4096, out_features=14336, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): Linear(\n",
       "                in_features=14336, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    device_map=\"cuda\",\n",
    "    # quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_path,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2d904e-856e-40c8-8ee1-4c178631731a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T10:04:26.521614Z",
     "iopub.status.busy": "2024-08-16T10:04:26.520632Z",
     "iopub.status.idle": "2024-08-16T10:04:31.349825Z",
     "shell.execute_reply": "2024-08-16T10:04:31.348726Z",
     "shell.execute_reply.started": "2024-08-16T10:04:26.521568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"时间\": [\"7,11月20日\"], \"人物\": [], \"国家\": [\"德国\"], \"装备名称\": [], \"装备类型\": [], \"装备数量\": [], \"组织机构\": [\"德国KMW公司\"], \"地点\": [\"德国\"], \"任务\": [], \"行动\": [], \"事件\": [\"豹2系列坦克诞生40周年\"], \"备注\": []}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = '<<SYS>>\\n 你是一个乐于助人的助手。\\n<</SYS>>\\n\\n'\n",
    "# sintruct = json.dumps(\n",
    "#     {\n",
    "#         \"instruction\": \"你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。\", \n",
    "#         \"schema\": [\"时间\", \"人物\", \"国家\", \"装备名称\", \"装备类型\", \"装备数量\", \"组织机构\", \"地点\"], \n",
    "#         \"input\": \"印度海军的梅尔华拉海军上将透露，明年印度将会下水第1艘完全自主生产的航母，并计划在2014年实现服役，同时第2艘国产航母的建造也将得到批准。此外，排水量为40000吨的印度国产防空舰正在科钦船厂建造，而维拉特号则将于2013年退役。\"\n",
    "#     },\n",
    "#     ensure_ascii=False\n",
    "# )\n",
    "sintruct = json.dumps(\n",
    "    {\n",
    "        \"instruction\": \"你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。\", \n",
    "        \"schema\": [\"时间\",\"人物\",\"国家\",\"装备名称\",\"装备类型\",\"装备数量\",\"组织机构\",\"地点\",\"任务\",\"行动\",\"事件\",\"备注\"], \n",
    "        \"input\": \"7,11月20日,德国KMW公司,德国,豹2主战坦克,主战坦克,1000.0,德国KMW公司,未提及,纪念这款传奇主战坦克诞生40周年,举办了一场别开生面的生日会,豹2系列坦克诞生40周年,40年来，豹2系列坦克一直是西方三代坦克的标杆，并深刻的影响了日本90式坦克、印度阿琼坦克、土耳其阿尔泰坦克等其他国家三代主战坦克的发展,8,5月16日,姜克红,俄罗斯,米-35M武装直升机,武装直升机,1,第6届俄罗斯国际直升机工业展,莫斯科,展出,参展,第6届俄罗斯国际直升机工业展开幕,当日，第6届俄罗斯国际直升机工业展在莫斯科开幕，共吸引来自18个国家的205家厂商参展，其中包括欧洲直升机公司、阿古斯塔和俄罗斯直升机公司等世界直升机制造巨头,9,三九寒天,塔台指挥员,未明确,直升机,未明确,10.0,第78集团军某陆航旅,东北某机场,开展大规模机群远距离转场飞行,腾空而起,提高部队在各种复杂多变气象条件下攻防能力,三九寒天东北某机场马达轰鸣，随着塔台指挥员一声令下，第78集团军某陆航旅数十架直升机依次腾空而起飞往千里之外的高原某机场，开展大规模机群远距离转场飞行，提高部队在各种复杂多变气象条件下攻防能力,\"\n",
    "    }, \n",
    "    ensure_ascii=False\n",
    ")\n",
    "sintruct = '[INST] ' + system_prompt + sintruct + ' [/INST]'\n",
    "# sintruct = '<reserved_106>' + system_prompt + sintruct + '<reserved_107>'\n",
    "\n",
    "input_ids = tokenizer.encode(sintruct, return_tensors=\"pt\").to(device)\n",
    "input_length = input_ids.size(1)\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        max_length=1024,\n",
    "        max_new_tokens=256,\n",
    "        return_dict_in_generate=True),\n",
    "    pad_token_id=tokenizer.eos_token_id)\n",
    "generation_output = generation_output.sequences[0]\n",
    "generation_output = generation_output[input_length:]\n",
    "output = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b87c00-1fd1-48ce-b99a-01e5c1ac72e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
