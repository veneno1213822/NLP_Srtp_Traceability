{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048196ed-19b0-4b37-ae1a-e2f1a8e762a5",
   "metadata": {},
   "source": [
    "# 训练数据生成\n",
    "进行内部数据转换，生成模型微调所需训练和测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b182a6-98a6-4352-a1d0-51a04caa75e9",
   "metadata": {},
   "source": [
    "### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8273dc35-b80d-4f61-aa80-6d33f00225fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:13:10.223562Z",
     "iopub.status.busy": "2024-08-20T10:13:10.222720Z",
     "iopub.status.idle": "2024-08-20T10:13:10.279040Z",
     "shell.execute_reply": "2024-08-20T10:13:10.278002Z",
     "shell.execute_reply.started": "2024-08-20T10:13:10.223502Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# 从 JSON 文件中读取数据\n",
    "input_file_path = 'data2text/data2text-test-all-v1.json'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "output_data = []\n",
    "output_data_test = []\n",
    "i = 0\n",
    "\n",
    "def retain_entity(value):\n",
    "    if value == \"\" or (\"未提及\" in value) or (\"未明确\" in value) or (\"未提供\" in value):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a82828-ef91-44f1-bc59-aed692d47e9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 将“装备”中的信息划分并加入“装备名称”和装备数量中\n",
    "# 遍历输入数据中的每个data项\n",
    "for excel in input_data:\n",
    "    for sheet in excel:\n",
    "        entities = []\n",
    "\n",
    "        # 遍历data中的每一行\n",
    "        for row, row_value in sheet['table']['data'].items():\n",
    "            for value in row_value:\n",
    "                header_index = row_value.index(value)\n",
    "                entity_type = sheet['table']['header'][header_index]\n",
    "                if entity_type == \"装备\":\n",
    "                    zb_name = []\n",
    "                    zb_number = []\n",
    "                    if '；' not in value:\n",
    "                        # 将装备直接加入装备名称\n",
    "                        if \"装备名称\" not in sheet['table']['header']:\n",
    "                            sheet['table']['header'].append(\"装备名称\")\n",
    "                        row_value.append(value)\n",
    "                    if '；' in value:\n",
    "                        if \"装备名称\" not in sheet['tabel']['header']:\n",
    "                            sheet['table']['header'].append(\"装备名称\")\n",
    "                        if \"装备数量\" not in sheet['tabel']['header']:\n",
    "                            sheet['table']['header'].append(\"装备数量\")\n",
    "                        # 将装备切分后加入装备名称和装备数量\n",
    "                        sheet['table']['header'].append(\"装备数量\")\n",
    "                        sub_values = value.split('；')\n",
    "                        for zhuangbei in sub_values:\n",
    "                            zb = zhaungbei.split('；')\n",
    "                            zb_name.append(zb[0])\n",
    "                            zb_number.append(zb[1])\n",
    "                            zb_number_set = list(set(zb_number))\n",
    "            row_value.append()\n",
    "\n",
    "\n",
    "                if entity_type not in [\"国家\", \"人物\", \"任务\", \"行动\", \"事件\"]:\n",
    "                    if value != \"\" and \"未提及\" not in value and \"未明确\" not in value and \"未提供\" not in value:\n",
    "                        entities.append({\n",
    "                            \"entity\": value,\n",
    "                            \"entity_type\": entity_type})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce0ac9-f5e9-466b-baa0-264be359faa0",
   "metadata": {},
   "source": [
    "### header出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85bea9f1-08da-46cd-8374-673603946586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T04:30:31.023597Z",
     "iopub.status.busy": "2024-08-06T04:30:31.022903Z",
     "iopub.status.idle": "2024-08-06T04:30:31.038325Z",
     "shell.execute_reply": "2024-08-06T04:30:31.037470Z",
     "shell.execute_reply.started": "2024-08-06T04:30:31.023557Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同的 entity_type 有： {'组织机构', '任务类型', '备注', '指挥员', '主体数量', '主体名称', '主体类型', '行动', '军种', '数量', '任务状态', '时间', '地点', '人物', '开始时间', '装备名称', '装备', '单位', '人数', '装备类型', '事件', '国家', '任务', '国别', '装备数量'}\n",
      "共有 25 种不同的 entity_type\n"
     ]
    }
   ],
   "source": [
    "entity_types = set([])\n",
    "for data in input_data:\n",
    "    entities = []\n",
    "    i += 1\n",
    "   \n",
    "    if isinstance(data, list):\n",
    "        for row in data:\n",
    "            if isinstance(row, dict):\n",
    "                for header in row['table']['header']:\n",
    "                    entity_types.add(header)\n",
    "                            \n",
    "\n",
    "    # # 遍历data中的每一行\n",
    "    # for row in data['table']['data']:\n",
    "    #     for item in row:\n",
    "    #         header_index = row.index(item)\n",
    "    #         entity_type = data['table']['header'][header_index]\n",
    "    #         entity_types.add(entity_type)\n",
    "\n",
    "# 打印集合内容\n",
    "print(\"不同的 entity_type 有：\", entity_types)\n",
    "\n",
    "# 打印总共有多少种不同的 entity_type\n",
    "print(\"共有\", len(entity_types), \"种不同的 entity_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b72d3a-cff4-4a81-96d8-4139a514d41f",
   "metadata": {},
   "source": [
    "### 生成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a76984f-efc5-4fe8-b33d-5102100620fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:32:32.536457Z",
     "iopub.status.busy": "2024-08-20T10:32:32.535522Z",
     "iopub.status.idle": "2024-08-20T10:32:32.650067Z",
     "shell.execute_reply": "2024-08-20T10:32:32.649119Z",
     "shell.execute_reply.started": "2024-08-20T10:32:32.536402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_val_set = []\n",
    "test_set = []\n",
    "all_set = []\n",
    "\n",
    "# 遍历输入数据中的每个data项\n",
    "for excel in input_data:\n",
    "    for sheet in excel:\n",
    "        entities = []\n",
    "\n",
    "        # 遍历data中的每一行\n",
    "        for row, row_value in sheet['table']['data'].items():\n",
    "            for value in row_value:\n",
    "                header_index = row_value.index(value)\n",
    "                entity_type = sheet['table']['header'][header_index]\n",
    "                if entity_type not in [\"国家\", \"人物\", \"任务\", \"行动\", \"事件\"]:\n",
    "                    if value != \"\" and \"未提及\" not in value and \"未明确\" not in value and \"未提供\" not in value:\n",
    "                        entities.append({\n",
    "                            \"entity\": value,\n",
    "                            \"entity_type\": entity_type})\n",
    "                        if entity_type in [\"装备\"]:\n",
    "                            if \"；\" in value:\n",
    "                                zb_name = []\n",
    "                                zb_number = []\n",
    "                                sub_values = value.split('；')\n",
    "                                for zhuangbei in sub_values:\n",
    "                                    zb = zhuangbei.split('，')\n",
    "                                    zb_name.append(zb[0])\n",
    "                                    zb_number.append(zb[1])\n",
    "                                    zb_number_set = list(set(zb_number))\n",
    "                                for zb_na in zb_name:\n",
    "                                    entities.append({\n",
    "                                        \"entity\": zb_na,\n",
    "                                        \"entity_type\": \"装备名称\"})\n",
    "                                for zb_nu in zb_number_set:\n",
    "                                    entities.append({\n",
    "                                        \"entity\": zb_nu,\n",
    "                                        \"entity_type\": \"装备数量\"})\n",
    "                            else:\n",
    "                                entities.append({\n",
    "                                    \"entity\": value,\n",
    "                                    \"entity_type\": \"装备名称\"})\n",
    "\n",
    "        # 创建输出格式\n",
    "        output = {\"text\": sheet[\"text\"], \"entity\": entities}\n",
    "        all_set.append(output)\n",
    "\n",
    "# 打乱数组顺序\n",
    "np.random.shuffle(all_set)\n",
    "\n",
    "# 计算分割点\n",
    "split_index = int(len(all_set) * 0.9)\n",
    "print(len(all_set))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_val_set = all_set[:split_index]\n",
    "test_set = all_set[split_index:]\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "output_file_path = os.path.join(current_path, 'data2text/data2text-ner-train-val.json')\n",
    "test_file_path = os.path.join(current_path, 'data2text/data2text-ner-test.json')\n",
    "\n",
    "# 打开输出路径\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file1:\n",
    "    json.dump(train_val_set, file1, ensure_ascii=False, indent=1)\n",
    "with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    json.dump(test_set, file2, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b95d2-340d-4eb3-a51e-a87e91e0361f",
   "metadata": {},
   "source": [
    "## 转化测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f095aabf-92eb-4496-9639-33e56267f4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:11:29.431425Z",
     "iopub.status.busy": "2024-08-09T16:11:29.430688Z",
     "iopub.status.idle": "2024-08-09T16:11:29.484859Z",
     "shell.execute_reply": "2024-08-09T16:11:29.484109Z",
     "shell.execute_reply.started": "2024-08-09T16:11:29.431378Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_val_set = []\n",
    "test_set = []\n",
    "all_set = []\n",
    "\n",
    "# 遍历输入数据中的每个data项\n",
    "for excel in input_data:\n",
    "    for sheet in excel:\n",
    "        entities = []\n",
    "\n",
    "        # 遍历data中的每一行\n",
    "        for row, row_value in sheet['table']['data'].items():\n",
    "            for value in row_value:\n",
    "                header_index = row_value.index(value)\n",
    "                entity_type = sheet['table']['header'][header_index]\n",
    "                if entity_type not in [\"任务\", \"行动\", \"事件\"]:\n",
    "                    if value != \"\" and \"未提及\" not in value and \"未明确\" not in value and \"未提供\" not in value:\n",
    "                        entities.append({\n",
    "                            \"entity\": value,\n",
    "                            \"entity_type\": entity_type})\n",
    "\n",
    "        # 创建输出格式\n",
    "        output = {\"text\": sheet[\"text\"], \"entity\": entities}\n",
    "        all_set.append(output)\n",
    "\n",
    "# 打乱数组顺序\n",
    "np.random.shuffle(all_set)\n",
    "\n",
    "# 计算分割点\n",
    "split_index = int(len(all_set) * 0.9)\n",
    "print(len(all_set))\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_val_set = all_set[:split_index]\n",
    "test_set = all_set[split_index:]\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "test_file_path = os.path.join(current_path, 'data2text/iepile-ner-test-v2.json')\n",
    "\n",
    "# 打开输出路径\n",
    "with open(test_file_path, 'w', encoding='utf-8') as file2:\n",
    "    json.dump(test_set, file2, ensure_ascii=False, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd784a-c922-4b49-b692-b6f8d4da08ad",
   "metadata": {},
   "source": [
    "## 转变为jsonl格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0bd245-6e83-456d-ab3a-2c0045433ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:37:11.216394Z",
     "iopub.status.busy": "2024-08-20T10:37:11.215524Z",
     "iopub.status.idle": "2024-08-20T10:37:11.289150Z",
     "shell.execute_reply": "2024-08-20T10:37:11.288270Z",
     "shell.execute_reply.started": "2024-08-20T10:37:11.216332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renamed from data2text/data2text-ner-train-val.jsonl to data2text/data2text-ner-train-val-json.json\n",
      "File renamed from data2text/data2text-ner-test.jsonl to data2text/data2text-ner-test-json.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "\n",
    "def rename_file(old_name, new_name):\n",
    "    # 确保旧文件存在\n",
    "    if os.path.exists(old_name):\n",
    "        # 重命名文件\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f\"File renamed from {old_name} to {new_name}\")\n",
    "    else:\n",
    "        print(f\"File {old_name} does not exist.\")\n",
    "\n",
    "\n",
    "train_json_file = \"data2text/data2text-ner-train-val.json\"\n",
    "train_jsonl_file = \"data2text/data2text-ner-train-val.jsonl\"\n",
    "test_json_file = \"data2text/data2text-ner-test.json\"\n",
    "test_jsonl_file = \"data2text/data2text-ner-test.jsonl\"\n",
    "\n",
    "with open(train_json_file, \"r\", encoding='utf-8') as file:\n",
    "    with jsonlines.open(train_jsonl_file, 'w') as writer:\n",
    "        input_data = json.load(file)\n",
    "        for data in input_data:\n",
    "            writer.write(data)\n",
    "\n",
    "with open(test_json_file, \"r\", encoding='utf-8') as file:\n",
    "    with jsonlines.open(test_jsonl_file, 'w') as writer:\n",
    "        input_data = json.load(file)\n",
    "        for data in input_data:\n",
    "            writer.write(data)\n",
    "\n",
    "rename_file(\"data2text/data2text-ner-train-val.jsonl\", \"data2text/data2text-ner-train-val-json.json\")\n",
    "rename_file(\"data2text/data2text-ner-test.jsonl\", \"data2text/data2text-ner-test-json.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf02155-a5dc-4c6c-9049-46186f32b1e6",
   "metadata": {},
   "source": [
    "## 转化测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0152768-ddaa-4292-a726-274c697a0d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T16:13:23.368475Z",
     "iopub.status.busy": "2024-08-09T16:13:23.368049Z",
     "iopub.status.idle": "2024-08-09T16:13:23.425363Z",
     "shell.execute_reply": "2024-08-09T16:13:23.424548Z",
     "shell.execute_reply.started": "2024-08-09T16:13:23.368454Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renamed from data2text/iepile-ner-test-v2.jsonl to data2text/iepile-ner-test-json-v2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "\n",
    "def rename_file(old_name, new_name):\n",
    "    # 确保旧文件存在\n",
    "    if os.path.exists(old_name):\n",
    "        # 重命名文件\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f\"File renamed from {old_name} to {new_name}\")\n",
    "    else:\n",
    "        print(f\"File {old_name} does not exist.\")\n",
    "\n",
    "\n",
    "test_json_file = \"data2text/iepile-ner-test-v2.json\"\n",
    "test_jsonl_file = \"data2text/iepile-ner-test-v2.jsonl\"\n",
    "\n",
    "\n",
    "with open(test_json_file, \"r\", encoding='utf-8') as file:\n",
    "    with jsonlines.open(test_jsonl_file, 'w') as writer:\n",
    "        input_data = json.load(file)\n",
    "        for data in input_data:\n",
    "            writer.write(data)\n",
    "\n",
    "rename_file(\"data2text/iepile-ner-test-v2.jsonl\", \"data2text/iepile-ner-test-json-v2.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a7689-2849-4be7-980a-dcc51554f857",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 生成测试集（按照单个schema）\n",
    "1. 按照单个schema生成测试集，目前其他测试方法优先级更高，此处代码保留但不作运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07463eab-999b-4dfb-b084-69e47262b065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:06:30.274110Z",
     "iopub.status.busy": "2024-08-02T08:06:30.273342Z",
     "iopub.status.idle": "2024-08-02T08:06:30.355517Z",
     "shell.execute_reply": "2024-08-02T08:06:30.354641Z",
     "shell.execute_reply.started": "2024-08-02T08:06:30.274070Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_file_path_test = 'NERsample_test.json'\n",
    "\n",
    "with open(input_file_path_test, 'r', encoding='utf-8') as file:\n",
    "    input_data_test = json.load(file)\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "output_file_path = os.path.join(current_path, 'schema_NERsample_test.json')\n",
    "\n",
    "output_data_test = []\n",
    "# 遍历输入数据中的每个data项\n",
    "for entity_type in [\"时间\", \"人物\", \"国家\", \"装备名称\", \"装备类型\", \"装备数量\", \"组织机构\", \"地点\", \"任务\", \"行动\", \"事件\"]:\n",
    "    output_file_path = os.path.join(current_path, entity_type + 'NERsample_test.json')\n",
    "    for data in input_data_test:\n",
    "        entities = []\n",
    "        for item in data['entity']:\n",
    "            if item[\"entity_type\"] == entity_type:\n",
    "                entities.append(item)\n",
    "                output = {\"text\": data[\"text\"], \"entity\": entities}\n",
    "                output_data_test.append(output)\n",
    "    # 打开输出路径\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output_data_test, file, ensure_ascii=False,indent=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac02515-d67e-4429-ac93-34cebbd024b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T08:32:31.099713Z",
     "iopub.status.busy": "2024-08-02T08:32:31.098863Z",
     "iopub.status.idle": "2024-08-02T08:32:31.113046Z",
     "shell.execute_reply": "2024-08-02T08:32:31.112169Z",
     "shell.execute_reply.started": "2024-08-02T08:32:31.099653Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_file_path_test = 'NERsample_test.json'\n",
    "\n",
    "with open(input_file_path_test, 'r', encoding='utf-8') as file:\n",
    "    input_data_test = json.load(file)\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "\n",
    "output_data_test = []\n",
    "# 遍历输入数据中的每个data项\n",
    "\n",
    "output_file_path = os.path.join(current_path,  '装备数量NERsample_test.json')\n",
    "for data in input_data_test:\n",
    "    for item in data['entity']:\n",
    "        if item[\"entity_type\"] == \"装备数量\":\n",
    "            entities = []\n",
    "            entities.append(item)\n",
    "            output = {\"text\": data[\"text\"], \"entity\": entities}\n",
    "            output_data_test.append(output)\n",
    "# 打开输出路径\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(output_data_test, file, ensure_ascii=False,indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b6ed53-fce6-4287-a47e-072fe8036bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T06:36:08.019618Z",
     "iopub.status.busy": "2024-08-02T06:36:08.018733Z",
     "iopub.status.idle": "2024-08-02T06:36:08.029005Z",
     "shell.execute_reply": "2024-08-02T06:36:08.028498Z",
     "shell.execute_reply.started": "2024-08-02T06:36:08.019556Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, 23)]\n"
     ]
    }
   ],
   "source": [
    "def find_all_spans(source_text, substring):\n",
    "    length = len(substring)\n",
    "    spans = []\n",
    "    for i in range(len(source_text) - length + 1):\n",
    "        if source_text[i:i+length] == substring:\n",
    "            spans.append((i, i + length))\n",
    "    return spans\n",
    "\n",
    "\n",
    "source_text = \"这是一个测试文本，这包含一些用于测试的子字符串。\"\n",
    "substring = \"字符串\"\n",
    "\n",
    "spans = find_all_spans(source_text, substring)\n",
    "print(spans)\n",
    "# span = find_span(source_text, substring)\n",
    "# print(span)  # 输出子字符串的位置span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98428d-231c-4782-aa8b-dd69bc202d18",
   "metadata": {},
   "source": [
    "### 将训练数据转换为模型微调使用的数据格式\n",
    "运行格式转换脚本进行格式转换，其中指令含义如下：\n",
    "- --src_path data2text/iepile-ner-train-val-json.json\n",
    "   - 训练数据的路径\n",
    "- --tgt_path data2text/iepile-ner-train-val-transformed.json\n",
    "   - 数据转换之后文件输出的路径\n",
    "- --language zh\n",
    "   - 选择zh语言\n",
    "- --task NER\n",
    "   - 选择实体抽取任务\n",
    "- --split_num 4\n",
    "   - 定义单个指令中可包含的最大schema数目\n",
    "- --random_sort\n",
    "   - 是否对指令中的schema随机排序，默认为false\n",
    "- --split train\n",
    "   - 指定数据集类型为train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db1583d-ee4a-487c-bedc-d687a04280de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:37:57.216833Z",
     "iopub.status.busy": "2024-08-20T10:37:57.215980Z",
     "iopub.status.idle": "2024-08-20T10:37:57.902733Z",
     "shell.execute_reply": "2024-08-20T10:37:57.900778Z",
     "shell.execute_reply.started": "2024-08-20T10:37:57.216772Z"
    }
   },
   "outputs": [],
   "source": [
    "!python ie2instruction/convert_func.py \\\n",
    "    --src_path data2text/data2text-ner-train-val-json.json \\\n",
    "    --tgt_path data2text/data2text-ner-train-val-transformed.json \\\n",
    "    --schema_path data2text/schema.json \\\n",
    "    --language zh \\\n",
    "    --task NER \\\n",
    "    --split_num 4 \\\n",
    "    --random_sort \\\n",
    "    --split train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fe99e-8a03-42b5-9177-269ffb2f014c",
   "metadata": {},
   "source": [
    "### 拆分训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aeac3ec-7259-49eb-8e39-35912461af01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:41:11.247345Z",
     "iopub.status.busy": "2024-08-20T10:41:11.246410Z",
     "iopub.status.idle": "2024-08-20T10:41:11.481371Z",
     "shell.execute_reply": "2024-08-20T10:41:11.480522Z",
     "shell.execute_reply.started": "2024-08-20T10:41:11.247312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和验证集已生成：\n",
      "训练集大小：7732\n",
      "验证集大小：1933\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# 读取JSONL文件\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    random.shuffle(data)  # 随机打乱数据\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "# 写入JSONL文件\n",
    "def write_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for entry in data:\n",
    "            file.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(input_file, train_file, val_file):\n",
    "    data = read_jsonl(input_file)\n",
    "    train_data, val_data = split_data(data)\n",
    "    write_jsonl(train_data, train_file)\n",
    "    write_jsonl(val_data, val_file)\n",
    "    print(f\"训练集和验证集已生成：\\n训练集大小：{len(train_data)}\\n验证集大小：{len(val_data)}\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "input_file = 'data2text/data2text-ner-train-val-transformed.json'  # 输入的JSONL文件\n",
    "train_file = 'data2text/data2text-ner-train-transformed.json'  # 训练集文件\n",
    "val_file = 'data2text/data2text-ner-val-transformed.json'      # 验证集文件\n",
    "\n",
    "main(input_file, train_file, val_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37506ec2-e4bc-4d31-944b-7b7530d4bba0",
   "metadata": {},
   "source": [
    "### 将测试数据转换为测试使用的数据格式\n",
    "运行格式转换脚本进行格式转换，其中指令含义如下：\n",
    "- --src_path data2text/iepile-ner-test-json.json\n",
    "   - 训练数据的路径\n",
    "- --tgt_path data2text/iepile-ner-test-transformed.json\n",
    "   - 数据转换之后文件输出的路径\n",
    "- --language zh\n",
    "   - 选择zh语言\n",
    "- --task NER\n",
    "   - 选择实体抽取任务\n",
    "- --split_num 4\n",
    "   - 定义单个指令中可包含的最大schema数目\n",
    "- --random_sort\n",
    "   - 是否对指令中的schema随机排序，默认为false\n",
    "- --split test\n",
    "   - 指定数据集类型为train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2541ae1a-61ef-4a5a-8fab-f42cb65d88e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T10:41:39.253572Z",
     "iopub.status.busy": "2024-08-20T10:41:39.252810Z",
     "iopub.status.idle": "2024-08-20T10:41:40.118749Z",
     "shell.execute_reply": "2024-08-20T10:41:40.117831Z",
     "shell.execute_reply.started": "2024-08-20T10:41:39.253531Z"
    }
   },
   "outputs": [],
   "source": [
    "!python ie2instruction/convert_func.py \\\n",
    "    --src_path data2text/data2text-ner-test-json.json \\\n",
    "    --tgt_path data2text/data2text-ner-test-transformed.json \\\n",
    "    --schema_path data2text/schema_test.json \\\n",
    "    --language zh \\\n",
    "    --task NER \\\n",
    "    --split_num 4 \\\n",
    "    --split test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a9b3d-4aaa-458e-838c-b3cdf94a531b",
   "metadata": {},
   "source": [
    "## 将测试输出每6条合并\n",
    "测试集的输出因为split_num = 4，故每条文本被拆分为6条数据输出，我们需要将其合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a557c2ab-f8b7-4d63-a890-db72335ebd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-09T12:50:34.921147Z",
     "iopub.status.busy": "2024-08-09T12:50:34.920492Z",
     "iopub.status.idle": "2024-08-09T12:50:34.961238Z",
     "shell.execute_reply": "2024-08-09T12:50:34.960345Z",
     "shell.execute_reply.started": "2024-08-09T12:50:34.921108Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def merge_dicts(dict_list):\n",
    "    \"\"\"合并多个字典，将相同键的值合并为一个列表\"\"\"\n",
    "    merged_dict = {}\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            if key in merged_dict:\n",
    "                merged_dict[key].extend(value)\n",
    "            else:\n",
    "                merged_dict[key] = value\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def process_jsonl_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    result = []\n",
    "    temp_dict_list = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        data = json.loads(line)\n",
    "        output_data = json.loads(data['output'])\n",
    "        temp_dict_list.append(output_data)\n",
    "\n",
    "        # 每6条数据合并一次\n",
    "        if (i + 1) % 6 == 0 or i == len(lines) - 1:\n",
    "            merged_output = merge_dicts(temp_dict_list)\n",
    "            result.append({\"result\": merged_output})\n",
    "            temp_dict_list = []\n",
    "\n",
    "    # 将结果写入JSON文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "input_file = 'results/baichuan7B-ft-test-output.json'\n",
    "output_file = 'results/baichuan7B-ft-test-output-merged.json'\n",
    "\n",
    "process_jsonl_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e0cc8e-9372-4d06-9b29-68e4bb944b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T10:01:26.249659Z",
     "iopub.status.busy": "2024-08-24T10:01:26.248840Z",
     "iopub.status.idle": "2024-08-24T10:01:26.257051Z",
     "shell.execute_reply": "2024-08-24T10:01:26.255956Z",
     "shell.execute_reply.started": "2024-08-24T10:01:26.249616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5\n"
     ]
    }
   ],
   "source": [
    "with open(\"config_train.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "base_model = config.get(\"base_model\", \"cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5\")\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8583351b-d5d8-498f-a0c8-986323fd3b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T10:02:57.263032Z",
     "iopub.status.busy": "2024-08-24T10:02:57.262089Z",
     "iopub.status.idle": "2024-08-24T10:02:57.271750Z",
     "shell.execute_reply": "2024-08-24T10:02:57.270830Z",
     "shell.execute_reply.started": "2024-08-24T10:02:57.262966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2text/data2text-ner-train-val.json\n",
      "data2text/data2text-ner-test.json\n",
      "data2text/data2text-ner-train-val.jsonl\n",
      "data2text/data2text-ner-test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_path = \"data2text\"\n",
    "output_file_path = os.path.join(current_path, 'data2text-ner-train-val.json')\n",
    "test_file_path = os.path.join(current_path, 'data2text-ner-test.json')\n",
    "\n",
    "train_json_file = output_file_path\n",
    "train_jsonl_file = os.path.join(current_path, 'data2text-ner-train-val.jsonl')\n",
    "test_json_file = test_file_path\n",
    "test_jsonl_file = os.path.join(current_path, 'data2text-ner-test.jsonl')\n",
    "print(output_file_path)\n",
    "print(test_file_path)\n",
    "print(train_jsonl_file)\n",
    "print(test_jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261cbb2-f6c0-4fce-9bbf-86261b2ff50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
