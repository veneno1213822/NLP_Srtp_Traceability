{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592aabd6-1af6-49e2-b06b-e2740cf7695d",
   "metadata": {},
   "source": [
    "# 模型量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d030b1-e465-4a4d-b72e-4bbaa302f0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T13:59:20.906504Z",
     "iopub.status.busy": "2024-08-29T13:59:20.906007Z",
     "iopub.status.idle": "2024-08-29T13:59:26.765770Z",
     "shell.execute_reply": "2024-08-29T13:59:26.765206Z",
     "shell.execute_reply.started": "2024-08-29T13:59:20.906477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from oneke_wrapper_quantization import OneKEWrapper\n",
    "\n",
    "model_path = \"./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6\"\n",
    "lora_path = \"./lora/llama3-8B-iepile-data2text-continue\"\n",
    "\n",
    "# device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12798823-5cb4-4d8d-b68d-a9677a823c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T13:59:26.766841Z",
     "iopub.status.busy": "2024-08-29T13:59:26.766550Z",
     "iopub.status.idle": "2024-08-29T13:59:26.769833Z",
     "shell.execute_reply": "2024-08-29T13:59:26.769377Z",
     "shell.execute_reply.started": "2024-08-29T13:59:26.766821Z"
    }
   },
   "outputs": [],
   "source": [
    "# oneke_wrapper = OneKEWrapper(model_path, lora_path, device,False,\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a043d6-fb87-475d-8123-3a6280965a22",
   "metadata": {},
   "source": [
    "### 普通推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c352a3-c14b-4522-8271-fb529c98a788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T08:12:10.462362Z",
     "iopub.status.busy": "2024-08-30T08:12:10.461461Z",
     "iopub.status.idle": "2024-08-30T08:12:10.509495Z",
     "shell.execute_reply": "2024-08-30T08:12:10.508713Z",
     "shell.execute_reply.started": "2024-08-30T08:12:10.462302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table': {'header': ['时间',\n",
       "    '人物',\n",
       "    '国家',\n",
       "    '装备名称',\n",
       "    '装备类型',\n",
       "    '装备数量',\n",
       "    '组织机构',\n",
       "    '地点',\n",
       "    '任务',\n",
       "    '行动',\n",
       "    '事件'],\n",
       "   'data': {'2': ['去年五月',\n",
       "     '军方检察官',\n",
       "     '台湾',\n",
       "     '笔记本电脑',\n",
       "     '导弹艇',\n",
       "     '一台',\n",
       "     '海军',\n",
       "     '未明确',\n",
       "     '未明确',\n",
       "     '展开调查',\n",
       "     '笔记本电脑丢失']}},\n",
       "  'origin': {},\n",
       "  'text': '一些消息人士透露，这两张地图中包含有关台湾海军舰艇布署的情况。如果发生战争，丢失这两张地图将使台湾海军在面对敌方时失去战斗力。此外，还有一台导弹艇上的笔记本电脑在去年五月丢失，军方检察官认真对此展开调查。'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据 \n",
    "data = json.load(open(\"./internal_data_type/data2text_test_all-v1.json\"))\n",
    "data[0]\n",
    "#选取第一条进行推理\n",
    "# oneke_wrapper.inference(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb02a44-32ce-454e-ae92-39d42fd5127f",
   "metadata": {},
   "source": [
    "### vllm推理——只支持量化llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae01ec7-fa10-4214-b98c-161beb314aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T13:59:26.831240Z",
     "iopub.status.busy": "2024-08-29T13:59:26.830107Z",
     "iopub.status.idle": "2024-08-29T13:59:35.677420Z",
     "shell.execute_reply": "2024-08-29T13:59:35.676895Z",
     "shell.execute_reply.started": "2024-08-29T13:59:26.831176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OneKEWrapper...\n",
      "./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6\n",
      "bitsandbytes\n",
      "./lora/llama3-8B-iepile-data2text-continue\n",
      "WARNING 08-29 13:59:27 config.py:254] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 08-29 13:59:27 config.py:1342] bitsandbytes quantization is not tested with LoRA yet.\n",
      "INFO 08-29 13:59:27 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6', speculative_config=None, tokenizer='./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-29 13:59:27 model_runner.py:720] Starting to load model ./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6...\n",
      "INFO 08-29 13:59:27 loader.py:871] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d0ba9ba7ca45daa8ee5f161f16c2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-29 13:59:31 model_runner.py:732] Loading model weights took 5.3168 GB\n",
      "INFO 08-29 13:59:34 gpu_executor.py:102] # GPU blocks: 7070, # CPU blocks: 2048\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = device[-1]\n",
    "oneke_wrapper_vllm = OneKEWrapper(model_path, lora_path, device, True, \"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51c1f5-c5fd-4764-b27b-3db6298e9ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T13:59:35.678461Z",
     "iopub.status.busy": "2024-08-29T13:59:35.678215Z",
     "iopub.status.idle": "2024-08-29T13:59:36.001791Z",
     "shell.execute_reply": "2024-08-29T13:59:36.000428Z",
     "shell.execute_reply.started": "2024-08-29T13:59:35.678443Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moneke_wrapper_vllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data2text-general/oneke_wrapper_quantization.py:240\u001b[0m, in \u001b[0;36mOneKEWrapper.inference_vllm\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# test_prompts = create_test_prompts(lora_path)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m prompts \u001b[38;5;241m=\u001b[39m (input_ids\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    235\u001b[0m          SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# logprobs=1,\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# prompt_logprobs=1,\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m),\n\u001b[1;32m    239\u001b[0m      LoRARequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_path))\n\u001b[0;32m--> 240\u001b[0m output_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# 记录推理结束时间\u001b[39;00m\n\u001b[1;32m    243\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/data2text-general/oneke_wrapper_quantization.py:72\u001b[0m, in \u001b[0;36mOneKEWrapper.process_requests\u001b[0;34m(self, engine, test_prompts)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_prompts:\n\u001b[1;32m     71\u001b[0m     prompt, sampling_params, lora_request \u001b[38;5;241m=\u001b[39m test_prompts\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     request_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     78\u001b[0m     request_outputs \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py:654\u001b[0m, in \u001b[0;36mLLMEngine.add_request\u001b[0;34m(self, request_id, inputs, params, arrival_time, lora_request, trace_headers, prompt_adapter_request)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arrival_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     arrival_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 654\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_model_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_processed_request(\n\u001b[1;32m    661\u001b[0m     request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    662\u001b[0m     processed_inputs\u001b[38;5;241m=\u001b[39mprocessed_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m     trace_headers\u001b[38;5;241m=\u001b[39mtrace_headers,\n\u001b[1;32m    668\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py:580\u001b[0m, in \u001b[0;36mLLMEngine.process_model_inputs\u001b[0;34m(self, request_id, inputs, lora_request, prompt_adapter_request)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_token_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m    576\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tokenizer_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts must be None if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m                                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_tokenizer_init is True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m     prompt_token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[0;32m--> 580\u001b[0m                                         prompt\u001b[38;5;241m=\u001b[39m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    581\u001b[0m                                         lora_request\u001b[38;5;241m=\u001b[39mlora_request)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    583\u001b[0m     prompt_token_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_token_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "oneke_wrapper_vllm.inference_vllm(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bd942-9229-4542-99f7-7e0b1ee941b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
