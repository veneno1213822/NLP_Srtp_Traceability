{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee394c8-83b7-41d0-892f-50697cdfed0c",
   "metadata": {},
   "source": [
    "# 将oneke_wrapper中常规推理更换为vllm推理并进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4694871-3759-44d9-93c4-b188b293587b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:53:32.381802Z",
     "iopub.status.busy": "2024-08-31T13:53:32.381474Z",
     "iopub.status.idle": "2024-08-31T13:53:38.725022Z",
     "shell.execute_reply": "2024-08-31T13:53:38.724351Z",
     "shell.execute_reply.started": "2024-08-31T13:53:32.381780Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from oneke_wrapper import OneKEWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bfd5f-37dc-48e4-8b4c-1351d9b244b1",
   "metadata": {},
   "source": [
    "安装了——ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd5b64-9d0d-48ea-9d44-978211763a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T13:46:44.175805Z",
     "iopub.status.busy": "2024-08-24T13:46:44.174936Z",
     "iopub.status.idle": "2024-08-24T13:46:44.181969Z",
     "shell.execute_reply": "2024-08-24T13:46:44.180622Z",
     "shell.execute_reply.started": "2024-08-24T13:46:44.175742Z"
    }
   },
   "source": [
    "## 定义模型路径以及卡号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16a5d9f-7908-456b-8673-0c0ef44ce992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:53:38.726219Z",
     "iopub.status.busy": "2024-08-31T13:53:38.725801Z",
     "iopub.status.idle": "2024-08-31T13:53:38.729474Z",
     "shell.execute_reply": "2024-08-31T13:53:38.728931Z",
     "shell.execute_reply.started": "2024-08-31T13:53:38.726200Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6\"\n",
    "lora_path = \"./lora/llama3-8B-iepile-data2text-continue\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94378f4-d95f-40a1-8c6c-25ed52bc19e1",
   "metadata": {},
   "source": [
    "## 初始化类对象,加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc667b2a-665a-4a3f-b136-21e81c8b271d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:53:38.730491Z",
     "iopub.status.busy": "2024-08-31T13:53:38.730206Z",
     "iopub.status.idle": "2024-08-31T13:54:21.145808Z",
     "shell.execute_reply": "2024-08-31T13:54:21.144936Z",
     "shell.execute_reply.started": "2024-08-31T13:53:38.730472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OneKEWrapper...\n",
      "cuda:0\n",
      "INFO 08-31 13:53:39 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6', speculative_config=None, tokenizer='./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-31 13:53:39 model_runner.py:720] Starting to load model ./cache/models--meta-llama--Meta-Llama-3-8B/snapshots/62bd457b6fe961a42a631306577e622c83876cb6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578b39e1c4664ba09a07b42619535faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:53:59 model_runner.py:732] Loading model weights took 14.9634 GB\n",
      "INFO 08-31 13:54:02 gpu_executor.py:102] # GPU blocks: 1021, # CPU blocks: 2048\n",
      "INFO 08-31 13:54:04 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-31 13:54:04 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-31 13:54:21 model_runner.py:1225] Graph capturing finished in 16 secs.\n"
     ]
    }
   ],
   "source": [
    "oneke_wrapper = OneKEWrapper(model_path, lora_path, device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622e06d-da33-49c6-9361-b1950d5fd3ce",
   "metadata": {},
   "source": [
    "## vllm推理单条测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8aff0a-d4e9-4684-b93d-b75afd17cc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:54:21.148345Z",
     "iopub.status.busy": "2024-08-31T13:54:21.147961Z",
     "iopub.status.idle": "2024-08-31T13:54:25.585999Z",
     "shell.execute_reply": "2024-08-31T13:54:25.585085Z",
     "shell.execute_reply.started": "2024-08-31T13:54:21.148322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-31 13:54:21 tokenizer.py:144] No tokenizer found in ./lora/llama3-8B-iepile-data2text-continue, using base model tokenizer instead. (Exception: ./lora/llama3-8B-iepile-data2text-continue does not appear to have a file named config.json. Checkout 'https://huggingface.co/./lora/llama3-8B-iepile-data2text-continue/tree/None' for available files.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensors = torch.load(lora_bin_file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'table': {'header': ['时间',\n",
       "    '人物',\n",
       "    '国家',\n",
       "    '装备名称',\n",
       "    '装备类型',\n",
       "    '装备数量',\n",
       "    '组织机构',\n",
       "    '地点',\n",
       "    '任务',\n",
       "    '行动',\n",
       "    '事件'],\n",
       "   'data': {'2': ['去年五月',\n",
       "     '军方检察官',\n",
       "     '台湾',\n",
       "     '笔记本电脑',\n",
       "     '导弹艇',\n",
       "     '一台',\n",
       "     '海军',\n",
       "     '未明确',\n",
       "     '未明确',\n",
       "     '展开调查',\n",
       "     '笔记本电脑丢失']}},\n",
       "  'origin': {},\n",
       "  'text': '一些消息人士透露，这两张地图中包含有关台湾海军舰艇布署的情况。如果发生战争，丢失这两张地图将使台湾海军在面对敌方时失去战斗力。此外，还有一台导弹艇上的笔记本电脑在去年五月丢失，军方检察官认真对此展开调查。',\n",
       "  'verify': {'2': [{'v': '去年五月', 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': '笔记本电脑', 'span': '', 'cf': ''},\n",
       "    {'v': '导弹艇', 'span': '', 'cf': ''},\n",
       "    {'v': '一台', 'span': '', 'cf': ''},\n",
       "    {'v': '海军', 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': '展开调查', 'span': '', 'cf': ''},\n",
       "    {'v': '笔记本电脑丢失', 'span': '', 'cf': ''}]},\n",
       "  'add_refs': []}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "data = json.load(open(\"./internal_data_type/data2text_test_all-v1.json\"))\n",
    "\n",
    "#选取第一条进行推理\n",
    "oneke_wrapper.inference_vllm(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225caee1-18a0-4294-88d6-87c2af81371c",
   "metadata": {},
   "source": [
    "## vllm推理全集测试，记录平均时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb26d13-0aec-44f3-a0cb-aaf53c5ab3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:54:25.587314Z",
     "iopub.status.busy": "2024-08-31T13:54:25.586973Z",
     "iopub.status.idle": "2024-08-31T15:04:13.242347Z",
     "shell.execute_reply": "2024-08-31T15:04:13.241237Z",
     "shell.execute_reply.started": "2024-08-31T13:54:25.587296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 300/2138 [09:34<1:22:39,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 193 (char 192)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['时间', '人物', '国家', '装备名称', '装备类型', '装备数量', '组织机构', '地点', '任务', '行动', '事件'], 'input': '一项合作研发的先进无人机系统——雀鹰系统，由以色列国防研究与发展局、以色列XTEND公司和美国国防部反恐技术支持办公室联合开发。该系统采用远程沉浸式界面，可有效保护特种部队免受各种入境空中威胁，特别是无人机的威胁。美军计划启动一项保护特种部队的计划，采用雀鹰无人机系统。'}, output:{\"时间\": [], \"人物\": [], \"国家\": [], \"装备名称\": [\"雀鹰系统\", \"雀鹰无人机系统\"], \"装备类型\": [\"无人机系统\"], \"装备数量\": [], \"组织机构\": [\"['以色列国防研究与发展局\", \"['以色列XTEND公司\", \"['美国国防部反恐技术支持办公室\"], \"地点\": [], \"任务\": [], \"行动\": [], \"事件\": []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2134/2138 [1:09:41<00:06,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['国别', '军种', '单位', '装备', '装备数量', '指挥员', '地点', '行动', '开始时间', '任务状态', '任务类型', '备注'], 'input': '7月13日，陆军第54集团军第127师在华中某训练基地进行夜间实弹射击演习，本次演习中，部队使用07式自行榴弹炮（代号PLZ-07）12辆，发射155毫米炮弹36发，成功摧毁夜间模拟敌方目标。7月14日，海军第一航空母舰支队的辽宁舰（航空母舰，舷号16）和第三驱逐舰支队的西安舰（052C驱逐舰，舷号153）共1280人，在黄海进行舰载机起降训练。此次训练中，歼-15舰载机12架次完成了起降任务。'}, output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2138/2138 [1:09:47<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['国别', '军种', '装备数量', '装备', '地点', '行动', '开始时间', '任务状态'], 'input': '7月21日，空军第十战斗机师的苏-30战斗机（代号Su-30MKK）8架，在东海上空进行空中拦截任务。此次任务中，苏-30战斗机成功拦截并驱离进入防空识别区的2架不明飞行器，确保了空域安全。7月22日，陆军第27集团军第79师在华东某训练基地进行联合实弹射击演习，本次演习中，部队使用PLZ-45自行榴弹炮（代号PLZ-45）18辆，发射155毫米炮弹54发，成功摧毁模拟敌方目标，演习效果显著。'}, output:\n",
      "1.9551926702191176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "oneke_wrapper.data_count = 0\n",
    "oneke_wrapper.total_time = 0\n",
    "results = []\n",
    "for d in tqdm(data):\n",
    "    results.append(oneke_wrapper.inference_vllm(d))\n",
    "print(oneke_wrapper.total_time/oneke_wrapper.data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525d7524-b9fd-4337-93e2-869433fe5e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T15:04:13.244326Z",
     "iopub.status.busy": "2024-08-31T15:04:13.243467Z",
     "iopub.status.idle": "2024-08-31T15:04:13.247983Z",
     "shell.execute_reply": "2024-08-31T15:04:13.247533Z",
     "shell.execute_reply.started": "2024-08-31T15:04:13.244305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 1.9551926702191176\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average inference time: {oneke_wrapper.total_time/oneke_wrapper.data_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1197303d-496b-40ed-ad83-a2ce4921b770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T15:04:13.249299Z",
     "iopub.status.busy": "2024-08-31T15:04:13.248896Z",
     "iopub.status.idle": "2024-08-31T15:04:13.437693Z",
     "shell.execute_reply": "2024-08-31T15:04:13.436866Z",
     "shell.execute_reply.started": "2024-08-31T15:04:13.249272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 17785.27 MB\n",
      "Memory occupied after inference: 73.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "memory_used = torch.cuda.memory_allocated() - oneke_wrapper.gpu_mem_before_load\n",
    "memory_occupied = torch.cuda.memory_allocated() / oneke_wrapper.gpu_mem_max * 100\n",
    "print(f\"Memory used for inference: {memory_used / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Memory occupied after inference: {memory_occupied:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630053fc-cf7a-45c7-9d7e-9123c01d0900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T15:04:13.439187Z",
     "iopub.status.busy": "2024-08-31T15:04:13.438763Z",
     "iopub.status.idle": "2024-08-31T15:04:14.388690Z",
     "shell.execute_reply": "2024-08-31T15:04:14.387531Z",
     "shell.execute_reply.started": "2024-08-31T15:04:13.439158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "json.dump(results, open(\"./results/llama_vllm.json\",'w+'),ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
