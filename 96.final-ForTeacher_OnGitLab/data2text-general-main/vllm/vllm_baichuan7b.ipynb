{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee394c8-83b7-41d0-892f-50697cdfed0c",
   "metadata": {},
   "source": [
    "# 将oneke_wrapper中常规推理更换为vllm推理并进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4694871-3759-44d9-93c4-b188b293587b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T12:32:48.317587Z",
     "iopub.status.busy": "2024-08-31T12:32:48.317119Z",
     "iopub.status.idle": "2024-08-31T12:32:54.124214Z",
     "shell.execute_reply": "2024-08-31T12:32:54.123264Z",
     "shell.execute_reply.started": "2024-08-31T12:32:48.317546Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from oneke_wrapper import OneKEWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bfd5f-37dc-48e4-8b4c-1351d9b244b1",
   "metadata": {},
   "source": [
    "安装了——ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd5b64-9d0d-48ea-9d44-978211763a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-24T13:46:44.175805Z",
     "iopub.status.busy": "2024-08-24T13:46:44.174936Z",
     "iopub.status.idle": "2024-08-24T13:46:44.181969Z",
     "shell.execute_reply": "2024-08-24T13:46:44.180622Z",
     "shell.execute_reply.started": "2024-08-24T13:46:44.175742Z"
    }
   },
   "source": [
    "## 定义模型路径以及卡号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16a5d9f-7908-456b-8673-0c0ef44ce992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T12:32:54.125449Z",
     "iopub.status.busy": "2024-08-31T12:32:54.125167Z",
     "iopub.status.idle": "2024-08-31T12:32:54.129611Z",
     "shell.execute_reply": "2024-08-31T12:32:54.128817Z",
     "shell.execute_reply.started": "2024-08-31T12:32:54.125430Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"./cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5\"\n",
    "lora_path = \"./lora/baichuan7B-data2text-continue\"\n",
    " \n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94378f4-d95f-40a1-8c6c-25ed52bc19e1",
   "metadata": {},
   "source": [
    "## 初始化类对象,加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc667b2a-665a-4a3f-b136-21e81c8b271d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T12:32:54.130544Z",
     "iopub.status.busy": "2024-08-31T12:32:54.130340Z",
     "iopub.status.idle": "2024-08-31T12:33:24.453708Z",
     "shell.execute_reply": "2024-08-31T12:33:24.452553Z",
     "shell.execute_reply.started": "2024-08-31T12:32:54.130523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OneKEWrapper...\n",
      "cuda:0\n",
      "INFO 08-31 12:32:54 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='./cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5', speculative_config=None, tokenizer='./cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=./cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-31 12:32:54 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 08-31 12:32:55 model_runner.py:720] Starting to load model ./cache/models--baichuan-inc--Baichuan2-7B-Chat/snapshots/ea66ced17780ca3db39bc9f8aa601d8463db3da5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ee934aff2241e1b0050f2693296a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py:405: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 12:33:05 model_runner.py:732] Loading model weights took 13.9819 GB\n",
      "INFO 08-31 12:33:07 gpu_executor.py:102] # GPU blocks: 441, # CPU blocks: 512\n",
      "INFO 08-31 12:33:08 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-31 12:33:08 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-31 12:33:24 model_runner.py:1225] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "oneke_wrapper = OneKEWrapper(model_path, lora_path, device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e622e06d-da33-49c6-9361-b1950d5fd3ce",
   "metadata": {},
   "source": [
    "## vllm推理单条测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8aff0a-d4e9-4684-b93d-b75afd17cc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T12:33:24.456138Z",
     "iopub.status.busy": "2024-08-31T12:33:24.455650Z",
     "iopub.status.idle": "2024-08-31T12:33:27.042680Z",
     "shell.execute_reply": "2024-08-31T12:33:27.041787Z",
     "shell.execute_reply.started": "2024-08-31T12:33:24.456116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-31 12:33:24 tokenizer.py:144] No tokenizer found in ./lora/baichuan7B-data2text-continue, using base model tokenizer instead. (Exception: ./lora/baichuan7B-data2text-continue does not appear to have a file named config.json. Checkout 'https://huggingface.co/./lora/baichuan7B-data2text-continue/tree/None' for available files.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/vllm/lora/models.py:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensors = torch.load(lora_bin_file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'table': {'header': ['时间',\n",
       "    '人物',\n",
       "    '国家',\n",
       "    '装备名称',\n",
       "    '装备类型',\n",
       "    '装备数量',\n",
       "    '组织机构',\n",
       "    '地点',\n",
       "    '任务',\n",
       "    '行动',\n",
       "    '事件'],\n",
       "   'data': {'2': ['去年五月',\n",
       "     '军方检察官',\n",
       "     '台湾',\n",
       "     '笔记本电脑',\n",
       "     '导弹艇',\n",
       "     '一台',\n",
       "     '海军',\n",
       "     '未明确',\n",
       "     '未明确',\n",
       "     '展开调查',\n",
       "     '笔记本电脑丢失']}},\n",
       "  'origin': {},\n",
       "  'text': '一些消息人士透露，这两张地图中包含有关台湾海军舰艇布署的情况。如果发生战争，丢失这两张地图将使台湾海军在面对敌方时失去战斗力。此外，还有一台导弹艇上的笔记本电脑在去年五月丢失，军方检察官认真对此展开调查。',\n",
       "  'verify': {'2': [{'v': '去年五月', 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': '台湾', 'span': '', 'cf': ''},\n",
       "    {'v': '笔记本电脑', 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': '一台', 'span': '', 'cf': ''},\n",
       "    {'v': '海军', 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': None, 'span': '', 'cf': ''},\n",
       "    {'v': '展开调查', 'span': '', 'cf': ''},\n",
       "    {'v': '笔记本电脑丢失', 'span': '', 'cf': ''}]},\n",
       "  'add_refs': []}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据\n",
    "data = json.load(open(\"./internal_data_type/data2text_test_all-v1.json\"))\n",
    "\n",
    "#选取第一条进行推理\n",
    "oneke_wrapper.inference_vllm(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225caee1-18a0-4294-88d6-87c2af81371c",
   "metadata": {},
   "source": [
    "## vllm推理全集测试，记录平均时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb26d13-0aec-44f3-a0cb-aaf53c5ab3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T12:33:27.044035Z",
     "iopub.status.busy": "2024-08-31T12:33:27.043561Z",
     "iopub.status.idle": "2024-08-31T13:30:23.143158Z",
     "shell.execute_reply": "2024-08-31T13:30:23.142060Z",
     "shell.execute_reply.started": "2024-08-31T12:33:27.044017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 409/2138 [10:32<44:06,  1.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 69 (char 68)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['时间', '人物', '国家', '装备名称', '装备类型', '装备数量', '组织机构', '地点', '任务', '行动', '事件'], 'input': '美国海军第七舰队旗舰「小鹰」号航空母舰于10月13日抵达北海道小樽市的小樽港，登陆日本民用港口是美日两国新防卫合作指针和「周边事态法」的成果之一，遭到担忧小樽朝军港化发展的民间组织在港口集结抗议，但航母仍然在胜纳埠头安然停泊。这艘满载排水量81,775吨的「小鹰」号是继1997年9月停泊于此的航母「独立」号之后第二艘进入小樽港的美国航空母舰，这也是美国军舰首次进入日本民用港口。'}, output:{\"时间\": [\"10月13日\"], \"人物\": [], \"国家\": [\"美国\"], \"装备名称\": [\"小鹰」号, «独立」号], \"装备类型\": [\"航空母舰\"], \"装备数量\": [\"1\"], \"组织机构\": [], \"地点\": [\"北海道小樽市的小樽港\"], \"任务\": [], \"行动\": [], \"事件\": []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1179/2138 [30:30<21:32,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 124 (char 123)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['时间', '人物', '国家', '主体名称', '主体类型', '主体数量', '组织机构', '地点', '行动', '事件'], 'input': '美国陆军最近与洛克希德・马丁公司达成一项总额达3.91亿美元的生产合同，旨在为士兵提供增强型全方位防护，以应对火箭、迫击炮和火炮攻击。根据该合同，洛克希德・马丁公司将制造33套AN/TPQ-53火力定位目标捕获侦察雷达，并提供相应备件、测试和培训服务，要求在2014年底之前完成交付。'}, output:{\"时间\": [\"2014年底之前\"], \"人物\": [], \"国家\": [\"美国\"], \"主体名称\": [\"美国陆军\", \"洛克希德・马丁公司\"], \"主体类型\": [\"军队\", \"公司\"], \"主体数量\": [\"2\"], \"组织机构\": []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1797/2138 [45:47<08:43,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 1 column 115 (char 114)\n",
      "input:{'instruction': '你是一个命名实体识别专家。请从input中抽取符合schema描述的实体，如果实体类型不存在就返回空列表，并输出为可解析的json格式。', 'schema': ['时间', '人物', '国家', '主体名称', '主体类型', '主体数量', '组织机构', '地点', '行动', '事件'], 'input': '8月12日，马来西亚皇家海军在南海进行了驯服莎丽20/21演习，展示了反水面目标作战能力。其中，两艘护卫舰勒基号和卡斯图里号，以及一艘潜艇，各试射了一枚飞鱼反舰导弹。这两艘护卫舰均为德国制造的1800吨卡斯图里级护卫舰，于1984年投入使用。'}, output:{\"时间\": [\"8月12日\"], \"人物\": [], \"国家\": [\"马来西亚\"], \"主体名称\": [\"勒基号和卡斯图里号\", \"潜艇\"], \"主体类型\": [\"护卫舰\", \"潜艇\"], \"主体数量\": [\"2\", \"1\"]， \"组织机构\": [], \"地点\": [\"南海\"], \"行动\": [], \"事件\": []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2138/2138 [56:56<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.592879522488882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 所有数据计算平均时间\n",
    "from tqdm import tqdm\n",
    "oneke_wrapper.data_count = 0\n",
    "oneke_wrapper.total_time = 0\n",
    "results = []\n",
    "for d in tqdm(data):\n",
    "    results.append(oneke_wrapper.inference_vllm(d))\n",
    "print(oneke_wrapper.total_time/oneke_wrapper.data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525d7524-b9fd-4337-93e2-869433fe5e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:30:23.144963Z",
     "iopub.status.busy": "2024-08-31T13:30:23.144456Z",
     "iopub.status.idle": "2024-08-31T13:30:23.150197Z",
     "shell.execute_reply": "2024-08-31T13:30:23.149384Z",
     "shell.execute_reply.started": "2024-08-31T13:30:23.144936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 1.592879522488882\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average inference time: {oneke_wrapper.total_time/oneke_wrapper.data_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1197303d-496b-40ed-ad83-a2ce4921b770",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:30:23.151810Z",
     "iopub.status.busy": "2024-08-31T13:30:23.151322Z",
     "iopub.status.idle": "2024-08-31T13:30:23.162941Z",
     "shell.execute_reply": "2024-08-31T13:30:23.162121Z",
     "shell.execute_reply.started": "2024-08-31T13:30:23.151768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for inference: 18177.55 MB\n",
      "Memory occupied after inference: 74.95%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "memory_used = torch.cuda.memory_allocated() - oneke_wrapper.gpu_mem_before_load\n",
    "memory_occupied = torch.cuda.memory_allocated() / oneke_wrapper.gpu_mem_max * 100\n",
    "print(f\"Memory used for inference: {memory_used / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Memory occupied after inference: {memory_occupied:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44d58b55-cb1a-4583-980a-d6beff3f43f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:30:23.164260Z",
     "iopub.status.busy": "2024-08-31T13:30:23.163937Z",
     "iopub.status.idle": "2024-08-31T13:30:23.918933Z",
     "shell.execute_reply": "2024-08-31T13:30:23.918136Z",
     "shell.execute_reply.started": "2024-08-31T13:30:23.164227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2138\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "json.dump(results, open(\"./results/baichuan_vllm.json\",'w+'),ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
